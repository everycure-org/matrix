{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Runbook: Count MLflow Experiments from Last 3 Months\n",
    "\n",
    "This runbook queries the cloud MLflow tracking server to count the number of experiments/runs executed in the last 3 months.\n",
    "\n",
    "## Prerequisites\n",
    "- kubectl configured for the GKE cluster\n",
    "- Access to the cluster where MLflow is running\n",
    "- Python environment with `mlflow` and `pandas` packages installed\n",
    "\n",
    "## How It Works\n",
    "1. Automatically authenticates with GKE cluster\n",
    "2. Discovers MLflow service using kubectl\n",
    "3. Sets up port-forwarding to MLflow service\n",
    "4. Queries and analyzes experiment data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 0: Configure GKE Connection and Discover MLflow Service\n",
    "\n",
    "Authenticate with GKE and discover the MLflow service configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "import socket\n",
    "import os\n",
    "\n",
    "# GKE Configuration\n",
    "CLOUD_PROJECT = \"mtrx-hub-dev-3of\"\n",
    "CLOUD_REGION = \"us-central1\"\n",
    "CLOUD_CLUSTER = \"compute-cluster\"\n",
    "\n",
    "# Port configuration\n",
    "LOCAL_PORT = 5001\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 0: GKE Authentication and Service Discovery\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Authenticate with GKE\n",
    "print(\"\\n1. Authenticating with GKE cluster...\")\n",
    "auth_cmd = [\n",
    "    \"gcloud\", \"container\", \"clusters\", \"get-credentials\",\n",
    "    CLOUD_CLUSTER,\n",
    "    \"--region\", CLOUD_REGION,\n",
    "    \"--project\", CLOUD_PROJECT\n",
    "]\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(auth_cmd, capture_output=True, text=True, timeout=30)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"✗ Failed to authenticate: {result.stderr}\")\n",
    "        raise Exception(\"GKE authentication failed\")\n",
    "    print(f\"✓ Successfully authenticated with GKE cluster: {CLOUD_CLUSTER}\")\n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"✗ Authentication timed out\")\n",
    "    raise\n",
    "\n",
    "# Discover MLflow service using kubectl\n",
    "print(\"\\n2. Discovering MLflow service...\")\n",
    "search_namespaces = [\"mlflow\"]\n",
    "mlflow_service_found = False\n",
    "MLFLOW_SERVICE_NAME = None\n",
    "MLFLOW_NAMESPACE = None\n",
    "MLFLOW_SERVICE_PORT = None\n",
    "\n",
    "for namespace in search_namespaces:\n",
    "    try:\n",
    "        # Get services in namespace\n",
    "        get_svc_cmd = [\"kubectl\", \"get\", \"svc\", \"-n\", namespace, \"-o\", \"json\"]\n",
    "        result = subprocess.run(get_svc_cmd, capture_output=True, text=True, timeout=10)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            services = json.loads(result.stdout)\n",
    "            \n",
    "            # Look for MLflow service\n",
    "            for svc in services.get(\"items\", []):\n",
    "                svc_name = svc[\"metadata\"][\"name\"]\n",
    "                if \"mlflow-tracking\" in svc_name.lower():\n",
    "                    MLFLOW_SERVICE_NAME = svc_name\n",
    "                    MLFLOW_NAMESPACE = namespace\n",
    "                    \n",
    "                    # Get the service port\n",
    "                    ports = svc[\"spec\"].get(\"ports\", [])\n",
    "                    if ports:\n",
    "                        MLFLOW_SERVICE_PORT = ports[0][\"port\"]\n",
    "                    \n",
    "                    mlflow_service_found = True\n",
    "                    print(f\"✓ Found MLflow service: {MLFLOW_SERVICE_NAME}\")\n",
    "                    print(f\"  Namespace: {MLFLOW_NAMESPACE}\")\n",
    "                    print(f\"  Port: {MLFLOW_SERVICE_PORT}\")\n",
    "                    break\n",
    "            \n",
    "            if mlflow_service_found:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not check namespace {namespace}: {e}\")\n",
    "        continue\n",
    "\n",
    "if not mlflow_service_found:\n",
    "    print(\"✗ Could not find MLflow service in common namespaces\")\n",
    "    print(f\"  Searched: {', '.join(search_namespaces)}\")\n",
    "    print(\"\\n  Please set manually:\")\n",
    "    print(\"    MLFLOW_SERVICE_NAME = 'your-mlflow-service-name'\")\n",
    "    print(\"    MLFLOW_NAMESPACE = 'your-namespace'\")\n",
    "    print(\"    MLFLOW_SERVICE_PORT = 5000\")\n",
    "    raise Exception(\"MLflow service not found\")\n",
    "\n",
    "print(f\"\\n✓ Configuration complete!\")\n",
    "print(f\"  Service: {MLFLOW_SERVICE_NAME}\")\n",
    "print(f\"  Namespace: {MLFLOW_NAMESPACE}\")\n",
    "print(f\"  Service Port: {MLFLOW_SERVICE_PORT}\")\n",
    "print(f\"  Local Port: {LOCAL_PORT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 1: Setup Port-Forward to MLflow\n",
    "\n",
    "Establish port-forwarding to the MLflow service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_port_open(port, host=\"127.0.0.1\"):\n",
    "    \"\"\"Check if a port is open and listening.\"\"\"\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    sock.settimeout(1)\n",
    "    try:\n",
    "        result = sock.connect_ex((host, port))\n",
    "        sock.close()\n",
    "        return result == 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Port-Forward Setup\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if port-forward is already running\n",
    "if check_port_open(LOCAL_PORT):\n",
    "    print(f\"✓ Port {LOCAL_PORT} is already in use (existing port-forward detected)\")\n",
    "    MLFLOW_ENDPOINT = f\"http://127.0.0.1:{LOCAL_PORT}\"\n",
    "else:\n",
    "    # Start port-forward in background\n",
    "    print(f\"\\nStarting kubectl port-forward...\")\n",
    "    print(f\"  Forwarding: 127.0.0.1:{LOCAL_PORT} -> {MLFLOW_SERVICE_NAME}:{MLFLOW_SERVICE_PORT}\")\n",
    "    \n",
    "    port_forward_cmd = [\n",
    "        \"kubectl\", \"port-forward\",\n",
    "        f\"service/{MLFLOW_SERVICE_NAME}\",\n",
    "        f\"{LOCAL_PORT}:{MLFLOW_SERVICE_PORT}\",\n",
    "        \"-n\", MLFLOW_NAMESPACE\n",
    "    ]\n",
    "    \n",
    "    # Start in background\n",
    "    process = subprocess.Popen(\n",
    "        port_forward_cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    # Wait for port-forward to establish\n",
    "    print(\"  Waiting for connection to establish...\")\n",
    "    for i in range(15):\n",
    "        time.sleep(1)\n",
    "        if check_port_open(LOCAL_PORT):\n",
    "            print(f\"✓ Port-forward established successfully!\")\n",
    "            print(f\"  Process PID: {process.pid}\")\n",
    "            print(f\"  To stop later: kill {process.pid}\")\n",
    "            break\n",
    "        print(f\"    ... waiting ({i+1}/15)\")\n",
    "    else:\n",
    "        print(\"✗ Port-forward did not establish within 15 seconds\")\n",
    "        stderr_output = process.stderr.read() if process.stderr else \"\"\n",
    "        print(f\"  Error: {stderr_output}\")\n",
    "        process.terminate()\n",
    "        raise Exception(\"Failed to establish port-forward\")\n",
    "    \n",
    "    MLFLOW_ENDPOINT = f\"http://127.0.0.1:{LOCAL_PORT}\"\n",
    "\n",
    "print(f\"\\n✓ MLflow endpoint ready: {MLFLOW_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 2: Connect to MLflow and Verify\n",
    "\n",
    "Connect to the MLflow tracking server and verify the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 2: MLflow Connection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(MLFLOW_ENDPOINT)\n",
    "print(f\"\\nMLflow Tracking URI: {MLFLOW_ENDPOINT}\")\n",
    "\n",
    "# Test the connection\n",
    "print(\"\\nTesting connection...\")\n",
    "try:\n",
    "    client = MlflowClient()\n",
    "    test_experiments = client.search_experiments(max_results=1)\n",
    "    print(f\"✓ Connected successfully to MLflow!\")\n",
    "    print(f\"  Server is responding\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Connection failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Verify port-forward is still running\")\n",
    "    print(\"2. Check MLflow pod is running: kubectl get pods -n\", MLFLOW_NAMESPACE)\n",
    "    print(\"3. Check MLflow logs: kubectl logs -n\", MLFLOW_NAMESPACE, \"-l app=mlflow\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 3: Define Time Range\n",
    "\n",
    "Calculate the timestamp for 3 months ago from today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate date 3 months ago\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=90)  # Approximately 3 months\n",
    "\n",
    "# Convert to Unix timestamp in milliseconds (MLflow format)\n",
    "start_timestamp_ms = int(start_date.timestamp() * 1000)\n",
    "end_timestamp_ms = int(end_date.timestamp() * 1000)\n",
    "\n",
    "print(f\"Start Date: {start_date.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"End Date: {end_date.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nQuerying runs between {start_date.date()} and {end_date.date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 4: Query All Experiments\n",
    "\n",
    "List all experiments in the MLflow tracking server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "# Get all experiments (including archived) without limit\n",
    "all_experiments = client.search_experiments(\n",
    "    view_type=mlflow.entities.ViewType.ALL,\n",
    "    max_results=50000  # Set very high limit to get all experiments\n",
    ")\n",
    "\n",
    "print(f\"Total experiments found: {len(all_experiments)}\")\n",
    "print(\"\\nExperiment List:\")\n",
    "for exp in all_experiments[:10]:  # Show first 10\n",
    "    print(f\"  - {exp.name} (ID: {exp.experiment_id})\")\n",
    "if len(all_experiments) > 10:\n",
    "    print(f\"  ... and {len(all_experiments) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Step 5: Query Runs from Last 3 Months\n",
    "\n",
    "Search for all runs across all experiments that were created in the last 3 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Build filter query for runs in the last 3 months\n",
    "# MLflow uses milliseconds since epoch for timestamps\n",
    "filter_string = f\"attributes.start_time >= {start_timestamp_ms}\"\n",
    "\n",
    "def query_experiment_runs(experiment):\n",
    "    \"\"\"Query runs for a single experiment.\"\"\"\n",
    "    try:\n",
    "        runs = client.search_runs(\n",
    "            experiment_ids=[experiment.experiment_id],\n",
    "            filter_string=filter_string,\n",
    "            max_results=50000  # Set very high limit to get all experiments\n",
    "        )\n",
    "        return experiment, runs, None\n",
    "    except Exception as e:\n",
    "        return experiment, [], str(e)\n",
    "\n",
    "print(\"Querying experiments in parallel...\")\n",
    "print(f\"Total experiments to query: {len(all_experiments)}\")\n",
    "\n",
    "all_runs = []\n",
    "errors = []\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel queries\n",
    "# Adjust max_workers based on your needs (default: min(32, num_experiments))\n",
    "max_workers = len(all_experiments)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Submit all queries\n",
    "    future_to_exp = {\n",
    "        executor.submit(query_experiment_runs, exp): exp \n",
    "        for exp in all_experiments\n",
    "    }\n",
    "    \n",
    "    # Process results as they complete\n",
    "    completed = 0\n",
    "    for future in as_completed(future_to_exp):\n",
    "        completed += 1\n",
    "        experiment, runs, error = future.result()\n",
    "        \n",
    "        if error:\n",
    "            errors.append(f\"{experiment.name}: {error}\")\n",
    "        else:\n",
    "            all_runs.extend(runs)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if completed % 10 == 0 or completed == len(all_experiments):\n",
    "            print(f\"  Progress: {completed}/{len(all_experiments)} experiments processed\")\n",
    "\n",
    "print(f\"\\n✓ Query complete!\")\n",
    "print(f\"  Total runs found in last 3 months: {len(all_runs)}\")\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\n⚠ Warnings ({len(errors)} experiments had errors):\")\n",
    "    for error in errors[:5]:  # Show first 5 errors\n",
    "        print(f\"  - {error}\")\n",
    "    if len(errors) > 5:\n",
    "        print(f\"  ... and {len(errors) - 5} more errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Results\n",
    "\n",
    "Break down the results by experiment, status, and time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize runs by experiment\n",
    "runs_by_experiment: Dict[str, List] = {}\n",
    "runs_by_status: Dict[str, int] = {}\n",
    "\n",
    "for run in all_runs:\n",
    "    exp_id = run.info.experiment_id\n",
    "    if exp_id not in runs_by_experiment:\n",
    "        runs_by_experiment[exp_id] = []\n",
    "    runs_by_experiment[exp_id].append(run)\n",
    "    \n",
    "    # Count by status\n",
    "    status = run.info.status\n",
    "    runs_by_status[status] = runs_by_status.get(status, 0) + 1\n",
    "\n",
    "# Create summary DataFrame\n",
    "experiment_summary = []\n",
    "for exp_id, runs in runs_by_experiment.items():\n",
    "    experiment = next((e for e in all_experiments if e.experiment_id == exp_id), None)\n",
    "    exp_name = experiment.name if experiment else f\"Unknown ({exp_id})\"\n",
    "    experiment_summary.append({\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Experiment ID\": exp_id,\n",
    "        \"Run Count\": len(runs)\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(experiment_summary).sort_values(\n",
    "    \"Run Count\", ascending=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: Runs by Experiment (Last 3 Months)\")\n",
    "print(\"=\"*60)\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: Runs by Status\")\n",
    "print(\"=\"*60)\n",
    "for status, count in sorted(runs_by_status.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {status}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 7: Detailed Statistics\n",
    "\n",
    "Get more detailed statistics including temporal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with all run details\n",
    "run_details = []\n",
    "for run in all_runs:\n",
    "    experiment = next(\n",
    "        (e for e in all_experiments if e.experiment_id == run.info.experiment_id),\n",
    "        None\n",
    "    )\n",
    "    exp_name = experiment.name if experiment else \"Unknown\"\n",
    "    \n",
    "    start_time = datetime.fromtimestamp(run.info.start_time / 1000)\n",
    "    end_time = (\n",
    "        datetime.fromtimestamp(run.info.end_time / 1000)\n",
    "        if run.info.end_time\n",
    "        else None\n",
    "    )\n",
    "    duration = (\n",
    "        (run.info.end_time - run.info.start_time) / 1000\n",
    "        if run.info.end_time\n",
    "        else None\n",
    "    )\n",
    "    \n",
    "    run_details.append({\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Run ID\": run.info.run_id,\n",
    "        \"Status\": run.info.status,\n",
    "        \"Start Time\": start_time,\n",
    "        \"End Time\": end_time,\n",
    "        \"Duration (seconds)\": duration,\n",
    "        \"User\": run.info.user_id,\n",
    "    })\n",
    "\n",
    "df_runs = pd.DataFrame(run_details)\n",
    "\n",
    "# Temporal distribution by week\n",
    "df_runs[\"Week\"] = pd.to_datetime(df_runs[\"Start Time\"]).dt.to_period(\"W\")\n",
    "weekly_counts = df_runs.groupby(\"Week\").size().reset_index(name=\"Run Count\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Runs per Week\")\n",
    "print(\"=\"*60)\n",
    "print(weekly_counts.to_string(index=False))\n",
    "\n",
    "# Average duration by experiment\n",
    "avg_duration = df_runs.groupby(\"Experiment\")[\"Duration (seconds)\"].agg(\n",
    "    [\"mean\", \"median\", \"count\"]\n",
    ").round(2)\n",
    "avg_duration.columns = [\"Avg Duration (s)\", \"Median Duration (s)\", \"Count\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Average Run Duration by Experiment\")\n",
    "print(\"=\"*60)\n",
    "print(avg_duration.sort_values(\"Count\", ascending=False).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Step 8: Export Results (Optional)\n",
    "\n",
    "Export the detailed results to CSV for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "output_file = f\"/tmp/mlflow_runs_last_3_months_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df_runs.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nResults exported to: {output_file}\")\n",
    "print(f\"Total rows: {len(df_runs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This runbook provides:\n",
    "1. **Total run count** for the last 3 months\n",
    "2. **Breakdown by experiment** showing which experiments are most active\n",
    "3. **Status distribution** (finished, failed, running, etc.)\n",
    "4. **Temporal distribution** showing runs per week\n",
    "5. **Duration statistics** by experiment\n",
    "6. **Exportable CSV** for further analysis\n",
    "\n",
    "## Key Metrics at a Glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY METRICS - LAST 3 MONTHS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Experiments: {len(all_experiments)}\")\n",
    "print(f\"Experiments with Runs: {len(runs_by_experiment)}\")\n",
    "print(f\"Total Runs: {len(all_runs)}\")\n",
    "print(f\"\\nMost Active Experiment: {df_summary.iloc[0]['Experiment']} ({df_summary.iloc[0]['Run Count']} runs)\")\n",
    "print(f\"\\nStatus Breakdown:\")\n",
    "for status, count in sorted(runs_by_status.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = (count / len(all_runs)) * 100\n",
    "    print(f\"  {status}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "if not df_runs.empty:\n",
    "    avg_weekly = weekly_counts[\"Run Count\"].mean()\n",
    "    print(f\"\\nAverage Runs per Week: {avg_weekly:.1f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matrix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
