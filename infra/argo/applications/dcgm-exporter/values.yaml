# DCGM Exporter configuration for comprehensive GPU monitoring
image:
  # DCGM Exporter image (connects to host engine)
  repository: nvcr.io/nvidia/k8s/dcgm-exporter
  tag: "4.2.3-4.1.3-ubuntu22.04"
  pullPolicy: IfNotPresent

hostEngine:
  # DCGM Host Engine image (privileged container for GPU access)
  image:
    repository: nvcr.io/nvidia/cloud-native/dcgm
    tag: "3.3.0-1-ubuntu22.04"
    pullPolicy: IfNotPresent
  # Resource requirements for host engine
  resources:
    requests:
      memory: 512Mi
      cpu: 50m
    limits:
      memory: 1Gi
      cpu: 200m
  # Host engine port configuration
  port: 5555
  # Host engine probe configuration
  probes:
    liveness:
      initialDelaySeconds: 30
      periodSeconds: 30
    readiness:
      initialDelaySeconds: 5
      periodSeconds: 10

# Exporter configuration
exporter:
  # Port configuration for metrics
  port: 9400
  # Host engine connection
  hostEngineAddress: "127.0.0.1:5555"
  # Probe configuration
  probes:
    liveness:
      initialDelaySeconds: 60
      periodSeconds: 30
      path: /metrics
    readiness:
      initialDelaySeconds: 30
      periodSeconds: 10
      path: /metrics

# Node affinity for GPU nodes (Google recommended approach)
nodeAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
    nodeSelectorTerms:
    - matchExpressions:
      - key: cloud.google.com/gke-accelerator
        operator: Exists

# Tolerations for GPU nodes (Google recommended approach)
tolerations:
  - operator: "Exists"

# Resource requirements
resources:
  requests:
    memory: 1Gi
    cpu: 100m
  limits:
    memory: 1Gi
    cpu: 200m

# Security context
securityContext:
  privileged: true
  capabilities:
    add: ["SYS_ADMIN"]

# Service configuration
service:
  enable: true
  port: 9400  # Should match exporter.port
  type: ClusterIP

# ServiceMonitor for Prometheus
serviceMonitor:
  enabled: true
  interval: 15s
  scrapeTimeout: 10s
  labels:
    release: kube-prometheus-stack
  # Metric relabeling to extract workflow information
  metricRelabelings:
    enabled: true
    # Extract workflow name from Argo workflow pod names
    workflowNameExtraction:
      # Pattern for full workflow name: {workflow-name}-{template-name}-{random-id}
      enabled: true
      sourceLabel: pod
      # Regex to match: graph-filter-pks-b2-run1-83f979e6-kedro-2157018018
      # Extracts: graph-filter-pks-b2-run1-83f979e6
      regex: '([^-]+-[^-]+-[^-]+-[^-]+-[a-f0-9]+)-.*'
      targetLabel: workflow_name

# Environment variables
env:
  DCGM_EXPORTER_KUBERNETES: "true"
  DCGM_EXPORTER_LISTEN: ":9400"  # Should match exporter.port
  DCGM_EXPORTER_KUBERNETES_GPU_ID_TYPE: "device-name"

# Host volumes for GPU access
hostVolumes:
  - name: nvidia-install-dir
    hostPath: /home/kubernetes/bin/nvidia
    mountPath: /usr/local/nvidia
    readOnly: true
  - name: pod-resources
    hostPath: /var/lib/kubelet/pod-resources
    mountPath: /var/lib/kubelet/pod-resources
    readOnly: true

# Additional labels for pods
podLabels:
  app: dcgm-exporter
  component: gpu-monitoring

# ConfigMap for DCGM metrics (Google recommended)
configMap:
  enabled: true
  data:
    counters.csv: |
      # Utilization (the sample period varies depending on the product),,
      DCGM_FI_DEV_GPU_UTIL, gauge, GPU utilization (in %).
      DCGM_FI_DEV_MEM_COPY_UTIL, gauge, Memory utilization (in %).
      
      # Temperature and power usage,,
      DCGM_FI_DEV_GPU_TEMP, gauge, Current temperature readings for the device in degrees C.
      DCGM_FI_DEV_MEMORY_TEMP, gauge, Memory temperature for the device.
      DCGM_FI_DEV_POWER_USAGE, gauge, Power usage for the device in Watts.
      
      # Utilization of IP blocks,,
      DCGM_FI_PROF_SM_ACTIVE, gauge, The ratio of cycles an SM has at least 1 warp assigned
      DCGM_FI_PROF_SM_OCCUPANCY, gauge, The fraction of resident warps on a multiprocessor
      DCGM_FI_PROF_PIPE_TENSOR_ACTIVE, gauge, The ratio of cycles the tensor (HMMA) pipe is active
      DCGM_FI_PROF_PIPE_FP64_ACTIVE, gauge, The fraction of cycles the FP64 (double precision) pipe was active.
      DCGM_FI_PROF_PIPE_FP32_ACTIVE, gauge, The fraction of cycles the FP32 (single precision) pipe was active.
      DCGM_FI_PROF_PIPE_FP16_ACTIVE, gauge, The fraction of cycles the FP16 (half precision) pipe was active.
      
      # Memory usage,,
      DCGM_FI_DEV_FB_FREE, gauge, Framebuffer memory free (in MiB).
      DCGM_FI_DEV_FB_USED, gauge, Framebuffer memory used (in MiB).
      DCGM_FI_DEV_FB_TOTAL, gauge, Total Frame Buffer of the GPU in MB.
      
      # PCIE,,
      DCGM_FI_PROF_PCIE_TX_BYTES, gauge, Total number of bytes transmitted through PCIe TX
      DCGM_FI_PROF_PCIE_RX_BYTES, gauge, Total number of bytes received through PCIe RX
      
      # NVLink,,
      DCGM_FI_PROF_NVLINK_TX_BYTES, gauge, The number of bytes of active NvLink tx (transmit) data including both header and payload.
      DCGM_FI_PROF_NVLINK_RX_BYTES, gauge, The number of bytes of active NvLink rx (read) data including both header and payload.

# Pod annotations
podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "9400"
  prometheus.io/path: "/metrics"

# DaemonSet configuration
daemonSet:
  # Update strategy
  updateStrategy:
    type: RollingUpdate
  # Host configuration
  hostNetwork: true
  hostPID: true

# Path configurations for GKE
paths:
  # NVIDIA driver installation directory
  nvidiaDriverPath: /home/kubernetes/bin/nvidia
  # NVIDIA library path
  nvidiaLibPath: /usr/local/nvidia/lib64

# Container names (for customization)
containerNames:
  hostEngine: nvidia-dcgm
  exporter: dcgm-exporter
