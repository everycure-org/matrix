---
title: "Run comparison pipeline"
---
# Cross run evaluation with the `run_comparison` pipeline
The `run_comparison` pipeline generates various performance metrics and visualisations that allow us to compare several sets of drug-disease predictions across all drugs and diseases.
As well as predictions generated by the MATRIX modelling pipeline, 
it supports any custom set of predictions satisfying the assumptions and schema described below. 

The pipeline includes the following metrics: 

  - *Full matrix ranking.*  Recall@n vs. n curve for on-label indications, off-label indications and known contraindications. 
  - *Disease-specific ranking.* Disease-specific Hit@k vs. k curve for on-label indications and  off-label indications.
  - *Known positive vs. known negative classification.* Precision-recall curve. 
  - *Prevalence of frequent flyers.* Drug and Disease Entropy@n vs. n curves.
  - *Similarity between models.* Commonality@n vs. n curve. 

An overview of these metrics is given in the [evaluation suite deep dive](/pipeline/data_science/evaluation_deep_dive/). 

In addition, the pipeline includes the following features:

  - *Uncertainty estimation.* Different options, namely, multifold uncertainty estimation, [bootstrap uncertainty estimation](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) or a combination of both. 
  - *Data consistency and harmonisation.*  Utilities to ensure a consistent and fair evaluation, such as taking intersection between drug and disease lists for all sets of predictions. 

## How do I use the `run_comparison` pipeline? 

To use the run comparison pipeline, follow these steps:

  1. Ensure that the MATRIX repository is cloned and the environment set-up. Create and checkout a new branch off `main`.
  2. Modify the parameters configuration file for the run comparison pipeline  `pipelines/matrix/conf/base/run_comparison/parameters.yml`
  to specify the predictions dataframes you would like to include in the evaluation. Ensure that the prediction dataframes do not include pairs used in training. Details given below. 
  3. Run the command (hint: ensure your Docker daemon is running): 
  ```
  kedro experiment run --pipeline=run_comparison --username=<your name>--run-name=<your run name>
  ```
  4. View the results in GCS:
  ```
   gs://mtrx-us-central1-hub-dev-storage/kedro/data/run_comparison/runs/<your run name>/
  ```

## How do I configure the `parameters.yml` file?

### Specify filepaths for input predictions

#### Assumptions on custom input predictions 

![Input dataframe](../../assets/run_comparison/input_dataframe.drawio.svg)

### Specify mode of uncertainty estimation
Mode of uncertainty estimation is specified by two boolean parameters: 

  - `perform_multifold_uncertainty_estimation` 
  - `perform_bootstrap_uncertainty_estimation`

For evaluations which don't ground truth data (e.g. Entropy@n and Commonality@n), bootstrap uncertainty estimation will skipped regardless of the value of `perform_bootstrap_uncertainty_estimation`.

If both parameters are set to true, multifold and bootstrap uncertainty estimation will be combined by performing ground truth resampling for each fold of data.  

### Specify data consistency procedure 



### (Optional) Custom evaluations
