<!-- TODO PBR complet -->
## Environments

We have 4 environments declared in the kedro project for `MATRIX`:

- `base`: Contains the base environment which reads the real data from GCS and operates in your local compute environment
- `cloud`: Contains the cloud environment with real data. All data is read and written to a GCP project as configured (see below). Assumes fully stateless local machine operations (e.g. in docker containers)
- `test`: Fully local and contains parameters that "break" the meaning of algorithms in the pipeline (e.g. 2 dimensions PCA). This is useful for running an integration test with mock data to validate the programming of the pipeline is correct to a large degree. 
- `local`: A default environment which you can use for local adjustments and tweaks. Changes to this repo are not usually committed to git as they are unique for every developer. 
- `sample`: Contains a sample of the data and is useful for fast iterations on the pipeline from the embeddings pipeline and on.

!!! info
    Our `cloud` environment is equipped with environment variables that allows for configuring the GCP project to use. This is especially relevant to switch between the `hub` and `wg` projects as desired.

    The source code contains a `.env.tmpl` configuration template file. To configure the `cloud` environment, create your own `.env` file from the template and uncomment variables relevant to your configuration. 

You can run any of the environments using the `--env` flag. For example, to run the pipeline in the `cloud` environment, you can use the following command:

```bash
kedro run --env cloud
```

!!! info
    Environments are abstracted away by Kedro's data catalog which is, in turn, defined as configuration in YAML. The catalog is dynamic, in the sense that it can combine the `base` environment with another environment during execution. This allows for overriding some of the configuration in `base` such that data can flow into different systems according to the selected _environment_. 

    The image below represents a pipeline configuration across three environments, `base`, `cloud` and `test`. By default the pipeline reads from Google Cloud Storage (GCS) and writes to the local filesystem. The `cloud` environment redefines the output dataset to write to `BigQuery` (as opposed to local). The `test` environment redefines the input dataset to read the output from the fabricator pipeline, thereby having the effect that the pipeline runs on synthetic data.

![](../assets/img/environments.drawio.svg)

### Run with fake data locally

To run the full pipeline locally with fake data, you can use the following command:

```bash
kedro run --env test -p test 
```

This runs the full pipeline with fake data.

### Run with real data locally

To run the full pipeline with real data by copying the RAW data from the central GCS bucket and then run everything locally you can simply run from the default environment. We've setup an intermediate pipeline that copies data to avoid constant copying of the data from cloud.

```bash
# Copy data from cloud to local
kedro run -p ingestion
```

Hereafter, you can run the default pipeline.

```bash
# Default pipeline in default environment
kedro run
```

## Pipeline with Jupyter notebooks

Kedro may be used with [Jupyter notebooks](https://docs.kedro.org/en/stable/notebooks_and_ipython/kedro_and_notebooks.html) for interactive experiments. This allows us to utilise data and models generated by pipeline runs in Jupyter notebooks, as well as to take advantage of the functions and classes in the Kedro project codebase. 

Jupyter notebooks should be created in the directory `pipelines/matrix/notebooks/scratch`. This will be ignored by the matrix git repository. 

!!! tip
    A separate git repository for notebook version control may be created inside the `scratch` directory. It can also be nice to create a symbolic link to `scratch` from a directory of your choice on your machine. 

    An example notebook is also added to our documentation [here](./walkthroughs/kedro_notebook_example.ipynb which you can copy into the scratch directory for a quickstart

Within a notebook, first run a cell with the following magic command:

```python
%load_ext kedro.ipython
```

By default, this loads the `base` Kedro environment which is used only with fabricated data. 
To load the `cloud` Kedro environment with real data, run another cell with the following command:
```python
%reload_kedro --env=cloud
```

These commands define several useful global variables on your behalf: `context`, `session`, `catalog` and `pipelines`.

In particular, the `catalog` variable provides an interface to the Kedro data catalog, which includes all data, models and model outputs produced during the latest `cloud` run of the Kedro pipeline. The following command lists the available items in the data catalog:
```python
catalog.list()
```
Items may be loaded into memory using the `catalog.load` method. For example, if we have a catalog item `modelling.model_input.splits`, it may be loaded in as follows: 
```python
splits = catalog.load('modelling.model_input.splits')
```

Functions and classes in the Kedro project source code may be imported as required. For example, a function `train_model` defined in the file `pipelines/matrix/src/matrix/pipelines/modelling/nodes.py` may be imported as follows:
```
from matrix.pipelines.modelling.nodes import train_model
```

Further information may be found [here](https://docs.kedro.org/en/stable/notebooks_and_ipython/kedro_and_notebooks.html). 

[Next up, some advanced kedro features in our codebase :material-skip-next:](./kedro_extensions.md){ .md-button .md-button--primary }

## Sample environment

The sample environment allows to run parts of the pipeline with a smaller dataset, sampled from the original data. This sample is stored in GCS, you can then run the pipeling with this sample data locally or in kubernetes.

Two pipelines are defined in the `sample` environment:
- `create_sample`: Creates the sample data and overrides the one currently in GCS.
- `test_sample`: Runs the pipeline from the embeddings step onwards with the sample data stored in GCS.

### Run with sample data locally

Local tests using sample are done in the `sample` environment. They will pull the latest sample 

```bash
kedro run -e sample -p test_sample
```

### Run with sample data in kubernetes 

```bash
kedro submit -e sample -p test_sample
```

### Update sample data

You can update sample data by running the `create_sample` pipeline locally. This will create a sample of the nodes and edges produced by a release of the integration layer. The release version can be found, and changed, in the `sample/globals.yml` file.

!!! warning
    There is only one version of the sample data in GCS. Updating it means deleting the previous sample.

```bash
kedro run -e sample -p create_sample
```