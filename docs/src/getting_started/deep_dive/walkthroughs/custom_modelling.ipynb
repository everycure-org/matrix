{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a custom model in the pipeline\n",
    "\n",
    "Mateusz Wasilewski\n",
    "\n",
    "Lee Lancashire\n",
    "\n",
    "Every Cure\n",
    "\n",
    "16 December 2024. \n",
    "\n",
    "# Introduction\n",
    "\n",
    "The purpose of this notebook is to show how a new, sample model with custom dependencies would be developed and integrated into the pipeline.\n",
    "\n",
    "This notebook follows a hypothetical scenario where Machine Learning Engineer Maya is developing a new model, with the aim of generating her own predictions of drug-disease treatment efficacy scores. Maya is new to the EveryCure / Matrix ecosystem, and is learning as she goes.\n",
    "\n",
    "In the end, she wants to train and submit a new model to the pipeline, and have it evaluated along with the other models.\n",
    "\n",
    "## Modelling assumptions\n",
    "\n",
    "Maya's goal is to train a new model that will predict the efficacy of drug-disease interactions.\n",
    "\n",
    "**Embeddings from Knowledge Graph:** Maya knows that EveryCure has generated embeddings for biomedical knowledge graph nodes, which meaningfully encode semantics of the nodes. Many of those nodes are drugs and diseases between which she wants to predict treatment efficacy.\n",
    "\n",
    "**Training Data:** Maya expects the training data to be a set of known positives and negatives, i.e. drug-disease pairs for which the treatment is known to be effective or ineffective.\n",
    "\n",
    "**Evaluation:** Maya assumes that the model will be evaluated using AUC-ROC. She also assumes that she will need to perform train-validation splits on her data, and that Matrix's pipeline downstream will be able to further test the predictions of her model.\n",
    "\n",
    "**Retrieving Data:** Importantly, Maya will retrieve the embeddings and training data from pipelines other than the modelling pipeline. She will avoid preprocessing the data itself as much as possible, relying on other resources provided by EveryCure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequesites\n",
    "\n",
    "Maya needs access to the GCS bucket containing the data (currently, `gs://mtrx-us-central1-hub-dev-storage`). Maya could retrieve those data sources manually from the bucket, using tools such as `gsutil`, but a much better way is to use Kedro's API.\n",
    "\n",
    "## Dev Environment\n",
    "\n",
    "Maya will build her prototype in Jupyter Kedro Lab. It is a standard jupyter lab, but with kedro context loaded into the notebook. To run kedro notebooks, using cloud environment (to have access to datasets in the cloud), Maya will run the command:\n",
    "\n",
    "```\n",
    "kedro jupyter notebook --env cloud\n",
    "```\n",
    "\n",
    "Importantly, she also set her `RELEASE_VERSION` in her `.env` file to `v0.2.4-rc.1`. She chose this release arbitrarily.\n",
    "\n",
    "Now, Maya will have access to Kedro datasets via Kedro API. For a full tutorial for Kedro API, see [the official documentation](https://docs.kedro.org/en/stable/notebooks_and_ipython/kedro_and_notebooks.html)\n",
    "\n",
    "Importantly, Kedro catalog object can be retrieved only if the notebook is executed in the root directory of a Kedro project (for Matrix that would be `matrix/pipelines/matrix/my_notebook.ipynb`). If this notebook is executed in the walkthough directory, it might not run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext kedro.ipython\n",
    "%reload_kedro  --env cloud\n",
    "cat = catalog #just a convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.list(\"^embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Required data sources\n",
    "\n",
    "### Embeddings \n",
    "\n",
    "Maya will use the embeddings generated by the Knowledge Graph pipeline to encode the drugs and disease into a vector space.\n",
    "\n",
    "In the embeddings pipeline, embeddings are extracted from Neo4j and saved to GCS. The pipeline is defined in the [embeddings pipeline](https://github.com/matrix-ml/matrix/blob/main/pipelines/matrix/src/matrix/pipelines/embeddings/pipeline.py). \n",
    "\n",
    "```python\n",
    "node(\n",
    "    func=nodes.extract_node_embeddings,\n",
    "    inputs={\n",
    "        \"nodes\": \"embeddings.model_output.graphsage\",\n",
    "        \"string_col\": \"params:embeddings.write_topological_col\",\n",
    "    },\n",
    "    outputs=\"embeddings.feat.nodes\",\n",
    "    name=\"extract_nodes_edges_from_db\",\n",
    "    tags=[\n",
    "        \"argowf.fuse\",\n",
    "        \"argowf.fuse-group.topological_embeddings\",\n",
    "        \"argowf.template-neo4j\",\n",
    "    ],\n",
    "),\n",
    "```\n",
    "\n",
    "Kedro Dataset to which the embeddings are saved: \n",
    "\n",
    "```yml\n",
    "embeddings.feat.nodes:\n",
    "  <<: *_spark_parquet\n",
    "  filepath: ${globals:paths.embeddings}/feat/nodes_with_embeddings\n",
    "```\n",
    "\n",
    "\n",
    "Maya knows that `${globals:paths.embeddings}/feat/nodes_with_embeddings` converts to `gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings`, and this is where that dataset will be available in GCP. However, a much easier way to retrieve it is via kedro catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.datasets.embeddings__feat__nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_spark = catalog.load(\"embeddings.feat.nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth data\n",
    "\n",
    "Maya needs to retrieve the training data from the preprocessing pipeline, containing True / False positives and negatives that she can use to train her model on the previously retrieved embeddings. First input to other modelling pipelines is `modelling.raw.ground_truth.positives@spark`, so Maya will retrieve that dataset first (together with its negative counterpart `modelling.raw.ground_truth.negatives@spark`).\n",
    "\n",
    "```python\n",
    "node(\n",
    "    func=nodes.create_int_pairs,\n",
    "    inputs=[\n",
    "        \"embeddings.feat.nodes\",\n",
    "        \"modelling.raw.ground_truth.positives@spark\",\n",
    "        \"modelling.raw.ground_truth.negatives@spark\",\n",
    "    ],\n",
    "    outputs=\"modelling.int.known_pairs@spark\",\n",
    "    name=\"create_int_known_pairs\",\n",
    "),\n",
    "```\n",
    "\n",
    "We retrieve ground truth data (conflated True Positives and True Negatives) from GCS. Both were produced by the `preprocessing` pipeline, as dataset `modelling.raw.ground_truth.positives@pandas` and `modelling.raw.ground_truth.negatives@pandas`, and will be read in as `@spark` dataframes by modelling steps. Maya will run the command below to copy the data to her local machine. Like in the previous step, the used version is arbitrary.\n",
    "\n",
    "Maya sees that other files live alongside the `*_conflated.tsv` files, and decides to download and investigate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths_positives = catalog.load(\"modelling.raw.ground_truth.positives@spark\")\n",
    "ground_truths_negatives = catalog.load(\"modelling.raw.ground_truth.negatives@spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "By now, Maya has obtained the embeddings and ground truth data. She will now preprocess the data to create the input for her model. She will also need to create splits for cross-validation.\n",
    "\n",
    "Maya will first inspect the ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths_positives_df = ground_truths_positives.toPandas()\n",
    "ground_truths_negatives_df = ground_truths_negatives.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True positives\n",
    "ground_truths_positives_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True negatives\n",
    "ground_truths_negatives_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True positives and true negatives are represented as sets of source-target pairs. `source` is the drug, `target` is the disease.\n",
    "\n",
    "Maya will not be loading entire PySpark dataframe with embedding to memory - before conducting an EDA, she wants to reduce unnecessary columns.\n",
    "\n",
    "She also wants to see what biolink categories the embeddings might have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Get unique values from the \"category\" column\n",
    "unique_categories = embeddings_spark.select(col(\"category\")).distinct()\n",
    "\n",
    "# Collect unique values to a list (will bring data to the driver)\n",
    "unique_values = [row[\"category\"] for row in unique_categories.collect()]\n",
    "\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, Maya (a) removed all pca_embeddings, (b) removed all entities which are not drugs or diseases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define the categories to keep\n",
    "categories_to_keep = {\n",
    "    \"biolink:DiseaseOrPhenotypicFeature\", \n",
    "    \"biolink:Drug\", \n",
    "    \"biolink:Disease\", \n",
    "    \"biolink:BehavioralFeature\", \n",
    "    \"biolink:SmallMolecule\", \n",
    "    \"biolink:PhenotypicFeature\"\n",
    "}\n",
    "\n",
    "# Filter the PySpark DataFrame\n",
    "filtered_df = embeddings_spark.filter(F.col(\"category\").isin(*categories_to_keep)) \\\n",
    "    .select(\"topological_embedding\", \"id\") \\\n",
    "    .na.drop(subset=[\"id\", \"topological_embedding\"])\n",
    "\n",
    "# Convert the filtered PySpark DataFrame to a Pandas DataFrame\n",
    "embeddings_df = filtered_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`filtered_embeddings_path` now contains topological embeddings of drugs and diseases.\n",
    "\n",
    "Maya filters down the ground truths to a simple list of node ids, to create training data for her model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_ids = set(ground_truths_negatives_df[\"target\"].unique()) | set(ground_truths_negatives_df[\"source\"].unique()) | set(ground_truths_positives_df[\"target\"].unique()) | set(ground_truths_positives_df[\"source\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate how many ground truths ids have an embedding\n",
    "\n",
    "len(ground_truth_ids.intersection(embeddings_df[\"id\"].unique())) / len(ground_truth_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tn_filtered = ground_truths_negatives_df[ground_truths_negatives_df[\"target\"].isin(embeddings_df[\"id\"]) & ground_truths_negatives_df[\"source\"].isin(embeddings_df[\"id\"])]\n",
    "df_tp_filtered = ground_truths_positives_df[ground_truths_positives_df[\"target\"].isin(embeddings_df[\"id\"]) & ground_truths_positives_df[\"source\"].isin(embeddings_df[\"id\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many of the ground truth pairs are left\n",
    "(len(df_tn_filtered) + len(df_tp_filtered)) / (len(ground_truths_negatives_df) + len(ground_truths_positives_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maya filtered down the dataset to 3GB from 24GB, and reduced it to only relevant drugs and diseases. 97% of drugs and diseases from the ground truth data are included in the filtered embeddings, which is satisfactory.\n",
    "\n",
    "Now, she can proceed to creating her model. \n",
    "\n",
    "- `embeddings_df` is the filtered embeddings plus node ids\n",
    "- `df_tn_filtered` and `df_tp_filtered` are the ground truth data, filtered down to only include rows with a drug and disease that have an embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for modelling\n",
    "\n",
    "Maya combines the filtered embeddings with the ground truth data to create a dataset for model training. She concatenates true positives and negatives, adding a label column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate true positives and negatives, adding label column\n",
    "df_model = pd.concat([\n",
    "    df_tp_filtered.assign(label=1),\n",
    "    df_tn_filtered.assign(label=0)\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "# Join with embeddings to get source and target embeddings\n",
    "df_model = (\n",
    "    df_model\n",
    "    .merge(\n",
    "        embeddings_df[['id', 'topological_embedding']],\n",
    "        left_on='source',\n",
    "        right_on='id',\n",
    "        how='left'\n",
    "    )\n",
    "    .drop('id', axis=1)\n",
    "    .rename(columns={'topological_embedding': 'source_embedding'})\n",
    "    .merge(\n",
    "        embeddings_df[['id', 'topological_embedding']], \n",
    "        left_on='target',\n",
    "        right_on='id',\n",
    "        how='left'\n",
    "    )\n",
    "    .drop('id', axis=1)\n",
    "    .rename(columns={'topological_embedding': 'target_embedding'})\n",
    ")\n",
    "\n",
    "print(f\"Final dataset shape: {df_model.shape}\")\n",
    "df_model.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataframe with all drug-disease pairs\n",
    "\n",
    "Maya creates a cartesian product of all unique drugs and diseases to generate every possible drug-disease combination that needs a prediction. This creates a comprehensive matrix of all possible pairs, regardless of whether they were in the training data or not.\n",
    "\n",
    "The resulting matrix (shown in the heatmap) allows for easy visualization of predicted relationships across the entire drug-disease space.\n",
    "\n",
    "The code shows that this creates a large number of pairs (number of unique drugs × number of unique diseases), which is why Maya later implements batch processing to handle the predictions efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique diseases from both dataframes\n",
    "all_diseases = pd.concat([\n",
    "    df_tn_filtered[\"target\"],\n",
    "    df_tp_filtered[\"target\"]\n",
    "]).dropna().unique()\n",
    "\n",
    "# Get unique drugs from both dataframes \n",
    "all_drugs = pd.concat([\n",
    "    df_tn_filtered[\"source\"],\n",
    "    df_tp_filtered[\"source\"]\n",
    "]).dropna().unique()\n",
    "\n",
    "print(f\"Number of unique diseases: {len(all_diseases)}\")\n",
    "print(f\"Number of unique drugs: {len(all_drugs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all possible combinations of drugs and diseases\n",
    "all_pairs = pd.DataFrame(\n",
    "    [(drug, disease) for drug in all_drugs for disease in all_diseases],\n",
    "    columns=['source', 'target']\n",
    ")\n",
    "\n",
    "print(f\"Total number of drug-disease pairs: {len(all_pairs):,}\")\n",
    "print(f\"Shape of all pairs dataframe: {all_pairs.shape}\")\n",
    "all_pairs.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare features dataset\n",
    "\n",
    "\n",
    "Maya prepares features by converting embeddings into numpy arrays and concatenating them to form input features for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "\n",
    "def prepare_features(df):\n",
    "    # Convert list embeddings to numpy arrays\n",
    "    source_embeddings = np.vstack(df['source_embedding'].values)\n",
    "    target_embeddings = np.vstack(df['target_embedding'].values)\n",
    "    \n",
    "    # Concatenate the embeddings horizontally\n",
    "    return np.hstack([source_embeddings, target_embeddings])\n",
    "\n",
    "# Prepare features\n",
    "X = prepare_features(df_model)\n",
    "y = df_model['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train / Test split\n",
    "\n",
    "Maya splits the data into training and test sets, ensuring an 80/20 split, and stratifies the data based on labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Maya uses logistic regression for model training, performing cross-validation to evaluate the model's performance on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cross-validation on training data only\n",
    "n_splits = 5\n",
    "cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    ")\n",
    "\n",
    "# Perform cross-validation on training data\n",
    "cv_scores = cross_val_score(\n",
    "    model, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    cv=cv,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"Cross-validation results (on training data):\")\n",
    "print(f\"CV scores: {cv_scores}\")\n",
    "print(f\"Mean AUC-ROC: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on all training data\n",
    "final_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    ")\n",
    "trained_model = final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Maya evaluates the trained model on a held-out test set, calculating the AUC-ROC to assess its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on held-out test set\n",
    "test_predictions = trained_model.predict_proba(X_test)[:, 1]\n",
    "test_auc = roc_auc_score(y_test, test_predictions)\n",
    "print(f\"\\nFinal model performance on test set:\")\n",
    "print(f\"Test AUC-ROC: {test_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce model predictions\n",
    "\n",
    "Maya generates efficacy score predictions for all possible drug-disease pairs using a batching function to handle large data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Maya wants to generate efficacy score predictions for all drug-disease pairs. To avoid fitting all embedidngs in memory, she creates a batching function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_pairs_with_embeddings(all_pairs, embeddings_df, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Generate batches of drug-disease pairs with their embeddings.\n",
    "    \n",
    "    Args:\n",
    "        all_pairs (pd.DataFrame): DataFrame with 'source' and 'target' columns\n",
    "        embeddings_df (pd.DataFrame): DataFrame with 'id' and 'topological_embedding' columns\n",
    "        batch_size (int): Number of pairs to process in each batch\n",
    "        \n",
    "    Yields:\n",
    "        np.array: Array of concatenated source and target embeddings for the batch\n",
    "        pd.DataFrame: Corresponding batch of pairs\n",
    "    \"\"\"\n",
    "    # Create embeddings lookup dictionary for faster access\n",
    "    embeddings_dict = dict(zip(embeddings_df['id'], embeddings_df['topological_embedding']))\n",
    "    \n",
    "    # Process pairs in batches\n",
    "    for start_idx in range(0, len(all_pairs), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(all_pairs))\n",
    "        batch_pairs = all_pairs.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Get embeddings for the batch\n",
    "        source_embeddings = np.vstack([\n",
    "            embeddings_dict[source] for source in batch_pairs['source']\n",
    "        ])\n",
    "        target_embeddings = np.vstack([\n",
    "            embeddings_dict[target] for target in batch_pairs['target']\n",
    "        ])\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        batch_features = np.hstack([source_embeddings, target_embeddings])\n",
    "        \n",
    "        yield batch_features, batch_pairs\n",
    "\n",
    "# Example usage:\n",
    "def predict_all_pairs(model, all_pairs, embeddings_df, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Generate predictions for all drug-disease pairs.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model with predict_proba method\n",
    "        all_pairs (pd.DataFrame): DataFrame with all drug-disease pairs\n",
    "        embeddings_df (pd.DataFrame): DataFrame with embeddings\n",
    "        batch_size (int): Batch size for processing\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Original pairs with prediction scores\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "    all_processed_pairs = []\n",
    "    \n",
    "    for batch_features, batch_pairs in batch_pairs_with_embeddings(all_pairs, embeddings_df, batch_size):\n",
    "        # Get predictions for the batch\n",
    "        batch_predictions = model.predict_proba(batch_features)[:, 1]\n",
    "        \n",
    "        # Store results\n",
    "        batch_results = batch_pairs.copy()\n",
    "        batch_results['prediction_score'] = batch_predictions\n",
    "        all_processed_pairs.append(batch_results)\n",
    "            \n",
    "    # Combine all results\n",
    "    final_results = pd.concat(all_processed_pairs, ignore_index=True)\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "results_df = predict_all_pairs(\n",
    "    model=trained_model,\n",
    "    all_pairs=all_pairs,\n",
    "    embeddings_df=embeddings_df,\n",
    "    batch_size=50000\n",
    ")\n",
    "\n",
    "# View results\n",
    "print(f\"Generated predictions for {len(results_df):,} pairs\")\n",
    "print(\"\\nSample predictions:\")\n",
    "print(results_df.sort_values('prediction_score', ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Prediction Score distributions\n",
    "\n",
    "Maya visualizes the distribution of prediction scores using histograms and density plots to understand the model's output.\n",
    "\n",
    "### Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create figure and axes\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Histogram\n",
    "sns.histplot(\n",
    "    data=test_predictions,\n",
    "    bins=50,\n",
    "    ax=ax1\n",
    ")\n",
    "ax1.set_title('Distribution of Prediction Scores (Histogram)')\n",
    "ax1.set_xlabel('Prediction Score')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "# Separate distributions for positive and negative classes\n",
    "sns.kdeplot(\n",
    "    data=test_predictions[y_test == 1],\n",
    "    ax=ax2,\n",
    "    label='Positive Class',\n",
    "    color='green'\n",
    ")\n",
    "sns.kdeplot(\n",
    "    data=test_predictions[y_test == 0],\n",
    "    ax=ax2,\n",
    "    label='Negative Class',\n",
    "    color='red'\n",
    ")\n",
    "ax2.set_title('Distribution of Prediction Scores by Class (Density)')\n",
    "ax2.set_xlabel('Prediction Score')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some basic statistics\n",
    "print(\"\\nPrediction Score Statistics:\")\n",
    "print(f\"Mean: {test_predictions.mean():.3f}\")\n",
    "print(f\"Median: {np.median(test_predictions):.3f}\")\n",
    "print(f\"Std Dev: {test_predictions.std():.3f}\")\n",
    "print(f\"Min: {test_predictions.min():.3f}\")\n",
    "print(f\"Max: {test_predictions.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Matrix predictions\n",
    "\n",
    "Maya creates a heatmap to visualize prediction scores for a random sample of drug-disease pairs, providing insights into the model's predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Overall distribution of all predictions\n",
    "sns.kdeplot(\n",
    "    data=results_df['prediction_score'],\n",
    "    ax=ax,\n",
    "    label='All Predictions',\n",
    "    color='blue'\n",
    ")\n",
    "\n",
    "ax.set_title('Distribution of Prediction Scores for All Drug-Disease Pairs')\n",
    "ax.set_xlabel('Prediction Score')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print basic statistics for all predictions\n",
    "print(\"\\nPrediction Score Statistics for All Pairs:\")\n",
    "print(f\"Mean: {results_df['prediction_score'].mean():.3f}\")\n",
    "print(f\"Median: {results_df['prediction_score'].median():.3f}\")\n",
    "print(f\"Std Dev: {results_df['prediction_score'].std():.3f}\")\n",
    "print(f\"Min: {results_df['prediction_score'].min():.3f}\")\n",
    "print(f\"Max: {results_df['prediction_score'].max():.3f}\")\n",
    "print(f\"Total number of predictions: {len(results_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Randomly sample 20 drugs and 20 diseases\n",
    "np.random.seed(42)\n",
    "sample_drugs = np.random.choice(all_drugs, size=20, replace=False)\n",
    "sample_diseases = np.random.choice(all_diseases, size=20, replace=False)\n",
    "\n",
    "# Filter results for sampled drugs and diseases\n",
    "sample_results = results_df[\n",
    "    results_df['source'].isin(sample_drugs) & \n",
    "    results_df['target'].isin(sample_diseases)\n",
    "]\n",
    "\n",
    "# Create prediction matrix\n",
    "pred_matrix = sample_results.pivot(\n",
    "    index='source', \n",
    "    columns='target', \n",
    "    values='prediction_score'\n",
    ")\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(\n",
    "    pred_matrix,\n",
    "    cmap='YlOrRd',\n",
    "    center=0.5,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cbar_kws={'label': 'Prediction Score'}\n",
    ")\n",
    "plt.title('Drug-Disease Prediction Scores Heatmap\\n(20 random drugs and diseases)')\n",
    "plt.xlabel('Diseases')\n",
    "plt.ylabel('Drugs')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics about the sampled predictions\n",
    "print(\"\\nSample Prediction Statistics:\")\n",
    "print(f\"Mean: {sample_results['prediction_score'].mean():.3f}\")\n",
    "print(f\"Median: {sample_results['prediction_score'].median():.3f}\")\n",
    "print(f\"Std Dev: {sample_results['prediction_score'].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation conclusion\n",
    "\n",
    "Maya generated a full matrix of drug-disease treatment efficacy scores.\n",
    "\n",
    "We already can see that the model is far from perfect, and it highlights some of the issues our more advanced models has ran into - many drugs and diseases are \"frequent flyers\" with consistently high scores all across the board. She can also see that many too many drugs-disease pairs have treat scored close to 1.\n",
    "\n",
    "However, her model is only a basic logistic regression, and for the sake of this exercise we will not be focusing on improving the results she's obrained. \n",
    "\n",
    "Now Maya will add her new model as Kedro node.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating model with Kedro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that Maya has created and prototyped her model, she wants to integrate it with EveryCure's Matrix repository, train and run it as part of the `matrix` pipeline.\n",
    "\n",
    "### Integration with Kedro Pipeline\n",
    "\n",
    "1. **Pipeline Overview**:\n",
    "   - Data is prepared and preprocessed in `raw`, `kg_raw`, `ingestion`, `integration`, and `embeddings` pipelines.\n",
    "   - These datasets are consumed by downstream `modelling`, `evaluation`, `matrix_generation`, and `inference` pipelines.\n",
    "   - Kedro handles many steps automatically, such as sharding and using ApacheSpark datasets.\n",
    "\n",
    "2. **Model Configuration**:\n",
    "   - Models are injected into the Kedro pipeline via dependency injection mechanism.\n",
    "   - Maya adds her model to `DYNAMIC_PIPELINES_MAPPING` in `pipelines/matrix/src/matrix/settings.py`:\n",
    "\n",
    "   ```python\n",
    "   DYNAMIC_PIPELINES_MAPPING = {\n",
    "       \"modelling\": [\n",
    "           {\"model_name\": \"xg_baseline\", \"num_shards\": 1, \"run_inference\": False},\n",
    "           {\"model_name\": \"xg_ensemble\", \"num_shards\": 3, \"run_inference\": True},\n",
    "           {\"model_name\": \"rf\", \"num_shards\": 1, \"run_inference\": False},\n",
    "           {\"model_name\": \"xg_synth\", \"num_shards\": 1, \"run_inference\": False},\n",
    "           {\"model_name\": \"mayas_logistic_regression\", \"num_shards\": 1, \"run_inference\": False},\n",
    "       ],\n",
    "       \"evaluation\": [\n",
    "   ```\n",
    "\n",
    "3. **Model Parameters**:\n",
    "   - Configure parameters in `pipelines/matrix/conf/base/modelling/parameters/mayas_logistic_regression.yml`:\n",
    "\n",
    "   ```yml\n",
    "   modelling.mayas_logistic_regression:\n",
    "       _overrides:\n",
    "         model_tuning_args:\n",
    "           tuner:\n",
    "             object: matrix.pipelines.modelling.tuning.NopTuner\n",
    "             estimator:\n",
    "               object: sklearn.linear_model.LogisticRegression\n",
    "               random_state: ${globals:random_state}\n",
    "               device: cpu # TODO: Add cuda\n",
    "           features:\n",
    "             - source_+\n",
    "             - target_+\n",
    "           target_col_name: y\n",
    "       model_options: ${merge:${.._model_options},${._overrides}}\n",
    "   ```\n",
    "\n",
    "   - **Key Points**:\n",
    "     - The `estimator` is set to `sklearn.linear_model.LogisticRegression`. If she wanted to use an actual custom model object (one she would define and customise), she could reference it's `sklearn`-compliant class here.\n",
    "     - Uses `NopTuner` for hyperparameter tuning.\n",
    "     - Those parameters get automatically passed and injected into the model.\n",
    "     - Following a similar logic, `modelling.<MODEL_NAME>.model_tuning_args` defines parameters for the estimator object. So  all parameters that `sklearn.linear_model.LogisticRegression` eg. `penalty`, `C`, `class_weight`.  etc. could be passed via the config file. For the full list of parameters, have a look at [official documentation](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html#logisticregression).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Running model\n",
    "\n",
    "Now that the model was added to modelling suite, it will be trained and used to generate matrix when the pipeline is executed. The next section describes how she might do this in a notebook to test out the model first. This will then be compatible with the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom model\n",
    "\n",
    "Maya is now interested in creating a custom model that takes the dot product of embeddings and applies a softmax activation. This is her own novel idea that she wants to test out. Since this is a custom model, she will need to take a look in `pipelines/matrix/src/matrix/pipelines/modelling/model.py` and define her own model class.\n",
    "\n",
    "Maya codes this up into a class called `SumSoftmaxClassifier`. This is compatable with the existing pipeline infrastructure leveraging the `sklearn` interface.\n",
    "\n",
    "We can now use this as we would any other model by including it in the modelling pipeline and config file as above, replaceing logistic regression with her new model e.g. `mayas_softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kedro.io import DataCatalog\n",
    "from kedro.runner import SequentialRunner\n",
    "from matrix.pipelines.modelling.nodes import (\n",
    "    attach_embeddings,\n",
    "    make_splits,\n",
    "    train_model,\n",
    "    create_model,\n",
    "    get_model_predictions,\n",
    "    check_model_performance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the embeddings using the kedro catalog. \n",
    "We already did this earlier in the notebook, so to save time, we will reuse those in this example. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Look at embeddings we previously created\n",
    "embeddings_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need the training data to create splits- again we will reuse the same data we used previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the original df_model structure but rename 'label' to 'y'\n",
    "df_model_prepared = df_model.rename(columns={'label': 'y'})\n",
    "\n",
    "df_model_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next sections will use the pipeline nodes to train the model, test and evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create splits\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "splitter = StratifiedShuffleSplit(\n",
    "    n_splits=1,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create splits using existing df_model\n",
    "splits = make_splits(\n",
    "    data=df_model_prepared,\n",
    "    splitter=splitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from matrix.pipelines.modelling import model\n",
    "reload(model)\n",
    "from matrix.pipelines.modelling.model import DotProductSoftmaxClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train the SumSoftmaxClassifier\n",
    "model = DotProductSoftmaxClassifier(random_state=42)\n",
    "\n",
    "# The model expects concatenated embeddings\n",
    "features = ['source_embedding', 'target_embedding']\n",
    "trained_model = train_model(\n",
    "    data=splits,\n",
    "    estimator=model,\n",
    "    features=features,\n",
    "    target_col_name='y'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final model using create_model function\n",
    "final_model = create_model(trained_model)  # This wraps it in ModelWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Get predictions\n",
    "predictions = get_model_predictions(\n",
    "    data=splits,\n",
    "    model=trained_model,\n",
    "    features=features,\n",
    "    target_col_name='y'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Check performance\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from matrix.pipelines.modelling.utils import partial_\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': accuracy_score,\n",
    "    'macro_f1': partial_(f1_score, average='macro')\n",
    "}\n",
    "\n",
    "# Check performance\n",
    "performance = check_model_performance(\n",
    "    data=predictions,\n",
    "    metrics=metrics,\n",
    "    target_col_name='y'\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel Performance:\")\n",
    "for metric, value in performance.items():\n",
    "    print(f\"{metric}: {value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the model is not good, but this is to be expected since it was a simple dot product between embeddings just to show how to integrate a custom model. \n",
    "For this to work in the pipeline, we can create a custom pipeline node that takes the embeddings and applies the softmax function. This should follow scikit-learn's interface methods.\n",
    "\n",
    "For example, within the `pipelines/matrix/src/matrix/pipelines/modelling/model.py` file, we can add a new class:\n",
    "\n",
    "```python\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "\n",
    "class DotProductSoftmaxClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"A classifier that uses dot product between source and target embeddings.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, random_state: Optional[int] = None):\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _prepare_features(self, X):\n",
    "        \"\"\"Convert embeddings from DataFrame or array to numpy array.\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # Handle PySpark DataFrame conversion to numpy\n",
    "            if hasattr(X, 'toPandas'):\n",
    "                X = X.toPandas()\n",
    "            \n",
    "            # Convert source embeddings\n",
    "            if 'source_embedding' in X.columns:\n",
    "                source_embeddings = np.vstack([\n",
    "                    np.array(x) if isinstance(x, list) else x \n",
    "                    for x in X['source_embedding']\n",
    "                ])\n",
    "                target_embeddings = np.vstack([\n",
    "                    np.array(x) if isinstance(x, list) else x \n",
    "                    for x in X['target_embedding']\n",
    "                ])\n",
    "            else:\n",
    "                # Assume columns are already split into individual features\n",
    "                source_cols = [col for col in X.columns if col.startswith('source_')]\n",
    "                target_cols = [col for col in X.columns if col.startswith('target_')]\n",
    "                source_embeddings = X[source_cols].values\n",
    "                target_embeddings = X[target_cols].values\n",
    "                \n",
    "            return np.hstack([source_embeddings, target_embeddings])\n",
    "        elif isinstance(X, np.ndarray) and X.dtype == object:\n",
    "            n_samples = X.shape[0]\n",
    "            source_embeddings = np.vstack([\n",
    "                np.array(x) if isinstance(x, list) else x \n",
    "                for x in X[:, 0]\n",
    "            ])\n",
    "            target_embeddings = np.vstack([\n",
    "                np.array(x) if isinstance(x, list) else x \n",
    "                for x in X[:, 1]\n",
    "            ])\n",
    "            return np.hstack([source_embeddings, target_embeddings])\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model.\"\"\"\n",
    "        X = self._prepare_features(X)\n",
    "        self.n_classes_ = len(np.unique(y))\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.embedding_dim_ = X.shape[1] // 2\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Compute probabilities using dot product and softmax.\"\"\"\n",
    "        X = self._prepare_features(X)\n",
    "        \n",
    "        # Split features into source and target embeddings\n",
    "        source_embeddings = X[:, :self.embedding_dim_]\n",
    "        target_embeddings = X[:, self.embedding_dim_:]\n",
    "        \n",
    "        # Compute dot product\n",
    "        dot_products = np.sum(source_embeddings * target_embeddings, axis=1)\n",
    "        \n",
    "        # Reshape to (n_samples, 2) for binary classification\n",
    "        scores = np.column_stack([-dot_products, dot_products])\n",
    "        \n",
    "        # Apply softmax\n",
    "        probs = softmax(scores, axis=1)\n",
    "        \n",
    "        return probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(probs, axis=1)]\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, we can also configure parameters in `pipelines/matrix/conf/base/modelling/parameters/mayas_logistic_regression.yml`:\n",
    "\n",
    "```yml\n",
    "   modelling.mayas_softmax:\n",
    "    _overrides:\n",
    "\n",
    "      model_tuning_args:\n",
    "        tuner:\n",
    "          object: matrix.pipelines.modelling.tuning.NopTuner\n",
    "          estimator:\n",
    "            object: matrix.pipelines.modelling.model.DotProductSoftmaxClassifier\n",
    "            random_state: ${globals:random_state}\n",
    "        features: # Features use regex, source_0, source_1, .., target_0, target_1\n",
    "          - source_+\n",
    "          - target_+\n",
    "        target_col_name: y\n",
    "      \n",
    "    # NOTE: This leverages the custom `merge` resolver to override\n",
    "    # the base options with the override block above.\n",
    "    model_options: ${merge:${.._model_options},${._overrides}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Models are injected into the Kedro pipeline via dependency injection mechanism.\n",
    "   - Maya adds her model to `DYNAMIC_PIPELINES_MAPPING` in `pipelines/matrix/src/matrix/settings.py`:\n",
    "\n",
    "   ```python\n",
    "   DYNAMIC_PIPELINES_MAPPING = {\n",
    "       \"modelling\": [\n",
    "           {\"model_name\": \"xg_baseline\", \"num_shards\": 1, \"run_inference\": False},\n",
    "           {\"model_name\": \"xg_ensemble\", \"num_shards\": 3, \"run_inference\": True},\n",
    "           {\"model_name\": \"rf\", \"num_shards\": 1, \"run_inference\": False},\n",
    "           {\"model_name\": \"xg_synth\", \"num_shards\": 1, \"run_inference\": False},\n",
    "           {\"model_name\": \"mayas_softmax\", \"num_shards\": 1, \"run_inference\": False},\n",
    "       ],\n",
    "       \"evaluation\": [\n",
    "   ```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can then be run as part of the pipeline. For example to test: \n",
    "\n",
    "```bash\n",
    "kedro run --env test -p test\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
