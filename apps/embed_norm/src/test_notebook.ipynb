{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "from pathlib import Path\n",
    "\n",
    "utils_path = os.path.abspath('/home/wadmin/embed_norm/apps/embed_norm/src')\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "# root_path = subprocess.check_output(['git', 'rev-parse', '--show-toplevel']).decode().strip()\n",
    "# os.chdir(Path(root_path) / 'pipelines' / 'matrix')\n",
    "\n",
    "# configure_logging()\n",
    "# utils_path = Path.cwd()\n",
    "# if utils_path not in sys.path:\n",
    "#     sys.path.append(utils_path)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import functions/classes from main.py\n",
    "from test import (\n",
    "    configure_logging,\n",
    "    setup_environment,\n",
    "    CacheManager,\n",
    "    Config,\n",
    "    DataLoader,\n",
    "    Normalizer,\n",
    "    EmbeddingGenerator,\n",
    "    LLMEnhancer,\n",
    "    load_data,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_logging()\n",
    "setup_environment(utils_path=utils_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path.cwd().parents[1]\n",
    "\n",
    "# Set up variables\n",
    "cache_dir = Path(root_path) / \"apps\" / \"embed_norm\" / \"cached_datasets\"\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "logging.info(f\"Cache directory set at '{cache_dir}'.\")\n",
    "\n",
    "for subdir in [\"embeddings\", \"datasets\"]:\n",
    "    subdir_path = cache_dir / subdir\n",
    "    subdir_path.mkdir(parents=True, exist_ok=True)\n",
    "    logging.info(f\"Subdirectory '{subdir}' created at '{subdir_path}'.\")\n",
    "\n",
    "pos_seed = 54321\n",
    "neg_seed = 67890\n",
    "dataset_name = \"rtx_kg2.int\"\n",
    "nodes_dataset_name = \"integration.int.rtx.nodes\"\n",
    "edges_dataset_name = \"integration.int.rtx.edges\"\n",
    "categories = [\"All Categories\"]\n",
    "model_names = [\"OpenAI\", \"PubMedBERT\", \"SapBERT\", \"BlueBERT\", \"BioBERT\"]\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    logging.error(\"OPENAI_API_KEY environment variable is not set.\")\n",
    "    sys.exit(1)\n",
    "openai.api_key = openai_api_key\n",
    "logging.info(\"OpenAI API key is set.\")\n",
    "\n",
    "total_sample_size = 1000\n",
    "positive_ratio = 0.2\n",
    "positive_n = int(total_sample_size * positive_ratio)\n",
    "negative_n = total_sample_size - positive_n\n",
    "cache_suffix = f\"_pos_{positive_n}_neg_{negative_n}\"\n",
    "\n",
    "config = Config(\n",
    "    cache_dir=cache_dir,\n",
    "    pos_seed=pos_seed,\n",
    "    neg_seed=neg_seed,\n",
    "    dataset_name=dataset_name,\n",
    "    nodes_dataset_name=nodes_dataset_name,\n",
    "    edges_dataset_name=edges_dataset_name,\n",
    "    categories=categories,\n",
    "    model_names=model_names,\n",
    "    total_sample_size=total_sample_size,\n",
    "    positive_ratio=positive_ratio,\n",
    "    positive_n=positive_n,\n",
    "    negative_n=negative_n,\n",
    "    cache_suffix=cache_suffix,\n",
    "    use_llm_enhancement=False  # Set to True to enable LLM enhancement\n",
    ")\n",
    "logging.info(\"Configuration variables are set.\")\n",
    "\n",
    "cache_manager = CacheManager(config.cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories, positive_datasets, negative_datasets, nodes_df = load_data(config, cache_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(nodes_df.head())\n",
    "\n",
    "# 1. Count missing values per column\n",
    "missing_counts = nodes_df.isnull().sum()\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# 2. Identify columns with unhashable types\n",
    "def is_column_unhashable(col):\n",
    "    try:\n",
    "        # Attempt to hash the first non-null entry\n",
    "        sample = col.dropna().iloc[0]\n",
    "        hash(sample)\n",
    "        return False\n",
    "    except TypeError:\n",
    "        return True\n",
    "    except IndexError:\n",
    "        # Column is entirely NaN\n",
    "        return False\n",
    "\n",
    "# Identify unhashable columns\n",
    "unhashable_columns = [col for col in nodes_df.columns if is_column_unhashable(nodes_df[col])]\n",
    "\n",
    "print(\"\\nColumns with unhashable types:\", unhashable_columns)\n",
    "\n",
    "# 3. Handle unhashable columns by converting them to tuples\n",
    "for col in unhashable_columns:\n",
    "    nodes_df[col] = nodes_df[col].apply(lambda x: tuple(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "\n",
    "print(\"\\nConverted unhashable columns to tuples.\")\n",
    "\n",
    "# 4. Now, count unique rows\n",
    "unique_rows_count = nodes_df.drop_duplicates().shape[0]\n",
    "print(f\"\\nTotal number of unique rows: {unique_rows_count}\")\n",
    "\n",
    "# 5. Number of rows per category\n",
    "rows_per_category = nodes_df['category'].value_counts(dropna=False)\n",
    "print(\"\\nNumber of rows per category:\")\n",
    "print(rows_per_category)\n",
    "\n",
    "# 6. Missing values per column for each category\n",
    "missing_counts_per_category = nodes_df.groupby('category').apply(lambda x: x.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values per column for each category:\")\n",
    "print(missing_counts_per_category)\n",
    "\n",
    "# (Optional) Improved readability\n",
    "# for category, group in nodes_df.groupby('category'):\n",
    "#     print(f\"\\nCategory: {category}\")\n",
    "#     missing = group.isnull().sum()\n",
    "#     print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.use_llm_enhancement:\n",
    "    text_fields = ['llm_enhanced_text']\n",
    "else:\n",
    "    text_fields = ['name', 'category', 'labels', 'all_categories']\n",
    "\n",
    "embedding_generator = EmbeddingGenerator(config, cache_manager)\n",
    "embeddings_dict_all_models = embedding_generator.process_models(\n",
    "    model_names=config.model_names,\n",
    "    positive_datasets=positive_datasets,\n",
    "    negative_datasets=negative_datasets,\n",
    "    seed=config.neg_seed,\n",
    "    text_fields=text_fields,\n",
    "    label_generation_func=None,\n",
    "    dataset_name=config.dataset_name,\n",
    "    use_ontogpt=False,\n",
    "    cache_suffix=config.cache_suffix,\n",
    "    use_combinations=False,\n",
    "    combine_fields=False,\n",
    ")\n",
    "logging.info(\"Embeddings for models processed successfully using process_models().\")\n",
    "embeddings_dict = embeddings_dict_all_models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
