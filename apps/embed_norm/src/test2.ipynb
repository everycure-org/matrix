{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = catalog.load(\"integration.int.rtx.nodes\")\n",
    "edges = catalog.load(\"integration.int.rtx.edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Load the TSV file into a Polars DataFrame\n",
    "df = pl.read_csv(\"/home/wadmin/edges_c.tsv\", separator='\\t')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print column names\n",
    "print(df.columns)\n",
    "\n",
    "# Print the first 5 rows\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "for index, row in enumerate(df.iter_rows(named=True)):\n",
    "    if index == 5:\n",
    "        break\n",
    "    print(f\"{index} **************** {index}\")\n",
    "    for column in df.columns:\n",
    "        print(f\"{column}: {row[column]}\")\n",
    "    print(f\"{index} **************** {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create dictionaries to store unique entries and their counts\n",
    "unique_entries = {}\n",
    "empty_counts = {}\n",
    "\n",
    "# Iterate through each column in the DataFrame with a progress bar\n",
    "for column in tqdm(df.columns, desc=\"Processing columns\"):\n",
    "    # Get unique entries and their counts\n",
    "    unique_entries[column] = df[column].unique().to_list()\n",
    "    unique_count = df[column].n_unique()\n",
    "    # Count total rows with an empty column\n",
    "    empty_count = df[column].is_null().sum()\n",
    "    # Print the results\n",
    "    tqdm.write(f\"Column: {column}\")\n",
    "    tqdm.write(f\"Unique Entries ({unique_count}): {unique_entries[column]}\")\n",
    "    tqdm.write(f\"Empty Count: {empty_count}\\n\")\n",
    "    # Store the counts in the dictionary\n",
    "    empty_counts[column] = empty_count\n",
    "\n",
    "# Combine the results into a single dictionary\n",
    "output = {\n",
    "    \"unique_entries\": unique_entries,\n",
    "    \"empty_counts\": empty_counts\n",
    "}\n",
    "\n",
    "# Save the output to a JSON file\n",
    "with open(\"output.json\", \"w\") as f:\n",
    "    json.dump(output, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "utils_path = os.path.abspath('/home/wadmin/embed_norm/apps/embed_norm/src')\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "from main import Environment, CacheManager, Config, Pipeline\n",
    "\n",
    "Environment.configure_logging()\n",
    "# utils_path = Path(__file__).parent.resolve()\n",
    "Environment.setup_environment(utils_path=utils_path)\n",
    "\n",
    "project_path = Path.cwd().parents[2]\n",
    "cache_dir = project_path / \"apps\" / \"embed_norm\" / \"cached_datasets\"\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "for subdir in [\"embeddings\", \"datasets\"]:\n",
    "    (cache_dir / subdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pos_seed = 54321\n",
    "neg_seed = 67890\n",
    "dataset_name = \"rtx_kg2.int\"\n",
    "nodes_dataset_name = \"integration.int.rtx.nodes\"\n",
    "edges_dataset_name = \"integration.int.rtx.edges\"\n",
    "categories = [\"All Categories\"]\n",
    "model_names = [\"OpenAI\", \"PubMedBERT\", \"BioBERT\", \"BlueBERT\", \"SapBERT\"]\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    sys.exit(1)\n",
    "\n",
    "total_sample_size = 1000\n",
    "positive_ratio = 0.2\n",
    "positive_n = int(total_sample_size * positive_ratio)\n",
    "negative_n = total_sample_size - positive_n\n",
    "cache_suffix = f\"_pos_{positive_n}_neg_{negative_n}\"\n",
    "\n",
    "config = Config(\n",
    "    cache_dir=cache_dir,\n",
    "    pos_seed=pos_seed,\n",
    "    neg_seed=neg_seed,\n",
    "    dataset_name=dataset_name,\n",
    "    nodes_dataset_name=nodes_dataset_name,\n",
    "    edges_dataset_name=edges_dataset_name,\n",
    "    categories=categories,\n",
    "    model_names=model_names,\n",
    "    total_sample_size=total_sample_size,\n",
    "    positive_ratio=positive_ratio,\n",
    "    positive_n=positive_n,\n",
    "    negative_n=negative_n,\n",
    "    cache_suffix=cache_suffix,\n",
    ")\n",
    "\n",
    "cache_manager = CacheManager(cache_dir=cache_dir)\n",
    "pipeline = Pipeline(config=config, cache_manager=cache_manager, package_name=\"matrix\", project_path=project_path)\n",
    "categories, positive_datasets, negative_datasets, nodes_df = pipeline.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nodes DataFrame Columns:\", list(nodes_df.columns))\n",
    "print(\"\\nSample of Nodes DataFrame:\")\n",
    "print(nodes_df.head())\n",
    "\n",
    "print(\"\\nMissing values per column in Nodes DataFrame:\")\n",
    "print(nodes_df.isnull().sum())\n",
    "\n",
    "print(\"\\nUnique Categories:\")\n",
    "print(nodes_df['category'].unique())\n",
    "\n",
    "rows_per_category = nodes_df['category'].value_counts(dropna=False)\n",
    "print(\"\\nNumber of rows per category:\")\n",
    "print(rows_per_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPositive Datasets Categories:\")\n",
    "for cat, df in positive_datasets.items():\n",
    "    print(f\"Category: {cat}, Shape: {df.shape}\")\n",
    "\n",
    "print(\"\\nNegative Datasets Categories:\")\n",
    "for cat, df in negative_datasets.items():\n",
    "    print(f\"Category: {cat}, Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_columns(datasets):\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        print(f\"\\nDataset: {dataset_name}\")\n",
    "        print(\"Columns:\", list(dataset.columns))\n",
    "\n",
    "print(\"\\nPositive Datasets Columns:\")\n",
    "display_columns(positive_datasets)\n",
    "\n",
    "print(\"\\nNegative Datasets Columns:\")\n",
    "display_columns(negative_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_labels_sample(datasets, sample_size=5):\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        if \"labels\" in dataset.columns:\n",
    "            print(f\"\\nDataset: {dataset_name}\")\n",
    "            print(\"Labels Sample:\", dataset['labels'].head(sample_size).to_list())\n",
    "\n",
    "print(\"\\nPositive Datasets Labels Sample:\")\n",
    "display_labels_sample(positive_datasets)\n",
    "\n",
    "print(\"\\nNegative Datasets Labels Sample:\")\n",
    "display_labels_sample(negative_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(nodes_df.head())\n",
    "\n",
    "# 1. Count missing values per column\n",
    "missing_counts = nodes_df.isnull().sum()\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# 2. Identify columns with unhashable types\n",
    "def is_column_unhashable(col):\n",
    "    try:\n",
    "        # Attempt to hash the first non-null entry\n",
    "        sample = col.dropna().iloc[0]\n",
    "        hash(sample)\n",
    "        return False\n",
    "    except TypeError:\n",
    "        return True\n",
    "    except IndexError:\n",
    "        # Column is entirely NaN\n",
    "        return False\n",
    "\n",
    "# Identify unhashable columns\n",
    "unhashable_columns = [col for col in nodes_df.columns if is_column_unhashable(nodes_df[col])]\n",
    "\n",
    "print(\"\\nColumns with unhashable types:\", unhashable_columns)\n",
    "\n",
    "# 3. Handle unhashable columns by converting them to tuples\n",
    "for col in unhashable_columns:\n",
    "    nodes_df[col] = nodes_df[col].apply(lambda x: tuple(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "\n",
    "print(\"\\nConverted unhashable columns to tuples.\")\n",
    "\n",
    "# 4. Now, count unique rows\n",
    "unique_rows_count = nodes_df.drop_duplicates().shape[0]\n",
    "print(f\"\\nTotal number of unique rows: {unique_rows_count}\")\n",
    "\n",
    "# 5. Number of rows per category\n",
    "rows_per_category = nodes_df['category'].value_counts(dropna=False)\n",
    "print(\"\\nNumber of rows per category:\")\n",
    "print(rows_per_category)\n",
    "\n",
    "# 6. Missing values per column for each category\n",
    "missing_counts_per_category = nodes_df.groupby('category').apply(lambda x: x.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values per column for each category:\")\n",
    "print(missing_counts_per_category)\n",
    "\n",
    "# (Optional) Improved readability\n",
    "# for category, group in nodes_df.groupby('category'):\n",
    "#     print(f\"\\nCategory: {category}\")\n",
    "#     missing = group.isnull().sum()\n",
    "#     print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Function to parse edge_text into a list of edge dictionaries\n",
    "def parse_edge_text(edge_text):\n",
    "    edges = []\n",
    "    if not edge_text:\n",
    "        return edges\n",
    "    # Pattern to match each edge component\n",
    "    pattern = r'has_edge_to:(.*?)_relation:(.*?)_props:(\\{.*?\\})'\n",
    "    matches = re.findall(pattern, edge_text)\n",
    "    for match in matches:\n",
    "        obj, predicate, props_json = match\n",
    "        try:\n",
    "            props = json.loads(props_json)\n",
    "        except json.JSONDecodeError:\n",
    "            props = {}\n",
    "        edges.append({\n",
    "            'object': obj,\n",
    "            'predicate': predicate,\n",
    "            'properties': props\n",
    "        })\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Parse edge_text for all nodes\n",
    "nodes_df['parsed_edges'] = nodes_df['edge_text'].apply(parse_edge_text)\n",
    "\n",
    "# Function to extract all edges from the DataFrame\n",
    "def extract_all_edges(df):\n",
    "    all_edges = []\n",
    "    for _, row in df.iterrows():\n",
    "        node_id = row['id']\n",
    "        for edge in row['parsed_edges']:\n",
    "            all_edges.append({\n",
    "                'subject': node_id,\n",
    "                'object': edge['object'],\n",
    "                'predicate': edge['predicate'],\n",
    "                'properties': edge['properties']\n",
    "            })\n",
    "    return pd.DataFrame(all_edges)\n",
    "\n",
    "# Extract edges from positive and negative datasets\n",
    "def get_edges_from_datasets(datasets, all_nodes_df):\n",
    "    edges = []\n",
    "    for category, df in datasets.items():\n",
    "        subset_nodes = all_nodes_df[all_nodes_df['id'].isin(df['id'])]\n",
    "        edges_df = extract_all_edges(subset_nodes)\n",
    "        edges.append(edges_df)\n",
    "    if edges:\n",
    "        return pd.concat(edges, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['subject', 'object', 'predicate', 'properties'])\n",
    "\n",
    "positive_edges = get_edges_from_datasets(positive_datasets, nodes_df)\n",
    "negative_edges = get_edges_from_datasets(negative_datasets, nodes_df)\n",
    "\n",
    "print(f\"Total positive edges: {positive_edges.shape[0]}\")\n",
    "print(f\"Total negative edges: {negative_edges.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample edges for positive datasets\n",
    "print(\"Positive Edges Sample:\")\n",
    "print(positive_edges.head())\n",
    "\n",
    "# Display sample edges for negative datasets\n",
    "print(\"Negative Edges Sample:\")\n",
    "print(negative_edges.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predicates in positive edges\n",
    "positive_predicate_counts = positive_edges['predicate'].value_counts()\n",
    "print(\"\\nPredicate counts in positive edges:\")\n",
    "print(positive_predicate_counts)\n",
    "\n",
    "# Analyze predicates in negative edges\n",
    "negative_predicate_counts = negative_edges['predicate'].value_counts()\n",
    "print(\"\\nPredicate counts in negative edges:\")\n",
    "print(negative_predicate_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 10 predicates in positive edges\n",
    "top_n = 10\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=positive_predicate_counts.head(top_n).values, y=positive_predicate_counts.head(top_n).index, palette=\"viridis\")\n",
    "plt.title(f\"Top {top_n} Predicates in Positive Edges\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Predicate\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 10 predicates in negative edges\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=negative_predicate_counts.head(top_n).values, y=negative_predicate_counts.head(top_n).index, palette=\"magma\")\n",
    "plt.title(f\"Top {top_n} Predicates in Negative Edges\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Predicate\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze number of edges per node in positive datasets\n",
    "positive_edges_per_node = positive_edges['subject'].value_counts()\n",
    "\n",
    "# Analyze number of edges per node in negative datasets\n",
    "negative_edges_per_node = negative_edges['subject'].value_counts()\n",
    "\n",
    "print(f\"\\nAverage number of edges per node in positive datasets: {positive_edges_per_node.mean():.2f}\")\n",
    "print(f\"Average number of edges per node in negative datasets: {negative_edges_per_node.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of number of edges per node in positive datasets\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(positive_edges_per_node, bins=30, kde=False, color='blue')\n",
    "plt.title(\"Distribution of Number of Edges per Node (Positive Datasets)\")\n",
    "plt.xlabel(\"Number of Edges\")\n",
    "plt.ylabel(\"Number of Nodes\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot distribution of number of edges per node in negative datasets\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(negative_edges_per_node, bins=30, kde=False, color='red')\n",
    "plt.title(\"Distribution of Number of Edges per Node (Negative Datasets)\")\n",
    "plt.xlabel(\"Number of Edges\")\n",
    "plt.ylabel(\"Number of Nodes\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze properties in positive edges\n",
    "def extract_property_counts(edges_df, property_key):\n",
    "    property_values = edges_df['properties'].apply(lambda x: x.get(property_key, None)).dropna()\n",
    "    # Flatten the list if values are lists\n",
    "    flattened = [item for sublist in property_values for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "    return pd.Series(flattened).value_counts()\n",
    "\n",
    "# Example: Analyze a specific property, e.g., 'strength'\n",
    "# Replace 'strength' with actual property keys present in your data\n",
    "property_key = 'strength'\n",
    "positive_property_counts = extract_property_counts(positive_edges, property_key)\n",
    "negative_property_counts = extract_property_counts(negative_edges, property_key)\n",
    "\n",
    "print(f\"\\nProperty '{property_key}' counts in positive edges:\")\n",
    "print(positive_property_counts.head(10))\n",
    "\n",
    "print(f\"\\nProperty '{property_key}' counts in negative edges:\")\n",
    "print(negative_property_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are properties to analyze, plot them\n",
    "if not positive_property_counts.empty:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=positive_property_counts.head(top_n).values, y=positive_property_counts.head(top_n).index, palette=\"coolwarm\")\n",
    "    plt.title(f\"Top {top_n} '{property_key}' Properties in Positive Edges\")\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.ylabel(property_key.capitalize())\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if not negative_property_counts.empty:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=negative_property_counts.head(top_n).values, y=negative_property_counts.head(top_n).index, palette=\"inferno\")\n",
    "    plt.title(f\"Top {top_n} '{property_key}' Properties in Negative Edges\")\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.ylabel(property_key.capitalize())\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (matrix)",
   "language": "python",
   "name": "kedro_matrix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
