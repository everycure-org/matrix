spark.sql.execution.arrow.pyspark.enabled: true
spark.scheduler.mode: FAIR
spark.driver.memory: 32g
# only using this due to 
# https://github.com/fsspec/gcsfs/issues/632
spark.driver.maxResultSize: 12g

# FUTURE: Use Maven based dependency when fixed
# https://github.com/GoogleCloudDataproc/hadoop-connectors/issues/1233#issuecomment-2262873434
spark.jars: gcs-connector-hadoop3-2.2.2-shaded.jar
spark.jars.packages: com.google.cloud.spark:spark-3.5-bigquery:0.39.0,org.neo4j:neo4j-connector-apache-spark_2.12:5.3.0_for_spark_3,org.xerial:sqlite-jdbc:3.47.0.0
spark.hadoop.fs.gs.impl: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem

# for controlling the parallelism in Spark to avoid OOM
spark.executor.instances: 4
spark.executor.cores: 2
spark.dynamicAllocation.enabled: false
# Amount of memory to use per executor process
spark.executor.memory: 32g
# https://spark.apache.org/docs/latest/tuning.html
# The higher this is, the less working memory may be available to execution and tasks may spill to disk more often.