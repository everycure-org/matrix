_neo4j_ds: &_neo4j_ds
  type: matrix.datasets.neo4j.Neo4JSparkDataset
  database: analytics
  url: ${globals:neo4j.host}
  credentials: neo4j_credentials
  save_args:
    mode: "overwrite"
  metadata:
    kedro-viz:
      layer: embeddings

_spark_parquet_ds: &_spark_parquet
  type: matrix.datasets.gcp.LazySparkDataset
  file_format: parquet
  save_args:
    mode: overwrite
  metadata:
    kedro-viz:
      layer: embeddings


# -------------------------------------------------------------------------
# Datasets
# -------------------------------------------------------------------------

# Dataset to feed node embedding computation
embeddings.prm.graph_nodes:
  <<: *_neo4j_ds
  save_args:
    # NOTE: Neo4j nodes can have multiple labels. We're adding _all_ nodes in a global
    # `entity` label on which we generate a unique constraint. This has the side-effect
    # that an index is created which allows for quick edge inserts down the line.
    # https://neo4j.com/docs/cypher-manual/current/constraints/
    mode: overwrite
    # script: >
    #   CREATE CONSTRAINT IF NOT EXISTS FOR (n:Entity) REQUIRE n.id IS UNIQUE;
    query: > 
      CREATE (n:Entity {id: event.id, kg_sources: event.kg_sources})
      WITH event, n
      CALL apoc.create.addLabels(n, [event.label]) YIELD node
      CALL apoc.create.setProperties(n, event.property_keys, event.property_values) YIELD node AS n2
      RETURN node
  load_args:
    # NOTE: The 'biolink' prefixes was removed to have a clean Neo4J UI interface. We're adding
    # it back into the query as the modelling expects it.
    query: >  
      MATCH (n) 
      RETURN 
        n.id AS id, 
        n.name as name,
        n.category as category,
        n.description as description

# Catalog directly expresses a query to add OpenAI embeddings.
# https://neo4j.com/labs/apoc/5/installation/
# https://neo4j.com/labs/apoc/5/ml/vertexai/#_generate_embeddings_api
embeddings.prm.graph.embeddings@yaml:
  type: yaml.YAMLDataset
  filepath: ${globals:paths.prm}/embeddings/result.yml

embeddings.prm.graph.embeddings@neo:
  <<: *_neo4j_ds
  load_args:
    partitions: 16
    labels: ":Entity"

embeddings.feat.graph.node_embeddings:
  <<: *_spark_parquet
  filepath: ${globals:paths.raw}/embeddings/tmp_nodes_with_embeddings

embeddings.feat.graph.pca_node_embeddings:
  <<: *_spark_parquet
  filepath: ${globals:paths.raw}/embeddings/tmp_nodes_with_pca_embeddings

# NOTE: Needs renaming
embeddings.tmp.input_nodes:
  <<: [*_neo4j_ds]
  save_args:
    # NOTE: Neo4j nodes can have multiple labels. We're adding _all_ nodes in a global
    # `entity` label on which we generate a unique constraint. This has the side-effect
    # that an index is created which allows for quick edge inserts down the line.
    # https://neo4j.com/docs/cypher-manual/current/constraints/
    mode: overwrite
    script: >
      CREATE CONSTRAINT IF NOT EXISTS FOR (n:Entity) REQUIRE n.id IS UNIQUE;
    query: > 
      CREATE (n:Entity {id: event.id, pca_embedding: event.pca_embedding, category: event.category, kg_sources: event.kg_sources})
      WITH event, n
      CALL apoc.create.addLabels(n, event.labels) YIELD node
      RETURN node

# NOTE: Needs renaming
embeddings.tmp.input_edges:
  <<: [*_neo4j_ds]
  save_args:
    # NOTE: The `match:Entity` ensures that the index we created above is leveraged
    # while inserting edges, thereby speeding up the process massively.
    query: > 
      MATCH (subject:Entity {id: event.subject}), (object:Entity {id: event.object})
      WITH subject, object, event
      CALL apoc.create.relationship(subject, event.label, {kg_sources: event.kg_sources}, object) YIELD rel
      RETURN rel

embeddings.feat.include_in_graphsage@yaml:
  filepath: ${globals:paths.feat}/graphsage_filter/result.yml
  type:  yaml.YAMLDataset

embeddings.models.graphsage:
  # we do not actually do anything with this DS, thus it's a dummy
  # https://github.com/kedro-org/kedro/discussions/3758
  filepath: ${globals:paths.prm}/graphsage/result.yml
  type:  yaml.YAMLDataset

# NOTE: Dummy catalog entry to enforce Kedro dependency
# between embedding and modelling pipeline.
embeddings.model_output.graphsage:
  <<: *_neo4j_ds
  save_args:
    # Dummy Query that never gets written to because we use GDS to execute the write-back of graphsage
    persist: false
  load_args:
    # NOTE: Using the `query` reader option highly impacts performance, hence resorting to `labels`.
    # https://neo4j.com/docs/spark/current/read/options/
    partitions: 16 # Should increase parallelism
    labels: ":Entity" # avoids query which drastically increases performance

embeddings.feat.nodes:
  <<: *_spark_parquet
  filepath: ${globals:paths.feat}/nodes
      

embeddings.reporting.loss:
  type: kedro_mlflow.io.artifacts.MlflowArtifactDataset
  artifact_path: topological
  dataset:
    # NOTE: Data needs to be stored locally for MLFlow
    # to be able to upload to GCS. The ML Tracking server
    # is setup to not serve artifacts, thereby interacting
    # directly with GCS as a result.
    type: kedro_datasets.matplotlib.MatplotlibWriter
    filepath: ${globals:paths.tmp}/convergence_plot.png


embeddings.feat.edges:
  <<: *_spark_parquet
  filepath: ${globals:paths.feat}/edges

embeddings.reporting.topological_pca:
  <<: *_spark_parquet
  filepath: ${globals:paths.reporting}/topological_pca

embeddings.reporting.topological_pca_plot:
  type: kedro_mlflow.io.artifacts.MlflowArtifactDataset
  artifact_path: topological
  dataset:
    # NOTE: Data needs to be stored locally for MLFlow
    # to be able to upload to GCS. The ML Tracking server
    # is setup to not serve artifacts, thereby interacting
    # directly with GCS as a result.
    type: kedro_datasets.matplotlib.MatplotlibWriter
    filepath: ${globals:paths.tmp}/pca_plot.png