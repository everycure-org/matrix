_neo4j_ds: &_neo4j_ds
  type: matrix.datasets.neo4j.Neo4JSparkDataset
  database: analytics
  url: ${globals:neo4j.host}
  credentials: neo4j_credentials
  save_args:
    mode: "overwrite"
  metadata:
    kedro-viz:
      layer: embeddings

_spark_parquet_ds: &_spark_parquet
  type: matrix.datasets.gcp.LazySparkDataset
  file_format: parquet
  save_args:
    mode: overwrite
  metadata:
    kedro-viz:
      layer: embeddings
      
# -------------------------------------------------------------------------
# Datasets
# -------------------------------------------------------------------------

embeddings.feat.graph.node_embeddings@spark:
  <<: *_spark_parquet
  filepath: ${globals:paths.tmp}/feat/tmp_nodes_with_embeddings

embeddings.feat.graph.pca_node_embeddings:
  <<: *_spark_parquet
  filepath: ${globals:paths.embeddings}/feat/tmp_nodes_with_pca_embeddings

embeddings.feat.graph.edges_for_topological:
  <<: *_spark_parquet
  filepath: ${globals:paths.embeddings}/feat/edges_for_topological
  
# NOTE: Needs renaming
embeddings.tmp.input_nodes:
  <<: [*_neo4j_ds]
  save_args:
    # NOTE: Neo4j nodes can have multiple labels. We're adding _all_ nodes in a global
    # `entity` label on which we generate a unique constraint. This has the side-effect
    # that an index is created which allows for quick edge inserts down the line.
    # https://neo4j.com/docs/cypher-manual/current/constraints/
    mode: overwrite
    script: >
      CREATE CONSTRAINT IF NOT EXISTS FOR (n:Entity) REQUIRE n.id IS UNIQUE;
    query: > 
      CREATE (n:Entity {id: event.id, pca_embedding: event.pca_embedding})
      WITH event, n
      CALL apoc.create.addLabels(n, event.labels) YIELD node
      RETURN node

# NOTE: Needs renaming
embeddings.tmp.input_edges:
  <<: [*_neo4j_ds]
  save_args:
    # NOTE: The `match:Entity` ensures that the index we created above is leveraged
    # while inserting edges, thereby speeding up the process massively.
    query: >
      MATCH (subject:Entity {id: event.subject}), (object:Entity {id: event.object})
      WITH subject, object, event
      CALL apoc.create.relationship(subject, event.label, {}, object) YIELD rel
      RETURN rel

embeddings.models.topological:
  # we do not actually do anything with this DS, thus it's a dummy
  # https://github.com/kedro-org/kedro/discussions/3758
  filepath: ${globals:paths.embeddings}/models/topological_result.yml
  type:  yaml.YAMLDataset

# NOTE: Dummy catalog entry to enforce Kedro dependency
# between embedding and modelling pipeline.
embeddings.model_output.topological:
  <<: *_neo4j_ds
  save_args:
    # Dummy Query that never gets written to because we use GDS to execute the write-back of graphsage
    persist: false
  load_args:
    # NOTE: Using the `query` reader option highly impacts performance, hence resorting to `labels`.
    # https://neo4j.com/docs/spark/current/read/options/
    partitions: 16 # Should increase parallelism
    labels: ":Entity" # avoids query which drastically increases performance

embeddings.feat.nodes:
  <<: *_spark_parquet
  filepath: ${globals:paths.embeddings}/feat/nodes_with_embeddings

embeddings.reporting.loss:
  type: kedro_datasets.matplotlib.MatplotlibWriter
  filepath: ${globals:paths.embeddings}/reporting/convergence_plot.png

embeddings.reporting.topological_pca:
  <<: *_spark_parquet
  filepath: ${globals:paths.embeddings}/reporting/topological_pca

embeddings.reporting.topological_pca_plot:
  type: kedro_datasets.matplotlib.MatplotlibWriter
  filepath: ${globals:paths.embeddings}/reporting/pca_plot.png


# cache:
embeddings.node.cache.read: &cache_in
  type: matrix.datasets.gcp.SparkWithSchemaDataset
  filepath: &cache_location ${globals:paths.cache}/embeddings/node/cache
  provide_empty_if_not_present: true
  load_args:
    schema:
      _object: pyspark.sql.types.StructType
      fields:
        - _object: &_field pyspark.sql.types.StructField
          name: key
          dataType:
            _object: &_stringtype pyspark.sql.types.StringType
          nullable: False
        - _object: *_field
          name: value
          dataType:
            _object: pyspark.sql.types.ArrayType
            elementType:
              _object: pyspark.sql.types.FloatType
          nullable: False
        - _object: *_field
          name: api
          dataType:
            _object: *_stringtype
          nullable: False
embeddings.node.cache.write:
  type: matrix.datasets.gcp.PartitionedAsyncParallelDataset
  path: *cache_location
  dataset: kedro_datasets.pandas.ParquetDataset
  timeout: 600  # if the batch size increases, you'll likely need to increase this too, as most APIs simply take longer to process larger batches.
  filename_suffix: ".parquet"
  max_workers: 20  # Affects the number of concurrent threads, so more = faster. But more also implies more memory needed, as more transformed batches are returned, prior to writing.
embeddings.node.cache.reload:  # an easy way to force Kedro to reload a dataset that has been altered in a previous node, but which is not using the same reference (e.g. due to transcription)
  <<: *cache_in
embeddings.node.cache.misses:
  <<: *_spark_parquet
  filepath: ${globals:paths.tmp}/embeddings/node/cache_misses

