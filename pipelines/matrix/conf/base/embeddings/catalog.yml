_neo4j_ds: &_neo4j_ds
  type: matrix.datasets.neo4j.Neo4JSparkDataset
  database: analytics
  url: ${globals:neo4j.host}
  credentials: neo4j_credentials
  save_args:
    mode: "overwrite"
  metadata:
    kedro-viz:
      layer: embeddings

_spark_parquet_ds: &_spark_parquet
  type: matrix.datasets.gcp.LazySparkDataset
  file_format: parquet
  save_args:
    mode: overwrite
  metadata:
    kedro-viz:
      layer: embeddings
      
# -------------------------------------------------------------------------
# Datasets
# -------------------------------------------------------------------------


embeddings.feat.graph.node_embeddings@partitioned:
  type: matrix.datasets.gcp.PartitionedTQDMDataset
  path: ${globals:paths.embeddings}/feat/tmp_nodes_with_embeddings
  overwrite: True
  dataset: 
    <<: *_spark_parquet
  filename_suffix: ".parquet"

embeddings.feat.graph.node_embeddings@spark:
  <<: *_spark_parquet
  filepath: ${globals:paths.embeddings}/feat/tmp_nodes_with_embeddings # Why did I call this tmp?


embeddings.feat.graph.pca_node_embeddings:
  <<: *_spark_parquet
  filepath: ${globals:paths.embeddings}/feat/tmp_nodes_with_pca_embeddings
  
# NOTE: Needs renaming
embeddings.tmp.input_nodes:
  <<: [*_neo4j_ds]
  save_args:
    # NOTE: Neo4j nodes can have multiple labels. We're adding _all_ nodes in a global
    # `entity` label on which we generate a unique constraint. This has the side-effect
    # that an index is created which allows for quick edge inserts down the line.
    # https://neo4j.com/docs/cypher-manual/current/constraints/
    mode: overwrite
    script: >
      CREATE CONSTRAINT IF NOT EXISTS FOR (n:Entity) REQUIRE n.id IS UNIQUE;
    query: > 
      CREATE (n:Entity {id: event.id, pca_embedding: event.pca_embedding, category: event.category, kg_sources: event.kg_sources})
      WITH event, n
      CALL apoc.create.addLabels(n, event.labels) YIELD node
      RETURN node

# NOTE: Needs renaming
embeddings.tmp.input_edges:
  <<: [*_neo4j_ds]
  save_args:
    # NOTE: The `match:Entity` ensures that the index we created above is leveraged
    # while inserting edges, thereby speeding up the process massively.
    query: >
      MATCH (subject:Entity {id: event.subject}), (object:Entity {id: event.object})
      WITH subject, object, event
      CALL apoc.create.relationship(subject, event.label, {kg_sources: event.kg_sources}, object) YIELD rel
      RETURN rel

embeddings.feat.include_in_graphsage@yaml:
  filepath: ${globals:paths.embeddings}/feat/graphsage_filter_result.yml
  type:  yaml.YAMLDataset

embeddings.models.graphsage:
  # we do not actually do anything with this DS, thus it's a dummy
  # https://github.com/kedro-org/kedro/discussions/3758
  filepath: ${globals:paths.embeddings}/models/graphsage_result.yml
  type:  yaml.YAMLDataset

# NOTE: Dummy catalog entry to enforce Kedro dependency
# between embedding and modelling pipeline.
embeddings.model_output.graphsage:
  <<: *_neo4j_ds
  save_args:
    # Dummy Query that never gets written to because we use GDS to execute the write-back of graphsage
    persist: false
  load_args:
    # NOTE: Using the `query` reader option highly impacts performance, hence resorting to `labels`.
    # https://neo4j.com/docs/spark/current/read/options/
    partitions: 16 # Should increase parallelism
    labels: ":Entity" # avoids query which drastically increases performance

embeddings.feat.nodes:
  <<: *_spark_parquet
  filepath: ${globals:paths.embeddings}/feat/nodes_with_embeddings

embeddings.reporting.loss:
  type: kedro_datasets.matplotlib.MatplotlibWriter
  filepath: ${globals:paths.embeddings}/reporting/convergence_plot.png

embeddings.reporting.topological_pca:
  <<: *_spark_parquet
  filepath: ${globals:paths.embeddings}/reporting/topological_pca

embeddings.reporting.topological_pca_plot:
  type: kedro_datasets.matplotlib.MatplotlibWriter
  filepath: ${globals:paths.embeddings}/reporting/pca_plot.png