# Neo4J graph database configuration. Used to orchestrate
# queries into Neo4j.
embeddings.gdb:
  object: matrix.pipelines.embeddings.nodes.GraphDB
  endpoint: ${globals:neo4j.host}
  database: analytics
  auth:
    -  ${globals:neo4j.user}
    -  ${globals:neo4j.password}

# Establishes connection to localized mock GenAI API for
# testing purposes.
# https://neo4j.com/labs/apoc/5/ml/openai/
embeddings.ai_config:
  api_key: ${globals:openai.api_key}
# https://platform.openai.com/docs/guides/embeddings/faq
# OpenAI embeddings support batching, up to 8000 tokens.
  batch_size: 200
  endpoint: ${globals:openai.endpoint}
  model: text-embedding-3-small
  attribute: &_property embedding

# Attributes used as input features to compute
# node embeddings.
embeddings.node.features: ["category", "name"]

# Defines strategy used to reduce dimensions of the GenAI
# generated node embeddings.
# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.PCA.html
embeddings.dimensionality_reduction:
  skip: false # Set to true to skip PCA
  input: *_property
  output: &_pca_property 'pca_embedding'
  transformer:
    object: pyspark.ml.feature.PCA
    k: 100

# FUTURE: This is a highly temp. solution that serves as PoC of leveraging Neo4J's
# AI functionality. If works as expected, we'll do a refinement step to clean this up.
embeddings.gds:
  object: matrix.pipelines.embeddings.nodes.GraphDS
  endpoint: ${globals:neo4j.host}
  database: analytics
  auth:
    -  ${globals:neo4j.user}
    -  ${globals:neo4j.password}

# Defines strategy used to compute topological embeddings,
# all nodes and relationships in our KG are projected, along with
# the PCA feature. The graphSage algorithm is hereafter applied
# and the result is written back to the KG.
# 
# Sources:
#   - https://neo4j.com/docs/graph-data-science/current/management-ops/graph-creation/graph-project/
#   - https://neo4j.com/docs/graph-data-science-client/current/graph-object/
#   - https://neo4j.com/docs/graph-data-science-client/current/model-object/
#   - https://neo4j.com/docs/graph-data-science-client/current/algorithms/ 
# 
embeddings.topological:
  # Configuration to perform Neo4J graph
  # projection into GDS library for algorithm
  # execution.
  projection:
    graphName: embeddings
    nodeProjection:
      Entity:
        label: 'Entity'
        properties: *_pca_property
    relationshipProjection: '*'
    configuration:
      relationshipProperties: 
        include_in_graphsage:
          property: 'include_in_graphsage'
          defaultValue: 1
  
  # Configuration to apply filtering on projected GDS 
  # graph, ensuring various edges are removed
  # prior to model training.
  filtering:
    graphName: filtered_embeddings 
    args:
      node_filter: '*'
      relationship_filter: r.include_in_graphsage = 1.0
  
  # Name of the model in Neo4j
  estimator:
    modelName: topological_embeddings
  
  write_property: &_topological_property 'topological_embedding'

# uncomment for node2vec
# embeddings.topological_estimator:
#   object: matrix.pipelines.embeddings.graph_algorithms.GDSNode2Vec 
#   concurrency: 4
#   embedding_dim: 512
#   random_seed: 42
#   iterations: 100
#   in_out_factor: 1
#   return_factor: 1
#   initial_learning_rate: 0.025
#   min_learning_rate: 0.000025
#   negative_sampling_rate: 5
#   walk_length: 30
#   walks_per_node: 10
#   window_size: 100

# embeddings.topological_estimator:
#   object: matrix.pipelines.embeddings.graph_algorithms.PygNode2Vec
#   embedding_dim: 512
#   walk_length: 30
#   walks_per_node: 10
#   q: 1.0
#   p: 1.0
#   context_size: 10
#   concurrency: 4
#   num_negative_samples: 1
#   num_workers: 0
#   epochs: 10
#   batch_size: 128
#   random_seed: 42
#   learning_rate: 0.01
#   optimizer: Adam
#   sparse: False

embeddings.topological_estimator:
  object: matrix.pipelines.embeddings.graph_algorithms.PygGraphSAGE
  embedding_dim: 512
  num_layers: 2
  hidden_channels: 256
  num_neighbors: [25, 10]
  concurrency: 4
  num_workers: 0
  epochs: 10
  batch_size: 128
  random_seed: 42
  learning_rate: 0.01
  optimizer: Adam
  aggregator: mean
  dropout: 0.0
  neg_sampling_ratio: 1.
  criterion: 
    object: matrix.pipelines.embeddings.torch_utils.BCE_contrastive_loss

# embeddings.topological_estimator:
#   object: matrix.pipelines.embeddings.graph_algorithms.GDSGraphSage
#   concurrency: 4
#   iterations: 100
#   sample_sizes: [25, 10]
#   tolerance: 1e-8
#   embedding_dim: 512
#   batch_size: 5000
#   epochs: 10
#   search_depth: 100
#   learning_rate: 0.01
#   activation_function: ReLu
#   random_seed: 42
#   feature_properties: [*_pca_property]
#   # negative_sampling_weight: 20

embeddings.write_topological_col: *_topological_property

embeddings.topological_pca:
  skip: false # Set to true to skip PCA
  input: *_topological_property
  output: 'pca_embedding'
  transformer:
    object: pyspark.ml.feature.PCA
    k: 2
