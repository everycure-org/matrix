# Defines strategy used to reduce dimensions of the node embeddings
# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.PCA.html
embeddings.dimensionality_reduction:
  skip: false
  input: embedding
  output: &_pca_property 'pca_embedding'
  transformer:
    _object: pyspark.ml.feature.PCA
    k: 100

# FUTURE: This is a highly temp. solution that serves as PoC of leveraging Neo4J's
# AI functionality. If works as expected, we'll do a refinement step to clean this up.
embeddings.gds:
  _object: matrix.datasets.neo4j.GraphDS
  endpoint: ${globals:neo4j.host}
  database: analytics
  auth:
    -  ${globals:neo4j.user}
    -  ${globals:neo4j.password}

# Defines strategy used to compute topological embeddings,
# all nodes and relationships in our KG are projected, along with
# the PCA feature. The graphSage algorithm is hereafter applied
# and the result is written back to the KG.
# 
# Sources:
#   - https://neo4j.com/docs/graph-data-science/current/management-ops/graph-creation/graph-project/
#   - https://neo4j.com/docs/graph-data-science-client/current/graph-object/
#   - https://neo4j.com/docs/graph-data-science-client/current/model-object/
#   - https://neo4j.com/docs/graph-data-science-client/current/algorithms/ 
# 
embeddings.topological:
  # Configuration to perform Neo4J graph
  # projection into GDS library for algorithm
  # execution.
  projection:
    graphName: embeddings
    nodeProjection:
      Entity:
        label: 'Entity'
        properties: *_pca_property
    relationshipProjection: '*'
  
  # Name of the model in Neo4j
  estimator:
    modelName: topological_embeddings
  
  write_property: &_topological_property 'topological_embedding'

# uncomment for node2vec
embeddings.topological_estimator:
  _object: matrix.pipelines.embeddings.graph_algorithms.GDSNode2Vec 
  concurrency: 63 # we run this on n2d-standard-64 machines.
  embedding_dim: 512
  random_seed: 42
  iterations: 10
  in_out_factor: 1
  return_factor: 1
  initial_learning_rate: 0.025
  min_learning_rate: 0.000025
  negative_sampling_rate: 5
  walk_length: 30
  walks_per_node: 10
  window_size: 10

# embeddings.topological_estimator:
#   _object: matrix.pipelines.embeddings.graph_algorithms.GDSGraphSage
#   concurrency: 4
#   iterations: 100
#   sample_sizes: [25, 10]
#   tolerance: 1e-8
#   embedding_dim: 512
#   batch_size: 5000
#   epochs: 10
#   search_depth: 100
#   learning_rate: 0.01
#   activation_function: ReLu
#   random_seed: 42
#   feature_properties: [*_pca_property]
#   # negative_sampling_weight: 20

embeddings.write_topological_col: *_topological_property

embeddings.topological_pca:
  skip: false # Set to true to skip PCA
  input: *_topological_property
  output: 'pca_embedding'
  transformer:
    _object: pyspark.ml.feature.PCA
    k: 2

embeddings.node:
  cache_schema:
    _object: matrix.pipelines.batch.schemas.embeddings
  preprocessor:
    _object: matrix.pipelines.embeddings.nodes.embeddings_preprocessor
    key_length: 100
    combine_cols: ["name", "category"]
    new_col: &pkey text_to_embed
  resolver:
    _object: matrix.pipelines.embeddings.encoders.LangChainEncoder
    dimensions: 512
    encoder:
      _object: langchain_openai.OpenAIEmbeddings
      model: text-embedding-3-small
      timeout: 10
  primary_key: *pkey
  target_col: embedding
  batch_size: 40000  # Bigger batch sizes result in larger Parquet files. +-250MB/file is a good rule of thumb on object stores like GCS. However, a bigger batch size also means you need to keep more elements in memory, so too big of a size will result in Out-Of-Memory errors.
