_spark_parquet_ds: &_spark_parquet
  type: matrix.datasets.gcp.LazySparkDataset
  file_format: parquet
  save_args:
    mode: overwrite
  metadata:
    kedro-viz:
      layer: embeddings

_pandas_parquet: &_pandas_parquet
  type: matrix.datasets.graph.PandasParquetDataset
  save_args:
    engine: pyarrow
  load_args:
    engine: pyarrow
    as_type: str

# -------------------------------------------------------------------------
# Datasets
# -------------------------------------------------------------------------

batch.int.{source}.input_bucketized@spark:
  <<: *_spark_parquet
  filepath: ${globals:paths.tmp}/batch/{source}/bucketized
  save_args:
    mode: overwrite
    partitionBy:
      - bucket

batch.int.{source}.input_bucketized@partitioned:
  type: matrix.datasets.gcp.PartitionedAsyncParallelDataset
  path: ${globals:paths.tmp}/batch/{source}/bucketized
  dataset: 
    <<: *_pandas_parquet
  filename_suffix: ".parquet"

batch.int.{source}.{workers}.input_transformed@partitioned:
  type: matrix.datasets.gcp.PartitionedAsyncParallelDataset
  overwrite: True # NOTE: Very important to ensure every run clear out previous partitions
  max_workers: "{workers}" # Used to enable API parallelization (note that filepath is not affected by this))
  path: ${globals:paths.tmp}/batch/{source}/transformed
  dataset: 
    <<: *_pandas_parquet
  filename_suffix: ".parquet"

batch.int.{source}.{workers}.input_transformed@spark:
  <<: *_spark_parquet
  filepath: ${globals:paths.tmp}/batch/{source}/transformed

# cache:
batch.{source}.cache.read: &cache_in
  type: matrix.datasets.gcp.LazySparkDataset
  filepath: &cache_location ${globals:paths.cache}/{source}
  provide_empty_if_not_present: true

batch.{source}.{workers}.cache.write:
  type: matrix.datasets.gcp.PartitionedAsyncParallelDataset
  path: *cache_location
  dataset: kedro_datasets.pandas.ParquetDataset
  timeout: 600  # if the batch size increases, you'll likely need to increase this too, as most APIs simply take longer to process larger batches.
  filename_suffix: ".parquet"
  max_workers: "{workers}"  # Affects the number of concurrent threads, so more = faster. But more also implies more memory needed, as more transformed batches are returned, prior to writing.

batch.{source}.cache.reload:  # an easy way to force Kedro to reload a dataset that has been altered in a previous node, but which is not using the same reference (e.g. due to transcription)
  <<: *cache_in

batch.{source}.cache.misses:
  <<: *_spark_parquet
  filepath:  ${globals:paths.cache}/tmp/cache_misses/{source}
