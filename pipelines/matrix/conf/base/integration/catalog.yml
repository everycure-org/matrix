_spark_parquet_ds: &_spark_parquet
  type: matrix.datasets.gcp.LazySparkDataset
  file_format: parquet
  load_args:
    header: True
  save_args:
    mode: overwrite

_pandas_parquet: &_pandas_parquet
  type: matrix.datasets.graph.PandasParquetDataset
  save_args:
    engine: pyarrow
  load_args:
    engine: pyarrow

_layer_int: &_layer_int
  metadata:
    kedro-viz:
      layer: integration

_layer_prm: &_layer_prm
  metadata:
    kedro-viz:
      layer: primary

# -------------------------------------------------------------------------
# Datasets
# -------------------------------------------------------------------------

integration.int.{source}.nodes:
  <<: [*_spark_parquet, *_layer_int]
  filepath: ${globals:paths.integration}/int/{source}/nodes

integration.int.{source}.edges:
  <<: [*_spark_parquet, *_layer_int]
  filepath: ${globals:paths.integration}/int/{source}/edges

# Normalized KGs
integration.int.{source}.nodes.norm@spark:
  <<: [*_spark_parquet, *_layer_int]
  filepath: ${globals:paths.integration}/int/{source}/nodes.norm

integration.int.{source}.nodes.norm@pandas:
  <<: [*_pandas_parquet, *_layer_int]
  filepath: ${globals:paths.integration}/int/{source}/nodes.norm

integration.int.{source}.edges.norm@spark:
  <<: [*_spark_parquet, *_layer_int]
  filepath: ${globals:paths.integration}/int/{source}/edges.norm

integration.int.{source}.edges.norm@pandas:
  <<: [*_pandas_parquet, *_layer_int]
  filepath: ${globals:paths.integration}/int/{source}/edges.norm

# mapping tables 
integration.int.{source}.nodes_norm_mapping:
  <<: [*_spark_parquet, *_layer_int]
  filepath: ${globals:paths.integration}/int/{source}/nodes_norm_mapping

# Unified KGs
integration.prm.unified_nodes:
  <<: [*_spark_parquet, *_layer_prm]
  filepath: ${globals:paths.integration}/prm/unified/nodes

integration.prm.unified_edges:
  <<: [*_spark_parquet, *_layer_prm]
  filepath: ${globals:paths.integration}/prm/unified/edges

# ==============================
# Original sources: only used for sampling

# The create_sample pipeline needs to pull data from the release, but write data in the sample environment
# We need to define another entry in the base catalog, otherwise the sample environment integration.prm.unified_nodes will reference itself

integration.prm.original.unified_nodes:
  <<: [*_spark_parquet]
  filepath: ${globals:paths.integration}/prm/unified/nodes

integration.prm.original.unified_edges:
  <<: [*_spark_parquet]
  filepath: ${globals:paths.integration}/prm/unified/edges

# Caching strategy
integration.cache.read: &cache_in
  type: matrix.datasets.gcp.SparkWithSchemaDataset
  filepath: &cache_location ${globals:paths.cache}/integration/cache/normalizer=NCATSNodeNormalizer/
  provide_empty_if_not_present: true
  load_args:
    schema:
      _object: pyspark.sql.types.StructType
      fields:
        - _object: &_field pyspark.sql.types.StructField
          name: key
          dataType:
            _object: &_stringtype pyspark.sql.types.StringType
          nullable: False
        - _object: *_field
          name: value
          dataType:
            _object: pyspark.sql.types.ArrayType
            elementType:
              _object: *_stringtype
          nullable: False
        - _object: *_field
          name: api
          dataType:
            _object: *_stringtype
          nullable: False
integration.{source}.cache.write:
    type: matrix.datasets.gcp.PartitionedAsyncParallelDataset
    path: *cache_location
    dataset: kedro_datasets.pandas.ParquetDataset
    timeout: 600  # if the batch size increases, you'll likely need to increase this too, as most APIs simply take longer to process larger batches.
    filename_suffix: ".parquet"
    max_workers: 20  # Affects the number of concurrent threads, so more = faster. But more also implies more memory needed, as more transformed batches are returned, prior to writing.
integration.{source}.cache.reload:  # an easy way to force Kedro to reload a dataset that has been altered in a previous node, but which is not using the same reference (e.g. due to transcription)
    <<: *cache_in
integration.int.{source}.cache_misses:
  <<: [*_spark_parquet, *_layer_int]
  filepath: ${globals:paths.cache}/integration/cache_misses/{source}_cache_misses
  save_args:
    mode: overwrite