_spark_parquet_ds: &_spark_parquet
  type: matrix.datasets.gcp.LazySparkDataset
  file_format: parquet
  load_args:
    header: True
  save_args:
    mode: overwrite

_pandas_parquet: &_pandas_parquet
  type: matrix.datasets.graph.PandasParquetDataset
  save_args:
    engine: pyarrow
  load_args:
    engine: pyarrow

_layer_int: &_layer_int
  metadata:
    kedro-viz:
      layer: integration

_layer_prm: &_layer_prm
  metadata:
    kedro-viz:
      layer: primary

# -------------------------------------------------------------------------
# Datasets
# -------------------------------------------------------------------------

integration.int.{source}.nodes:
  <<: [*_spark_parquet, *_layer_int]
  filepath: ${globals:paths.integration}/int/{source}/nodes

integration.int.{source}.edges:
  <<: [*_spark_parquet, *_layer_int]
  filepath: ${globals:paths.integration}/int/{source}/edges

# Normalized KGs
integration.int.{source}.nodes.norm@spark:
  <<: [*_spark_parquet, *_layer_int]
  filepath: ${globals:paths.integration}/int/{source}/nodes.norm

integration.int.{source}.nodes.norm@pandas:
  <<: [*_pandas_parquet, *_layer_int]
  filepath: ${globals:paths.integration}/int/{source}/nodes.norm

integration.int.{source}.edges.norm@spark:
  <<: [*_spark_parquet, *_layer_int]
  filepath: ${globals:paths.integration}/int/{source}/edges.norm

integration.int.{source}.edges.norm@pandas:
  <<: [*_pandas_parquet, *_layer_int]
  filepath: ${globals:paths.integration}/int/{source}/edges.norm

# Caching strategy
integration.cache.read: &cache_in
  type: matrix.datasets.gcp.SparkWithSchemaDataset
  filepath: &cache_location ${globals:paths.cache}/integration/normalizer=NCATSNodeNormalizer/
  provide_empty_if_not_present: true
  load_args:
    schema:
      _object: pyspark.sql.types.StructType
      fields:
        - _object: &_field pyspark.sql.types.StructField
          name: key
          dataType:
            _object: &_stringtype pyspark.sql.types.StringType
          nullable: False
        - _object: *_field
          name: value
          dataType:
            _object: *_stringtype
          nullable: False
        - _object: *_field
          name: api
          dataType:
            _object: *_stringtype
          nullable: False
integration.cache.write:
    type: matrix.datasets.gcp.PartitionedAsyncParallelDataset
    path: *cache_location
    dataset: kedro_datasets.pandas.ParquetDataset
    timeout: 600  # if the batch size increases, you'll likely need to increase this too, as most APIs simply take longer to process larger batches.
    filename_suffix: ".parquet"
    max_workers: 20  # Affects the number of concurrent threads, so more = faster. But more also implies more memory needed, as more transformed batches are returned, prior to writing.
integration.cache.reload:  # an easy way to force Kedro to reload a dataset that has been altered in a previous node, but which is not using the same reference (e.g. due to transcription)
    <<: *cache_in
integration.int.{source}.cache_misses:
  <<: [*_spark_parquet, *_layer_int]
  filepath: ${globals:path.tmp}/cache_misses/{source}/
  save_args:
    mode: overwrite

# mapping tables 
integration.int.{source}.nodes_norm_mapping:
  <<: [*_spark_parquet, *_layer_int]
  filepath: ${globals:paths.integration}/int/{source}/nodes_norm_mapping

# Unified KGs
integration.prm.unified_nodes:
  <<: [*_spark_parquet, *_layer_prm]
  filepath: ${globals:paths.integration}/prm/unified/nodes

integration.prm.unified_edges:
  <<: [*_spark_parquet, *_layer_prm]
  filepath: ${globals:paths.integration}/prm/unified/edges

# filtered KGs
integration.prm.filtered_edges:
  <<: [*_spark_parquet, *_layer_prm]
  filepath: ${globals:paths.integration}/prm/filtered/edges

# FUTURE more elegant: Apply all node filters, then all edge filters, then a "cleanup" final step that removes
# 1. all edges where nodes were deleted
# 2. all nodes that have not one edge connected anymore
integration.prm.prefiltered_nodes:
  <<: [*_spark_parquet, *_layer_prm]
  filepath: ${globals:paths.integration}/prm/prefiltered/nodes

integration.prm.filtered_nodes:
  <<: [*_spark_parquet, *_layer_prm]
  filepath: ${globals:paths.integration}/prm/filtered/nodes


# ==============================
# Original sources: only used for sampling

integration.prm.original.filtered_nodes:
  <<: [*_spark_parquet]
  filepath: ${globals:paths.integration}/prm/filtered/nodes
  
integration.prm.original.filtered_edges:
  <<: [*_spark_parquet]
  filepath: ${globals:paths.integration}/prm/filtered/edges