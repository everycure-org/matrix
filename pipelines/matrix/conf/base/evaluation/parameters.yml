# Name of column representing main probability score of the model 
# Column name for main probability score
evaluation.score_col_name: &score-col "treat score"

# Threshold-based classification metrics for ground truth data
evaluation.simple_ground_truth_classification:
  evaluation_options:
    generator:
      object: matrix.datasets.pair_generator.GroundTruthTestPairs
      positive_columns: 
        - "is_known_positive"
      negative_columns:
        - "is_known_negative"
    evaluation:
      object: matrix.pipelines.evaluation.evaluation.DiscreteMetrics 
      metrics:
        - object: sklearn.metrics.accuracy_score
        - object: sklearn.metrics.f1_score
      score_col_name: *score-col
      threshold: 0.5

# Disease-specific ranking
evaluation.disease_specific_ranking:
  evaluation_options:
    generator:
      object: matrix.datasets.pair_generator.MatrixTestDiseases
      positive_columns: 
        - "is_known_positive"
      removal_columns:
        - "trial_sig_better"
        - "trial_non_sig_better"
    evaluation:
      object: matrix.pipelines.evaluation.evaluation.SpecificRanking 
      specific_col: "target"
      rank_func_lst: 
        - object: matrix.pipelines.evaluation.evaluation.MRR
        - object:  matrix.pipelines.evaluation.evaluation.HitK
          k: 2
        - object:  matrix.pipelines.evaluation.evaluation.HitK
          k: 10
        - object:  matrix.pipelines.evaluation.evaluation.HitK
          k: 100
      score_col_name: *score-col

# Full matrix ranking 
evaluation.full_matrix:
  evaluation_options:
    generator:
      object: matrix.datasets.pair_generator.FullMatrixPositives
      positive_columns: 
        - "is_known_positive"
    evaluation:
      object: matrix.pipelines.evaluation.evaluation.FullMatrixRanking 
      rank_func_lst: 
        - object:  matrix.pipelines.evaluation.evaluation.RecallAtNFunc # TODO: Rename after moving to own file 
          n: 2
        - object:  matrix.pipelines.evaluation.evaluation.RecallAtNFunc
          n: 5
        - object:  matrix.pipelines.evaluation.evaluation.RecallAtNFunc
          n: 10
      quantile_func_lst: 
        - object: matrix.pipelines.evaluation.evaluation.AUROC

# Time split - TODO: rename evaluation to trials and write-up explanation

evaluation.simple_ground_truth_classification_time_split:
  evaluation_options:
    generator:
      object: matrix.datasets.pair_generator.GroundTruthTestPairs
      positive_columns: 
        - "trial_sig_better"
        - "trial_non_sig_better"
      negative_columns:
        - "trial_sig_worse"
        - "trial_non_sig_worse"
    evaluation:
      object: matrix.pipelines.evaluation.evaluation.DiscreteMetrics
      metrics:
        - object: sklearn.metrics.accuracy_score
        - object: sklearn.metrics.f1_score
      score_col_name: *score-col
      threshold: 0.5

evaluation.disease_specific_ranking_time_split:
  evaluation_options:
    generator:
      object: matrix.datasets.pair_generator.MatrixTestDiseases
      positive_columns: 
        - "trial_sig_better"
        - "trial_non_sig_better"
      removal_columns: 
        - "is_known_positive"
    evaluation:
      object: matrix.pipelines.evaluation.evaluation.SpecificRanking
      specific_col: "target"
      rank_func_lst: 
        - object: matrix.pipelines.evaluation.evaluation.MRR
        - object:  matrix.pipelines.evaluation.evaluation.HitK
          k: 2
        - object:  matrix.pipelines.evaluation.evaluation.HitK
          k: 10
        - object:  matrix.pipelines.evaluation.evaluation.HitK
          k: 100
      score_col_name: *score-col


# Other metrics

# # Threshold-independent metrics for ground truth data
# evaluation.continuous_ground_truth_classification:
#   evaluation_options:
#     generator:
#       object: matrix.datasets.pair_generator.GroundTruthTestPairs
#       positive_columns: 
#         - "is_known_positive"
#       negative_columns:
#         - "is_known_negative"
#     evaluation:
#       object: matrix.pipelines.evaluation.evaluation.ContinuousMetrics 
#       metrics:
#         - object: sklearn.metrics.roc_auc_score
#         - object: sklearn.metrics.average_precision_score
#       score_col_name: *score-col


# # AUROC with all drugs x test diseases matrix
# evaluation.disease_centric_matrix:
#   evaluation_options:
#     generator:
#       object: matrix.datasets.pair_generator.MatrixTestDiseases
#       positive_columns: 
#         - "is_known_positive"
#     evaluation:
#       object: matrix.pipelines.evaluation.evaluation.ContinuousMetrics 
#       metrics:
#         - object: sklearn.metrics.roc_auc_score
#         - object: sklearn.metrics.average_precision_score
#       score_col_name: *score-col

# # Recall@N
# evaluation.recall_at_n:
#   evaluation_options:
#     generator:
#       object: matrix.datasets.pair_generator.MatrixTestDiseases
#       positive_columns: 
#         - "is_known_positive"
#     evaluation:
#       object: matrix.pipelines.evaluation.evaluation.RecallAtN
#       n_values: [2, 5, 10]
#       score_col_name: *score-col

