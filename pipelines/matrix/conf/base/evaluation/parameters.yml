# Base options (ground truth classification)
evaluation.simple_ground_truth_classification:
  evaluation_options:
    generator:
      object: matrix.datasets.pair_generator.GroundTruthTestPairs
    evaluation:
      object: matrix.pipelines.evaluation.evaluation.DiscreteMetrics 
      metrics:
        - object: sklearn.metrics.accuracy_score
        - object: sklearn.metrics.f1_score
      threshold: 0.5

evaluation.continuous_ground_truth_classification:
  evaluation_options:
    generator:
      object: matrix.datasets.pair_generator.GroundTruthTestPairs
    evaluation:
      object: matrix.pipelines.evaluation.evaluation.DiscreteMetrics 
      metrics:
        - object: sklearn.metrics.roc_auc_score
        - object: sklearn.metrics.average_precision_score


# # MRR
# evaluation.mrr:
#   _overrides:
#     evaluation:
#       object: "matrix.pipelines.evaluation.nodes.MRREvaluation"

#   evaluation_options: ${merge:${.._evaluation_options},${._overrides}}


# # HitK
# # NOTE: We could also add a list of evaluation options
# evaluation.hitk:
#   _overrides:
#     evaluation:
#       object: "matrix.pipelines.evaluation.nodes.HitKEvaluation"

#   evaluation_options: ${merge:${.._evaluation_options},${._overrides}}




# Others can be added on demand..