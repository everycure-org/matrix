_bigquery_ds: &_bigquery_ds
  type: matrix.datasets.gcp.SparkDatasetWithBQExternalTable
  project_id: ${oc.env:RUNTIME_GCP_PROJECT_ID}
  dataset: release_${globals:versions.release}
  file_format: parquet
  save_args:
    mode: overwrite
    labels:
      git_sha: ${globals:git_sha}

# -------------------------------------------------------------------------
# Datasets
# -------------------------------------------------------------------------

# Overwriting the base catalog to use the bigquery datasets

#TODO how do we make this lke the integration catalog that depends on {source} rather than hardcoding the name
# Ingested RTX KG2 nodes/edges
ingestion.int.rtx_kg2.nodes:
  <<: *_bigquery_ds
  filepath: ${globals:paths.ingestion}/int/rtx_kg2/${globals:data_sources.rtx_kg2.version}/nodes
  table: rtx_kg2_nodes_ingested_${globals:data_sources.rtx_kg2.version}

ingestion.int.rtx_kg2.edges:
  <<: *_bigquery_ds
  filepath: ${globals:paths.ingestion}/int/rtx_kg2/${globals:data_sources.rtx_kg2.version}/edges
  table: rtx_kg2_edges_ingested_${globals:data_sources.rtx_kg2.version}

# Ingested Robokop nodes/edges
ingestion.int.robokop.nodes:
  <<: *_bigquery_ds
  filepath: ${globals:paths.ingestion}/int/robokop/${globals:data_sources.robokop.version}/nodes
  table: robokop_nodes_ingested_${globals:data_sources.robokop.version}

ingestion.int.robokop.edges:
  <<: *_bigquery_ds
  filepath: ${globals:paths.ingestion}/int/robokop/${globals:data_sources.robokop.version}/edges
  table: robokop_edges_ingested_${globals:data_sources.robokop.version}

# Valid edge types from matrix-schema
ingestion.int.valid_edge_types:
  <<: *_bigquery_ds
  filepath: ${globals:paths.ingestion}/int/valid_edge_types/${globals:data_sources.matrix_schema.version}/valid_edge_types
  table: valid_edge_types
