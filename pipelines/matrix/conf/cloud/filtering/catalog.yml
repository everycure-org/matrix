_bigquery_ds: &_bigquery_ds
  type: matrix.datasets.gcp.SparkDatasetWithBQExternalTable
  project_id: ${oc.env:RUNTIME_GCP_PROJECT_ID}
  dataset: run_${globals:run_name}
  file_format: parquet
  save_args:
    mode: overwrite
    labels:
      git_sha: ${globals:git_sha}


_spark_parquet_ds: &_spark_parquet
  type: matrix.datasets.gcp.LazySparkDataset
  file_format: parquet
  load_args:
    header: True
  save_args:
    mode: overwrite

# -------------------------------------------------------------------------
# Datasets
# -------------------------------------------------------------------------

# Overwriting the base catalog to use the bigquery datasets


# Removed nodes after initial filter step
filtering.prm.removed_nodes_initial:
  <<: *_spark_parquet
  filepath: ${globals:paths.filtering}/prm/removed/nodes_initial

# Removed edges due to filtering or missing nodes
filtering.prm.removed_edges:
  <<: *_spark_parquet
  filepath: ${globals:paths.filtering}/prm/removed/edges

filtering.prm.removed_nodes_final:
  <<: *_spark_parquet
  filepath: ${globals:paths.filtering}/prm/removed/nodes_final

# Filtered PRM nodes/edges written to BigQuery
filtering.prm.filtered_nodes:
  <<: *_bigquery_ds
  filepath: ${globals:paths.filtering}/prm/filtered/nodes
  table: nodes_filtered

filtering.prm.filtered_edges:
  <<: *_bigquery_ds
  filepath: ${globals:paths.filtering}/prm/filtered/edges
  table: edges_filtered

