_bigquery_ds: &_bigquery_ds
  type: matrix.datasets.gcp.BigQueryTableDataset
  project_id: ${oc.env:GCP_PROJECT_ID}
  dataset: release_${globals:versions.release}
  save_args:
    bigQueryTableLabel.git_sha: ${globals:git_sha}
    temporaryGcsBucket: ${globals:gcs_bucket}

# TODO we currently cannot handle datasets with bad characters in the name, thus writing features to storage
# embeddings.feat.nodes:
#   <<: *_bigquery_ds
#   table: embeddings

embeddings.reporting.loss:
  type: kedro_mlflow.io.artifacts.MlflowArtifactDataset
  artifact_path: topological
  dataset:
    # NOTE: Data needs to be stored locally for MLFlow
    # to be able to upload to GCS. The ML Tracking server
    # is setup to not serve artifacts, thereby interacting
    # directly with GCS as a result.
    type: kedro_datasets.matplotlib.MatplotlibWriter
    filepath: ${globals:paths.tmp}/convergence_plot.png

embeddings.reporting.topological_pca_plot:
  type: kedro_mlflow.io.artifacts.MlflowArtifactDataset
  artifact_path: topological
  dataset:
    # NOTE: Data needs to be stored locally for MLFlow
    # to be able to upload to GCS. The ML Tracking server
    # is setup to not serve artifacts, thereby interacting
    # directly with GCS as a result.
    type: kedro_datasets.matplotlib.MatplotlibWriter
    filepath: ${globals:paths.tmp}/pca_plot.png