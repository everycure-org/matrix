# https://platform.openai.com/docs/guides/embeddings/faq
# OpenAI embeddings support batching, up to 8000 tokens.
embeddings.node:
  encoder:
   openai_api_base: ${oc.env:OPENAI_ENDPOINT}
   batch_size: 1 # NOTE: The MockApi does not support batching, hence disabled

# Reducing dimensionality reduction to speed up runtime
embeddings.dimensionality_reduction:
  transformer:
    k: 2

embeddings.topological:
  estimator:
    args:
      batchSize: 20
      embeddingDimension: 3


caching.preprocessor:
  _object: matrix.pipelines.batch.pipeline.embeddings_preprocessor
  new_col: &pkey mykey
caching.resolver:
  _object: matrix.pipelines.embeddings.encoders.DummyResolver

caching.primary_key: *pkey
caching.api: "foo"
caching.new_col: lookup

embeddings.topological_estimator:
  iterations: 1
  embedding_dim: 3
  walk_length: 2

