# This file is used to set default values for environment variables.
# It is loaded first, then the .env file overwrites any values.
# If you want to set values only for yourself, create a .env file and put
# your overrides there.

# NOTE: This file is on .dockerignore, so changes will not be picked up by our pipeline run.
KEDRO_LOGGING_CONFIG=conf/logging.yml

# OpenAI configuration
OPENAI_API_KEY=dummy
OPENAI_ENDPOINT=http://localhost:1080/v1

# GitHub token
# https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-personal-access-token-classic
# requires repository contents scope
GH_TOKEN=add_token_with_repo_access

# Neo4J configuration
# NEO4J_PASSWORD=abc...

# by default, we disable the mlflow hook locally
KEDRO_HOOKS_DISABLE_MLFLOW=true
KEDRO_HOOKS_DISABLE_MLFLOW_KEDRO=true
KEDRO_HOOKS_DISABLE_RELEASE=true

# CAUTION: Use this with extreme care, setting these variables _does_ mess with `kedro submit` as the values
# from `.env.defaults` are used during Argo template generation, but are not copied into the Dockerfile. This 
# results in an Argo pipeline that is inconsistent with the pipeline docker container.
# Allow us to override dynamic pipeline configuration based on the environment, should be
# # used with extreme caution, and is only introduced to shorten duration of local tests.
# KEDRO_DYNAMIC_PIPELINES_MAPPING_CROSS_VALIDATION_N_CROSS_VAL_FOLDS=1
# KEDRO_DYNAMIC_PIPELINES_MAPPING_MODELLING_XG_ENSEMBLE_NUM_SHARDS=1

RUN_NAME=default    #defaults to local run, overwrite if you want to read from a remote specific run
WORKFLOW_ID=default #defaults to local workflow, overwrite if you want to read from a remote specific workflow

# MLFlow Run Name:
# (use for specifying a model for inference)
# ----------------
# WORKFLOW_ID=matrix-h2gf2

# GCP Configuration
# -----------------
# ensures all python code uses our read only service account and not an individual users'. This controls both spark and pandas!
GOOGLE_APPLICATION_CREDENTIALS=conf/local/service-account.json

# Hub project (dev)
RUNTIME_GCP_BUCKET=mtrx-us-central1-hub-dev-storage
MLFLOW_URL=https://mlflow.platform.dev.everycure.org/
ARGO_PLATFORM_URL=https://argo.platform.dev.everycure.org

# If you are an admin and want to run it in prod, copy these values to your .env file:
# RUNTIME_GCP_PROJECT_ID=mtrx-hub-prod-sms
# RUNTIME_GCP_BUCKET=mtrx-us-central1-hub-prod-storage
# MLFLOW_URL=https://mlflow.platform.prod.everycure.org/
# ARGO_PLATFORM_URL=https://argo.platform.prod.everycure.org
# GOOGLE_APPLICATION_CREDENTIALS=/Users/<YOUR_USERNAME>/.config/gcloud/application_default_credentials.json
# INCLUDE_PRIVATE_DATASETS=1


# WG 1
# GCP_PROJECT_ID=mtrx-wg1-data-dev-nb5
# GCP_BUCKET=mtrx-us-central1-wg1-data-dev-storage

# WG 2
# GCP_PROJECT_ID=mtrx-wg1-data-dev-nb5
# GCP_BUCKET=mtrx-us-central1-wg2-data-dev-storage

SPARK_DRIVER_MEMORY=30

# The service account used for impersonation in Spark jobs. This is used to run Spark jobs with the permissions of this service account, rather than the user's own permissions.
# SPARK_IMPERSONATION_SERVICE_ACCOUNT="<SA>d@mtrx-hub-prod-sms.iam.gserviceaccount.com"