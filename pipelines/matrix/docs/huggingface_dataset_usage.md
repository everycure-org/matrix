# HuggingFace Dataset for MATRIX Data Distribution

## Overview

The `HuggingFaceDataset` is a Kedro dataset implementation that enables seamless integration between MATRIX pipelines and Hugging Face Hub for data distribution. This allows Every Cure to publish knowledge graph data releases publicly for the research community.

## Features

- **Seamless Integration**: Works with existing Kedro pipelines and data catalogs
- **Multiple Data Formats**: Supports both single DataFrames and multiple DataFrames (e.g., nodes and edges)
- **Automatic Documentation**: Generates dataset cards with metadata and usage instructions
- **Version Control**: Leverages Hugging Face Hub's git-based versioning
- **Efficient Storage**: Uses Parquet format for optimal performance and compatibility

## Installation

The required dependencies are already included in the MATRIX pipeline:

```toml
# In pyproject.toml
"datasets>=4.0.0",
"huggingface_hub>=0.20.0",
```

## Basic Usage

### 1. Single DataFrame Upload

```python
from kedro_datasets.spark import SparkDataset
from matrix.datasets.huggingface import HuggingFaceDataset

# Load data from existing pipeline
sds = SparkDataset("gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.10.0/datasets/integration/prm/unified/nodes/*.parquet")
df = sds.load().toPandas()

# Upload to Hugging Face
hfds = HuggingFaceDataset(
    repo_id="everycure/matrix-kg-nodes-v0.10.0",
    token="${oc.env:HF_TOKEN}",  # Use environment variable
    private=False,
    save_args={
        "commit_message": "Upload KG nodes v0.10.0",
        "commit_description": "Knowledge graph nodes for drug repurposing research"
    }
)

hfds.save(df)
```

### 2. Multiple DataFrames Upload

```python
# Load nodes and edges
nodes_sds = SparkDataset("gs://.../nodes/*.parquet")
edges_sds = SparkDataset("gs://.../edges/*.parquet")

nodes_df = nodes_sds.load().toPandas()
edges_df = edges_sds.load().toPandas()

# Upload as multi-split dataset
data_dict = {
    "nodes": nodes_df,
    "edges": edges_df
}

hfds = HuggingFaceDataset(
    repo_id="everycure/matrix-kg-v0.10.0",
    token="${oc.env:HF_TOKEN}",
    private=False
)

hfds.save(data_dict)
```

### 3. Loading Data

```python
# Load single DataFrame
hfds = HuggingFaceDataset(repo_id="everycure/matrix-kg-nodes-v0.10.0")
df = hfds.load()

# Load multiple DataFrames
hfds = HuggingFaceDataset(repo_id="everycure/matrix-kg-v0.10.0")
data_dict = hfds.load()  # Returns {"nodes": DataFrame, "edges": DataFrame}
nodes_df = data_dict["nodes"]
edges_df = data_dict["edges"]
```

## Kedro Catalog Configuration

### Single Dataset

```yaml
# conf/base/catalog.yml
matrix_kg_nodes_hf:
  type: matrix.datasets.huggingface.HuggingFaceDataset
  repo_id: "everycure/matrix-kg-nodes-v0.10.0"
  token: ${oc.env:HF_TOKEN}
  private: false
  save_args:
    commit_message: "Upload KG nodes v0.10.0"
    commit_description: "Knowledge graph nodes for MATRIX drug repurposing platform"
```

### Multiple Datasets

```yaml
# conf/base/catalog.yml
matrix_kg_complete_hf:
  type: matrix.datasets.huggingface.HuggingFaceDataset
  repo_id: "everycure/matrix-kg-v0.10.0"
  token: ${oc.env:HF_TOKEN}
  private: false
  save_args:
    commit_message: "Upload complete KG v0.10.0"
    commit_description: |
      Complete knowledge graph dataset including:
      - nodes: Drug and disease entities with embeddings
      - edges: Relationships between entities
      
      Generated by MATRIX pipeline v0.10.0
      For more info: https://docs.dev.everycure.org
```

## Pipeline Integration

### Publishing Pipeline

Create a dedicated pipeline for publishing data to Hugging Face:

```python
# src/matrix/pipelines/publish/pipeline.py
from kedro import pipeline, node
from .nodes import publish_kg_to_huggingface

def create_pipeline(**kwargs) -> Pipeline:
    return pipeline([
        node(
            func=publish_kg_to_huggingface,
            inputs=["unified_nodes", "unified_edges"],
            outputs="matrix_kg_complete_hf",
            name="publish_kg_to_hf"
        )
    ])

# src/matrix/pipelines/publish/nodes.py
def publish_kg_to_huggingface(nodes_df, edges_df):
    """Combine nodes and edges for HuggingFace upload."""
    return {
        "nodes": nodes_df,
        "edges": edges_df
    }
```

### Release Pipeline

```python
# Run the publishing pipeline
kedro run --pipeline publish --env production
```

## Configuration Options

### Save Arguments

- `commit_message`: Commit message for the upload
- `commit_description`: Detailed description of the changes
- `create_pr`: Create a pull request instead of direct commit (default: False)
- `revision`: Branch/tag to commit to (default: "main")

### Load Arguments

- `split`: Specific split to load (e.g., "nodes", "edges"). If None, loads all splits
- `revision`: Branch/tag to load from (default: "main")
- `streaming`: Use streaming mode for large datasets (default: False)

## Environment Setup

### Hugging Face Token

1. Create account at [huggingface.co](https://huggingface.co)
2. Generate token at [Settings > Access Tokens](https://huggingface.co/settings/tokens)
3. Set environment variable:

```bash
export HF_TOKEN="hf_your_token_here"
```

Or in Kedro configuration:

```yaml
# conf/local/credentials.yml
huggingface:
  token: "hf_your_token_here"
```

## Best Practices

### Repository Naming

Use consistent naming convention:
- `everycure/matrix-kg-v{version}` - Complete knowledge graph
- `everycure/matrix-kg-nodes-v{version}` - Nodes only
- `everycure/matrix-kg-edges-v{version}` - Edges only
- `everycure/matrix-predictions-v{version}` - Model predictions

### Data Preparation

1. **Filter Sensitive Data**: Ensure no proprietary data is included
2. **Optimize Size**: Use `.limit()` for testing, full datasets for releases
3. **Validate Schema**: Ensure consistent column names and types
4. **Add Metadata**: Include version, creation date, and description

### Version Management

```python
# Example versioned upload
version = "v0.10.0"
hfds = HuggingFaceDataset(
    repo_id=f"everycure/matrix-kg-{version}",
    save_args={
        "commit_message": f"Release {version}",
        "revision": version  # Create version tag
    }
)
```

## Troubleshooting

### Common Issues

1. **Authentication Error**: Verify HF_TOKEN is set correctly
2. **Large Dataset Upload**: Use streaming or split into smaller chunks
3. **Permission Denied**: Ensure token has write permissions
4. **Network Timeout**: Retry upload or use smaller batch sizes

### Debug Mode

```python
import logging
logging.getLogger("matrix.datasets.huggingface").setLevel(logging.DEBUG)
```

## Integration with Existing Release Process

The HuggingFace dataset can be integrated into the existing release process:

```bash
# 1. Generate release data
kedro run --pipeline data_integration --env production

# 2. Upload to Hugging Face
kedro run --pipeline publish --env production

# 3. Update public documentation
# (automated via GitHub Actions)
```

This creates a complete data distribution pipeline that makes MATRIX data easily accessible to the research community while maintaining version control and proper documentation.