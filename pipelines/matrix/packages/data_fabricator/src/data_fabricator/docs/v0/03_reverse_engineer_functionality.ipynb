{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e972bd-b22d-4892-9e65-ef388b88f552",
   "metadata": {
    "nbsphinx": "hidden",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967af570-ca00-4f3e-90b3-dea34896cbdf",
   "metadata": {},
   "source": [
    "# Reverse Engineer\n",
    "\n",
    "If you have existing data and you would like to fabricate tables by referring that existing data,\n",
    "then you can use `reverse_engineer` functions to generate skeleton config.\n",
    "\n",
    "This document will highlight `reverse engineer` process i.e, the generation of config using the given data.\n",
    "It works only with spark dataframe not the pandas dataframe. Also, it doesn't allow the complex dtypes like arrays and structs.\n",
    "\n",
    "Lets create sample spark dataframes to demonstrate the `reverse_engineer` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3aedd5e-200d-4b81-99bb-08c37872d7a1",
   "metadata": {
    "nbsphinx": "hidden",
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/06 10:28:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType,\n",
    "    DateType,\n",
    "    DoubleType,\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "import random\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.config(\"spark.ui.showConsoleProgress\", False)\n",
    "    .config(\"spark.sql.shuffle.partitions\", 1)\n",
    "    .getOrCreate()\n",
    ")\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"int_col\", IntegerType(), True),\n",
    "        StructField(\"long_col\", LongType(), True),\n",
    "        StructField(\"string_col\", StringType(), True),\n",
    "        StructField(\"float_col\", FloatType(), True),\n",
    "        StructField(\"double_col\", DoubleType(), True),\n",
    "        StructField(\"date_col\", DateType(), True),\n",
    "        StructField(\"datetime_col\", TimestampType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data = [\n",
    "    (\n",
    "        1,\n",
    "        2,\n",
    "        \"hello world\",\n",
    "        13.01,\n",
    "        0.89,\n",
    "        pd.Timestamp(\"2012-05-01\").date(),\n",
    "        datetime.datetime(2020, 11, 30, 18, 29, 19, 990601),\n",
    "    ),\n",
    "    (\n",
    "        1,\n",
    "        2,\n",
    "        \"hello world\",\n",
    "        13.01,\n",
    "        0.89,\n",
    "        pd.Timestamp(\"2012-05-01\").date(),\n",
    "        datetime.datetime(2020, 11, 30, 18, 29, 19, 990601),\n",
    "    ),\n",
    "    (\n",
    "        1,\n",
    "        2,\n",
    "        \"language string\",\n",
    "        10.51,\n",
    "        6.79,\n",
    "        pd.Timestamp(\"2011-05-01\").date(),\n",
    "        datetime.datetime(2018, 11, 30, 18, 19, 19, 990601),\n",
    "    ),\n",
    "]\n",
    "\n",
    "sample_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "schema_with_complex_dtype = StructType(\n",
    "    [\n",
    "        StructField(\"int_col\", IntegerType(), True),\n",
    "        StructField(\"long_col\", LongType(), True),\n",
    "        StructField(\"string_col\", StringType(), True),\n",
    "        StructField(\"float_col\", FloatType(), True),\n",
    "        StructField(\"double_col\", DoubleType(), True),\n",
    "        StructField(\"date_col\", DateType(), True),\n",
    "        StructField(\"datetime_col\", TimestampType(), True),\n",
    "        StructField(\"array_int\", ArrayType(IntegerType()), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_with_complex_dtype = [\n",
    "    (\n",
    "        1,\n",
    "        2,\n",
    "        \"hello world\",\n",
    "        13.01,\n",
    "        0.89,\n",
    "        pd.Timestamp(\"2012-05-01\").date(),\n",
    "        datetime.datetime(2020, 11, 30, 18, 29, 19, 990601),\n",
    "        [1, 5, 7],\n",
    "    ),\n",
    "    (\n",
    "        1,\n",
    "        2,\n",
    "        \"hello world\",\n",
    "        13.01,\n",
    "        0.89,\n",
    "        pd.Timestamp(\"2012-05-01\").date(),\n",
    "        datetime.datetime(2020, 11, 30, 18, 29, 19, 990601),\n",
    "        [9, 2, 7],\n",
    "    ),\n",
    "    (\n",
    "        1,\n",
    "        2,\n",
    "        \"language string\",\n",
    "        10.51,\n",
    "        6.79,\n",
    "        pd.Timestamp(\"2011-05-01\").date(),\n",
    "        datetime.datetime(2018, 11, 30, 18, 19, 19, 990601),\n",
    "        [8, 2, 9],\n",
    "    ),\n",
    "]\n",
    "\n",
    "sample_df_with_complex_dtypes = spark.createDataFrame(\n",
    "    data_with_complex_dtype, schema_with_complex_dtype\n",
    ")\n",
    "\n",
    "data_dict = {\n",
    "    \"int_col\": [1, 2, 3],\n",
    "    \"long_col\": [2, 3, 4],\n",
    "    \"string_col\": [\"awesome_string\", \"hello\", \"world\"],\n",
    "    \"float_col\": [10.51, 1.1, 2.2],\n",
    "    \"double_col\": [6.79, 9.82, 8.99],\n",
    "}\n",
    "\n",
    "sample_pandas_df = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d60f2e7-91c5-4a90-a225-4879e1a28bda",
   "metadata": {},
   "source": [
    "Sample spark dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cdc3f8d-063d-4d85-be97-02f0d874bb57",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------------+---------+----------+----------+--------------------------+\n",
      "|int_col|long_col|string_col     |float_col|double_col|date_col  |datetime_col              |\n",
      "+-------+--------+---------------+---------+----------+----------+--------------------------+\n",
      "|1      |2       |hello world    |13.01    |0.89      |2012-05-01|2020-11-30 18:29:19.990601|\n",
      "|1      |2       |hello world    |13.01    |0.89      |2012-05-01|2020-11-30 18:29:19.990601|\n",
      "|1      |2       |language string|10.51    |6.79      |2011-05-01|2018-11-30 18:19:19.990601|\n",
      "+-------+--------+---------------+---------+----------+----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27df33e-39ab-49d0-9aaa-b76b7d88cbae",
   "metadata": {},
   "source": [
    "## Generate Config from a Spark DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "790904e3-aa57-4345-9298-1b7fe096c588",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x8/_9l2j54n1lx_71w8kncv7mc80000gp/T/ipykernel_42962/3402234767.py:1: DeprecationWarning: Deprecation Warning: You are using v0 of Data Fabricator API which will bedeprecated soon. Please migrate to v1 as soon as possible. For information onhow to migrate, please visit our documentation page.\n",
      "  from data_fabricator.v0.core.reverse_engineer import reverse_engineer_df\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns:\n",
      "  date_col:\n",
      "    sample_values:\n",
      "    - 2011-05-01\n",
      "    - 2012-05-01\n",
      "    type: generate_values\n",
      "  datetime_col:\n",
      "    sample_values:\n",
      "    - 2018-11-30 18:19:19.990601\n",
      "    - 2020-11-30 18:29:19.990601\n",
      "    type: generate_values\n",
      "  double_col:\n",
      "    sample_values:\n",
      "    - 0.89\n",
      "    - 6.79\n",
      "    type: generate_values\n",
      "  float_col:\n",
      "    sample_values:\n",
      "    - 10.510000228881836\n",
      "    - 13.010000228881836\n",
      "    type: generate_values\n",
      "  int_col:\n",
      "    sample_values:\n",
      "    - 1\n",
      "    type: generate_values\n",
      "  long_col:\n",
      "    sample_values:\n",
      "    - 2\n",
      "    type: generate_values\n",
      "  string_col:\n",
      "    sample_values:\n",
      "    - hello world\n",
      "    - language string\n",
      "    type: generate_values\n",
      "num_rows: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data_fabricator.v0.core.reverse_engineer import reverse_engineer_df\n",
    "\n",
    "table_config = reverse_engineer_df(df=sample_df, num_rows=10)\n",
    "\n",
    "print(yaml.safe_dump(table_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b3775-4798-4458-8390-497701886bce",
   "metadata": {},
   "source": [
    "## Generate Config from a Pandas DataFrame:\n",
    "\n",
    "Currently not implemented - convert the pandas dataframe to a spark dataframe then proceed. There are 2 ways to convert a pandas dataframe to spark, in memory (shown in the example below), or via parquet files.\n",
    "It is generally recommended to use parquet files because in memory is an implicit conversion of data types. But for the brevity of the example, we will use the in memory conversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ff8d12b-9d43-4e89-90f4-9b7795eb4629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data_fabricator.v0.core.reverse_engineer import reverse_engineer_df\n",
    "\n",
    "# some_pandas_df.head()\n",
    "\n",
    "# spark_df = spark.createDataFrame(some_pandas_df)\n",
    "\n",
    "# table_config = reverse_engineer_df(df=spark_df, num_rows=10)\n",
    "\n",
    "# print(yaml.safe_dump(table_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c72c446-baf7-41c0-a158-d9139fdc4cdb",
   "metadata": {},
   "source": [
    "##Generating Config for Spark Dataframes with Complex Data Types:\n",
    "\n",
    "sample spark dataframe with complex dtypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9434f2f2-f7e4-433d-918e-941a268a011c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------------+---------+----------+----------+--------------------------+---------+\n",
      "|int_col|long_col|string_col     |float_col|double_col|date_col  |datetime_col              |array_int|\n",
      "+-------+--------+---------------+---------+----------+----------+--------------------------+---------+\n",
      "|1      |2       |hello world    |13.01    |0.89      |2012-05-01|2020-11-30 18:29:19.990601|[1, 5, 7]|\n",
      "|1      |2       |hello world    |13.01    |0.89      |2012-05-01|2020-11-30 18:29:19.990601|[9, 2, 7]|\n",
      "|1      |2       |language string|10.51    |6.79      |2011-05-01|2018-11-30 18:19:19.990601|[8, 2, 9]|\n",
      "+-------+--------+---------------+---------+----------+----------+--------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df_with_complex_dtypes.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aca8e7-1d3b-4a2c-b4c7-9ab9b4ac17a4",
   "metadata": {},
   "source": [
    "As we can see, passing this dataframe to the function will result in the following error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "769d1b85-67bf-48a6-ac4b-1c538e4fac53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError -  dtype array<int> not allowed. Kindly drop this column.\n"
     ]
    }
   ],
   "source": [
    "from data_fabricator.v0.core.reverse_engineer import reverse_engineer_df\n",
    "\n",
    "try:\n",
    "    table_config = reverse_engineer_df(df=sample_df_with_complex_dtypes, num_rows=10)\n",
    "    print(yaml.safe_dump(table_config))\n",
    "\n",
    "except ValueError as error:\n",
    "    print(\"ValueError - \", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e1ed9-d29b-4128-9948-508128a0006f",
   "metadata": {},
   "source": [
    "## Creating Config for Multiple Dataframes in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395bb73c-84f3-45ab-a0f1-118a3033cb8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table1:\n",
      "  columns:\n",
      "    date_col:\n",
      "      sample_values:\n",
      "      - 2011-05-01\n",
      "      - 2012-05-01\n",
      "      type: generate_values\n",
      "    datetime_col:\n",
      "      sample_values:\n",
      "      - 2018-11-30 18:19:19.990601\n",
      "      - 2020-11-30 18:29:19.990601\n",
      "      type: generate_values\n",
      "    double_col:\n",
      "      sample_values:\n",
      "      - 0.89\n",
      "      - 6.79\n",
      "      type: generate_values\n",
      "    float_col:\n",
      "      sample_values:\n",
      "      - 10.510000228881836\n",
      "      - 13.010000228881836\n",
      "      type: generate_values\n",
      "    int_col:\n",
      "      sample_values:\n",
      "      - 1\n",
      "      type: generate_values\n",
      "    long_col:\n",
      "      sample_values:\n",
      "      - 2\n",
      "      type: generate_values\n",
      "    string_col:\n",
      "      sample_values:\n",
      "      - hello world\n",
      "      - language string\n",
      "      type: generate_values\n",
      "  num_rows: 10\n",
      "table2:\n",
      "  columns:\n",
      "    date_col:\n",
      "      sample_values:\n",
      "      - 2011-05-01\n",
      "      - 2012-05-01\n",
      "      type: generate_values\n",
      "    datetime_col:\n",
      "      sample_values:\n",
      "      - 2018-11-30 18:19:19.990601\n",
      "      - 2020-11-30 18:29:19.990601\n",
      "      type: generate_values\n",
      "    double_col:\n",
      "      sample_values:\n",
      "      - 0.89\n",
      "      - 6.79\n",
      "      type: generate_values\n",
      "    float_col:\n",
      "      sample_values:\n",
      "      - 10.510000228881836\n",
      "      - 13.010000228881836\n",
      "      type: generate_values\n",
      "    int_col:\n",
      "      sample_values:\n",
      "      - 1\n",
      "      type: generate_values\n",
      "    long_col:\n",
      "      sample_values:\n",
      "      - 2\n",
      "      type: generate_values\n",
      "    string_col:\n",
      "      sample_values:\n",
      "      - hello world\n",
      "      - language string\n",
      "      type: generate_values\n",
      "  num_rows: 10\n",
      "table3:\n",
      "  columns:\n",
      "    date_col:\n",
      "      sample_values:\n",
      "      - 2011-05-01\n",
      "      - 2012-05-01\n",
      "      type: generate_values\n",
      "    datetime_col:\n",
      "      sample_values:\n",
      "      - 2018-11-30 18:19:19.990601\n",
      "      - 2020-11-30 18:29:19.990601\n",
      "      type: generate_values\n",
      "    double_col:\n",
      "      sample_values:\n",
      "      - 0.89\n",
      "      - 6.79\n",
      "      type: generate_values\n",
      "    float_col:\n",
      "      sample_values:\n",
      "      - 10.510000228881836\n",
      "      - 13.010000228881836\n",
      "      type: generate_values\n",
      "    int_col:\n",
      "      sample_values:\n",
      "      - 1\n",
      "      type: generate_values\n",
      "    long_col:\n",
      "      sample_values:\n",
      "      - 2\n",
      "      type: generate_values\n",
      "    string_col:\n",
      "      sample_values:\n",
      "      - hello world\n",
      "      - language string\n",
      "      type: generate_values\n",
      "  num_rows: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data_fabricator.v0.core.reverse_engineer import reverse_engineer_tables\n",
    "\n",
    "valid_sample_df = sample_df.drop(\"array_int\")\n",
    "various_tables = {\n",
    "    \"table1\": valid_sample_df,\n",
    "    \"table2\": valid_sample_df,\n",
    "    \"table3\": valid_sample_df,\n",
    "}\n",
    "\n",
    "table_config = reverse_engineer_tables(various_tables)\n",
    "print(yaml.safe_dump(table_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ad727-ab50-40b5-a75a-74a6d58370f6",
   "metadata": {},
   "source": [
    "Notice that all the columns are generated with `type: generate_values` - this is because the function only samples from the dataframe and takes all unique values that are given to it.\n",
    "\n",
    "The general flow is to use these sets of functions to generate skeleton config, then proceed to modify to the appropriate function manually. Future work might include implementing a smarter profiler.\n",
    "\n",
    "\n",
    "## Now let's verify that the config is valid by passing it back to the `MockDataGenerator`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eac247f3-5a9c-4217-a1f3-ee9c8066cc22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   int_col  long_col       string_col  float_col  double_col    date_col               datetime_col\n",
      "0        1         2      hello world      10.51        6.79  2011-05-01 2018-11-30 18:19:19.990601\n",
      "1        1         2  language string      10.51        6.79  2012-05-01 2020-11-30 18:29:19.990601\n",
      "2        1         2  language string      10.51        0.89  2012-05-01 2020-11-30 18:29:19.990601\n",
      "3        1         2      hello world      10.51        0.89  2012-05-01 2020-11-30 18:29:19.990601\n",
      "4        1         2      hello world      10.51        6.79  2012-05-01 2018-11-30 18:19:19.990601\n",
      "5        1         2      hello world      10.51        6.79  2012-05-01 2018-11-30 18:19:19.990601\n",
      "6        1         2      hello world      13.01        6.79  2011-05-01 2020-11-30 18:29:19.990601\n",
      "7        1         2      hello world      13.01        0.89  2011-05-01 2020-11-30 18:29:19.990601\n",
      "8        1         2      hello world      13.01        6.79  2012-05-01 2020-11-30 18:29:19.990601\n",
      "9        1         2      hello world      10.51        6.79  2011-05-01 2018-11-30 18:19:19.990601\n"
     ]
    }
   ],
   "source": [
    "from data_fabricator.v0.core.fabricator import MockDataGenerator\n",
    "\n",
    "# Setting seed is not recommended for general use, please consider when to use seed\n",
    "mock_generator = MockDataGenerator(instructions=table_config, seed=1)\n",
    "mock_generator.generate_all()\n",
    "\n",
    "generated_table1_df = mock_generator.all_dataframes[\"table1\"]\n",
    "print(generated_table1_df)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "qblabs-monorepo",
   "language": "python",
   "name": "qblabs-monorepo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
