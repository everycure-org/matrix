apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  namespace: {{namespace}}
  name: {{run_name}}
spec:
  workflowMetadata:
    labels:
      run: {% raw %}'{{ workflow.parameters.run_name }}'
      {% endraw %}
      username: "{{ username }}"
      test_type: "gpu_stress_test"
      gpu_type: "nvidia-l4"
  entrypoint: "gpu-stress-test"
  arguments:
    parameters:
      - name: run_name
        value: "{{ run_name }}"
      - name: num_gpus
        value: "1"
      - name: memory_limit
        value: "8"
      - name: memory_request
        value: "4"
      - name: cpu_limit
        value: "2"
      - name: cpu_request
        value: "1"
  templates:
  - name: gpu-stress-test
    metrics:
      prometheus:
        - name: "argo_gpu_stress_test_duration_seconds"
          help: "Duration of GPU stress test"
          labels:
            - key: test_type
              value: "gpu_stress_test"
            - key: gpu_type
              value: "nvidia-l4"
          histogram:
            buckets: [60, 120, 180, 240, 300, 360]
            value: {% raw %}"{{workflow.duration}}"
            {% endraw %}
        - name: "argo_gpu_stress_test_completion"
          help: "GPU stress test completion status"
          labels:
            - key: test_type
              value: "gpu_stress_test"
            - key: status
              value: {% raw %}"{{status}}"
              {% endraw %}
          counter:
            value: "1"
    inputs:
      parameters:
      - name: num_gpus
      - name: memory_limit
      - name: memory_request
      - name: cpu_limit
      - name: cpu_request
    metadata:
      labels:
        app: kedro-argo  # Changed to match Prometheus service monitor
        test_type: gpu-stress-test
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9400"
        prometheus.io/path: "/metrics"
        prometheus.io/scheme: "http"
    # Sidecar for GPU monitoring
    sidecars:
    - name: nvidia-dcgm-exporter
      image: nvcr.io/nvidia/k8s/dcgm-exporter:4.2.3-4.1.3-ubuntu22.04
      command: ["/bin/bash", "-c"]
      args:
        - |
          echo "NODE_IP: $NODE_IP"
          echo "NODE_NAME: $NODE_NAME"
          echo "GPU_COUNT: {% raw %}{{inputs.parameters.num_gpus}}{% endraw %}"
          echo "Starting DCGM exporter for GPU stress test..."
          echo "Note: In GKE, GPU utilization metrics may be restricted"
          dcgm-exporter
      lifecycle:
        preStop:
          exec:
            command: ["/bin/true"]
      ports:
        - name: gpu-metrics
          containerPort: 9400
      securityContext:
        privileged: true
        capabilities:
          add: ["SYS_ADMIN"]
      env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: "DCGM_EXPORTER_KUBERNETES_GPU_ID_TYPE"
          value: "device-name"
        - name: LD_LIBRARY_PATH
          value: /usr/local/nvidia/lib64
        - name: NODE_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        - name: DCGM_EXPORTER_KUBERNETES
          value: 'true'
        - name: DCGM_EXPORTER_LISTEN
          value: ':9400'
        - name: NVIDIA_VISIBLE_DEVICES
          value: "all"
        - name: NVIDIA_DRIVER_CAPABILITIES
          value: "all"
      volumeMounts:
        - name: nvidia-install-dir-host
          mountPath: /usr/local/nvidia
          readOnly: true
        - name: pod-resources
          mountPath: /var/lib/kubelet/pod-resources
          readOnly: true
    # Alternative GPU monitoring sidecar using nvidia-smi
    - name: nvidia-smi-exporter
      image: nvidia/cuda:12.9.1-cudnn-runtime-ubuntu20.04
      command: ["/bin/bash", "-c"]
      args:
        - |
          # Install necessary tools
          apt-get update && apt-get install -y curl python3 python3-pip
          pip3 install prometheus_client
          
          # Create a simple nvidia-smi based exporter
          cat > /tmp/nvidia_smi_exporter.py << 'EOF'
          import subprocess
          import json
          import time
          from prometheus_client import start_http_server, Gauge
          
          # Create Prometheus metrics
          gpu_utilization = Gauge('nvidia_smi_gpu_utilization_percent', 'GPU utilization percentage', ['gpu_id', 'gpu_name'])
          gpu_memory_used = Gauge('nvidia_smi_memory_used_bytes', 'GPU memory used in bytes', ['gpu_id', 'gpu_name'])
          gpu_memory_total = Gauge('nvidia_smi_memory_total_bytes', 'GPU memory total in bytes', ['gpu_id', 'gpu_name'])
          gpu_temperature = Gauge('nvidia_smi_temperature_celsius', 'GPU temperature in celsius', ['gpu_id', 'gpu_name'])
          gpu_power_draw = Gauge('nvidia_smi_power_draw_watts', 'GPU power draw in watts', ['gpu_id', 'gpu_name'])
          
          def collect_metrics():
              try:
                  # Query nvidia-smi for JSON output
                  result = subprocess.run([
                      'nvidia-smi', '--query-gpu=index,name,utilization.gpu,memory.used,memory.total,temperature.gpu,power.draw',
                      '--format=csv,noheader,nounits'
                  ], capture_output=True, text=True, check=True)
                  
                  for line in result.stdout.strip().split('\n'):
                      if line.strip():
                          parts = [p.strip() for p in line.split(',')]
                          if len(parts) >= 7:
                              gpu_id = parts[0]
                              gpu_name = parts[1]
                              util = float(parts[2]) if parts[2] != '[Not Supported]' else 0
                              mem_used = float(parts[3]) * 1024 * 1024 if parts[3] != '[Not Supported]' else 0  # Convert MB to bytes
                              mem_total = float(parts[4]) * 1024 * 1024 if parts[4] != '[Not Supported]' else 0  # Convert MB to bytes
                              temp = float(parts[5]) if parts[5] != '[Not Supported]' else 0
                              power = float(parts[6]) if parts[6] != '[Not Supported]' else 0
                              
                              # Update metrics
                              gpu_utilization.labels(gpu_id=gpu_id, gpu_name=gpu_name).set(util)
                              gpu_memory_used.labels(gpu_id=gpu_id, gpu_name=gpu_name).set(mem_used)
                              gpu_memory_total.labels(gpu_id=gpu_id, gpu_name=gpu_name).set(mem_total)
                              gpu_temperature.labels(gpu_id=gpu_id, gpu_name=gpu_name).set(temp)
                              gpu_power_draw.labels(gpu_id=gpu_id, gpu_name=gpu_name).set(power)
                              
                              print(f"GPU {gpu_id}: {util}% utilization, {temp}Â°C, {power}W")
              except Exception as e:
                  print(f"Error collecting metrics: {e}")
          
          if __name__ == '__main__':
              # Start Prometheus metrics server
              start_http_server(9401)
              print("nvidia-smi exporter started on port 9401")
              
              while True:
                  collect_metrics()
                  time.sleep(5)  # Collect every 5 seconds
          EOF
          
          echo "Starting nvidia-smi based GPU exporter..."
          python3 /tmp/nvidia_smi_exporter.py
      ports:
        - name: smi-metrics
          containerPort: 9401
      env:
        - name: NVIDIA_VISIBLE_DEVICES
          value: "all"
        - name: NVIDIA_DRIVER_CAPABILITIES
          value: "compute,utility"
    # Main container for GPU stress testing
    container:
      image: oguzpastirmaci/gpu-burn:latest
      imagePullPolicy: Always
      command: ["/bin/bash", "-c"]
      args:
        - |
          set -e
          echo "=== GPU Stress Test Starting ==="
          echo "Test Duration: 5 minutes (300 seconds)"
          echo "Phase 1: Ramp up from 1% to 100% over 3 minutes (180 seconds)"
          echo "Phase 2: 100% load for 2 minutes (120 seconds)"
          echo "======================================="
          
          # Check GPU availability
          echo "Checking GPU status..."
          nvidia-smi
          echo ""
          
          # Check DCGM is working
          echo "Checking DCGM metrics availability..."
          for i in {1..30}; do
            if curl -s http://localhost:9400/metrics | grep -q "DCGM_FI_DEV"; then
              echo "DCGM metrics are available!"
              echo "Sample GPU metrics:"
              curl -s http://localhost:9400/metrics | grep -E "(DCGM_FI_DEV_GPU_UTIL|DCGM_FI_DEV_MEM_COPY_UTIL|DCGM_FI_DEV_GPU_TEMP)" | head -5
              break
            else
              echo "Attempt $i/30: DCGM metrics not ready yet, waiting 2s..."
              sleep 2
            fi
          done
          
          # Check nvidia-smi exporter
          echo ""
          echo "Checking nvidia-smi exporter availability..."
          for i in {1..30}; do
            if curl -s http://localhost:9401/metrics | grep -q "nvidia_smi_gpu_utilization"; then
              echo "nvidia-smi exporter metrics are available!"
              echo "Sample nvidia-smi GPU metrics:"
              curl -s http://localhost:9401/metrics | grep -E "(nvidia_smi_gpu_utilization|nvidia_smi_temperature)" | head -3
              break
            else
              echo "Attempt $i/30: nvidia-smi exporter not ready yet, waiting 2s..."
              sleep 2
            fi
          done
          echo ""
          
          echo "=== Phase 1: Ramping up GPU load (3 minutes) ==="
          
          # Phase 1: Gradual ramp up from low to high intensity
          # We'll use gpu-burn with varying durations and breaks
          
          for step in {1..18}; do
            # Calculate intensity: 5% to 100% over 18 steps
            intensity=$(( 5 + (95 * (step - 1) / 17) ))
            
            # Calculate burn duration and break duration for 10-second intervals
            # Higher intensity = longer burn time, shorter break
            burn_duration=$(( intensity * 10 / 100 ))
            break_duration=$(( 10 - burn_duration ))
            
            echo ""
            echo "Step $step/18: ${intensity}% intensity (${burn_duration}s burn, ${break_duration}s break)"
            
            if [ $burn_duration -gt 0 ]; then
              echo "  Starting GPU burn for ${burn_duration}s..."
              timeout ${burn_duration}s gpu_burn 600 > /dev/null 2>&1 || true
            fi
            
            if [ $break_duration -gt 0 ]; then
              echo "  Resting for ${break_duration}s..."
              sleep $break_duration
            fi
            
            # Show current GPU status every 5 steps
            if [ $((step % 5)) -eq 0 ]; then
              echo "  Current GPU status:"
              nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu --format=csv,noheader,nounits | head -1
              echo "  Current DCGM metrics:"
              curl -s http://localhost:9400/metrics | grep -E "(DCGM_FI_DEV_GPU_UTIL|DCGM_FI_DEV_MEM_COPY_UTIL)" | head -2
              echo "  Current nvidia-smi exporter metrics:"
              curl -s http://localhost:9401/metrics | grep "nvidia_smi_gpu_utilization" | head -1
            fi
          done
          
          echo ""
          echo "=== Phase 2: Maximum GPU load (2 minutes) ==="
          echo "Running at 100% GPU intensity for 120 seconds..."
          
          # Phase 2: Full intensity for 2 minutes
          echo "Starting maximum GPU burn..."
          timeout 120s gpu_burn 600 > /dev/null 2>&1 || true
          
          echo ""
          echo "=== Final GPU Status Check ==="
          nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu,power.draw --format=csv,noheader,nounits
          
          echo ""
          echo "=== Final DCGM Metrics Check ==="
          echo "GPU Utilization metrics:"
          curl -s http://localhost:9400/metrics | grep "DCGM_FI_DEV_GPU_UTIL" | head -5
          echo "Memory Utilization metrics:"
          curl -s http://localhost:9400/metrics | grep "DCGM_FI_DEV_MEM_COPY_UTIL" | head -5
          echo "Temperature metrics:"
          curl -s http://localhost:9400/metrics | grep "DCGM_FI_DEV_GPU_TEMP" | head -5
          
          echo ""
          echo "=== Prometheus Annotations Check ==="
          echo "Checking if Prometheus can discover this pod..."
          echo "Pod IP: $(hostname -i)"
          echo "Metrics endpoint: http://$(hostname -i):9400/metrics"
          echo ""
          echo "Testing metrics endpoint accessibility:"
          curl -s "http://$(hostname -i):9400/metrics" | grep -c "DCGM_FI_DEV" && echo "â DCGM metrics found" || echo "â No DCGM metrics found"
          echo ""
          echo "=== Key GPU Metrics Sample ==="
          echo "DCGM GPU Utilization:"
          curl -s http://localhost:9400/metrics | grep "DCGM_FI_DEV_GPU_UTIL{" | head -3
          echo "nvidia-smi GPU Utilization:"
          curl -s http://localhost:9401/metrics | grep "nvidia_smi_gpu_utilization" | head -3
          echo "GPU Memory Utilization:"
          curl -s http://localhost:9400/metrics | grep "DCGM_FI_DEV_MEM_COPY_UTIL{" | head -3
          curl -s http://localhost:9401/metrics | grep "nvidia_smi_memory" | head -3
          echo "GPU Power Usage:"
          curl -s http://localhost:9400/metrics | grep "DCGM_FI_DEV_POWER_USAGE{" | head -3
          curl -s http://localhost:9401/metrics | grep "nvidia_smi_power" | head -3
          
          echo ""
          echo "=== GPU Stress Test Completed Successfully ==="
          echo "Total test duration: 5 minutes"
          echo "Check Grafana for detailed GPU metrics during this period."
      env:
        - name: NVIDIA_VISIBLE_DEVICES
          value: "all"
        - name: NVIDIA_DRIVER_CAPABILITIES
          value: "compute,utility"
        - name: RUN_NAME
          value: {% raw %} "{{workflow.parameters.run_name}}" 
          {% endraw %}
    # Pod specification with GPU resources
    podSpecPatch: |
      tolerations:
        - key: "node-memory-size"
          operator: "Equal"
          value: "large"
          effect: "NoSchedule"
      containers:
        - name: main
          resources:
            requests:
              memory: {% raw %} "{{inputs.parameters.memory_request}}Gi"
              {% endraw %}
              cpu: {% raw %} "{{inputs.parameters.cpu_request}}"
              {% endraw %}
              nvidia.com/gpu: 1
            limits:
              memory: {% raw %} "{{inputs.parameters.memory_limit}}Gi"
              {% endraw %}
              cpu: {% raw %} "{{inputs.parameters.cpu_limit}}"
              {% endraw %}
              nvidia.com/gpu: 1
    # Volumes for GPU access
    volumes:
      - name: nvidia-install-dir-host
        hostPath:
          path: /home/kubernetes/bin/nvidia
          type: DirectoryOrCreate
      - name: pod-resources
        hostPath:
          path: /var/lib/kubelet/pod-resources
          type: DirectoryOrCreate
    # Node affinity and tolerations
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: cloud.google.com/gke-accelerator
              operator: Exists
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
            - key: accelerator
              operator: In
              values:
              - nvidia-tesla-l4
              - nvidia-l4
        - weight: 90
          preference:
            matchExpressions:
            - key: gpu_node
              operator: In
              values:
              - "true"
        - weight: 80
          preference:
            matchExpressions:
            - key: node.kubernetes.io/instance-type
              operator: In
              values:
              - g2-standard-4
              - g2-standard-8
              - g2-standard-12
              - g2-standard-16
        - weight: 60
          preference:
            matchExpressions:
            - key: cloud.google.com/gke-accelerator
              operator: Exists
    tolerations:
    - key: "node-memory-size"
      operator: "Equal"
      value: "large"
      effect: "NoSchedule"
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "cloud.google.com/gke-accelerator"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "ToBeDeletedByClusterAutoscaler"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "node.kubernetes.io/not-ready"
      operator: "Exists"
      effect: "NoExecute"
      tolerationSeconds: 300
    - key: "node.kubernetes.io/unreachable"
      operator: "Exists"
      effect: "NoExecute"
      tolerationSeconds: 300
