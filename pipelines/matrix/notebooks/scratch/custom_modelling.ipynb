{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling walkthrough\n",
    "\n",
    "The purpose of this notebook is to show how a new, sample model with custom dependencies would be developed and integrated into the pipeline.\n",
    "\n",
    "This notebook follows a hypothetical scenario where Machine Learning Engineer Maya is developing a new model, with the aim of generating her own predictions of drug-disease treatment efficacy scores. Maya is new to the EveryCure / Matrix ecosystem, and is learning as she goes.\n",
    "\n",
    "In the end, she wants to train and submit a new model to the pipeline, and have it evaluated along with the other models.\n",
    "\n",
    "## Modelling assumptions\n",
    "\n",
    "Maya's goal is to train a new model that will predict the efficacy of drug-disease interactions.\n",
    "\n",
    "**Embeddings from Knowledge Graph:** Maya knows that EveryCure has generated embeddings for biomedical knowledge graph nodes, which meaningfully encode semantics of the nodes. Many of those nodes are drugs and diseases between which she wants to predict treatment efficacy.\n",
    "\n",
    "**Training Data:** Maya expects the training data to be a set of known positives and negatives, i.e. drug-disease pairs for which the treatment is known to be effective or ineffective.\n",
    "\n",
    "**Evaluation:** Maya assumes that the model will be evaluated using AUC-ROC. She also assumes that she will need to perform train-validation splits on her data, and that Matrix's pipeline downstream will be able to further test the predictions of her model.\n",
    "\n",
    "**Retrieving Data:** Importantly, Maya will retrieve the embeddings and training data from pipelines other than the modelling pipeline. She will avoid preprocessing the data itself as much as possible, relying on other resources provided by EveryCure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequesites\n",
    "\n",
    "Maya needs access to the GCS bucket containing the data (currently, `gs://mtrx-us-central1-hub-dev-storage`). Maya will use `gsutil` to copy the data to her local machine, and will find the exact paths to the files in the Kedro Data Catalog.\n",
    "\n",
    "## Required data sources\n",
    "\n",
    "### Embeddings \n",
    "\n",
    "Maya will use the embeddings generated by the Knowledge Graph pipeline to encode the drugs and disease into a vector space.\n",
    "\n",
    "In the embeddings pipeline, embeddings are extracted from Neo4j and saved to GCS. The pipeline is defined in the [embeddings pipeline](https://github.com/matrix-ml/matrix/blob/main/pipelines/matrix/src/matrix/pipelines/embeddings/pipeline.py). \n",
    "\n",
    "```python\n",
    "node(\n",
    "    func=nodes.extract_node_embeddings,\n",
    "    inputs={\n",
    "        \"nodes\": \"embeddings.model_output.graphsage\",\n",
    "        \"string_col\": \"params:embeddings.write_topological_col\",\n",
    "    },\n",
    "    outputs=\"embeddings.feat.nodes\",\n",
    "    name=\"extract_nodes_edges_from_db\",\n",
    "    tags=[\n",
    "        \"argowf.fuse\",\n",
    "        \"argowf.fuse-group.topological_embeddings\",\n",
    "        \"argowf.template-neo4j\",\n",
    "    ],\n",
    "),\n",
    "```\n",
    "\n",
    "Kedro Dataset to which the embeddings are saved: \n",
    "\n",
    "```yml\n",
    "embeddings.feat.nodes:\n",
    "  <<: *_spark_parquet\n",
    "  filepath: ${globals:paths.embeddings}/feat/nodes_with_embeddings\n",
    "```\n",
    "\n",
    "\n",
    "Maya knows that `${globals:paths.embeddings}/feat/nodes_with_embeddings` converts to `gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings`.\n",
    "\n",
    "She arbitrarily chooses to use the latest version of the embeddings, which happens to be `v0.2.4-rc.1`. She will run the command below to copy the data to her local machine.\n",
    "\n",
    "The files take up about 24GB of space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "\n",
    "!mkdir -p data/pca_embeddings\n",
    "\n",
    "!gsutil -m cp \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings/_SUCCESS\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings/part-00000-1a0c48cc-af15-4889-8eaf-c6aa98d21c98-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings/part-00001-1a0c48cc-af15-4889-8eaf-c6aa98d21c98-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings/part-00002-1a0c48cc-af15-4889-8eaf-c6aa98d21c98-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings/part-00003-1a0c48cc-af15-4889-8eaf-c6aa98d21c98-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings/part-00004-1a0c48cc-af15-4889-8eaf-c6aa98d21c98-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings/part-00005-1a0c48cc-af15-4889-8eaf-c6aa98d21c98-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings/part-00006-1a0c48cc-af15-4889-8eaf-c6aa98d21c98-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings/part-00007-1a0c48cc-af15-4889-8eaf-c6aa98d21c98-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings/part-00008-1a0c48cc-af15-4889-8eaf-c6aa98d21c98-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings/part-00009-1a0c48cc-af15-4889-8eaf-c6aa98d21c98-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings/part-00010-1a0c48cc-af15-4889-8eaf-c6aa98d21c98-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings/part-00011-1a0c48cc-af15-4889-8eaf-c6aa98d21c98-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings/part-00012-1a0c48cc-af15-4889-8eaf-c6aa98d21c98-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings/part-00013-1a0c48cc-af15-4889-8eaf-c6aa98d21c98-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings/part-00014-1a0c48cc-af15-4889-8eaf-c6aa98d21c98-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings/part-00015-1a0c48cc-af15-4889-8eaf-c6aa98d21c98-c000.snappy.parquet\" \\\n",
    "  data/embeddings/\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth data\n",
    "\n",
    "Maya needs to retrieve the training data from the preprocessing pipeline, containing True / False positives and negatives that she can use to train her model on the previously retrieved embeddings. First input to other modelling pipelines is `modelling.raw.ground_truth.positives@spark`, so Maya will retrieve that dataset first (together with its negative counterpart `modelling.raw.ground_truth.negatives@spark`).\n",
    "\n",
    "```python\n",
    "node(\n",
    "    func=nodes.create_int_pairs,\n",
    "    inputs=[\n",
    "        \"embeddings.feat.nodes\",\n",
    "        \"modelling.raw.ground_truth.positives@spark\",\n",
    "        \"modelling.raw.ground_truth.negatives@spark\",\n",
    "    ],\n",
    "    outputs=\"modelling.int.known_pairs@spark\",\n",
    "    name=\"create_int_known_pairs\",\n",
    "),\n",
    "```\n",
    "\n",
    "We retrieve ground truth data (conflated True Positives and True Negatives) from GCS. Both were produced by the `preprocessing` pipeline, as dataset `modelling.raw.ground_truth.positives@pandas` and `modelling.raw.ground_truth.negatives@pandas`, and will be read in as `@spark` dataframes by modelling steps. Maya will run the command below to copy the data to her local machine. Like in the previous step, the used version is arbitrary.\n",
    "\n",
    "Maya sees that other files live alongside the `*_conflated.tsv` files, and decides to download and investigate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/known_pairs\n",
    "\n",
    "!gsutil -m cp \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/01_raw/ground_truth/translator/v2.7.3/tn_pairs_conflated.tsv\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/01_raw/ground_truth/translator/v2.7.3/tp_pairs_conflated.tsv\" \\\n",
    "  data/known_pairs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "By now, Maya has obtained the embeddings and ground truth data. She will now preprocess the data to create the input for her model. She will also need to create splits for cross-validation.\n",
    "\n",
    "Maya will first inspect the ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# True positives\n",
    "df_tp = pd.read_csv(\"data/known_pairs/tp_pairs_conflated.tsv\", sep=\"\\t\")\n",
    "df_tp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True negatives\n",
    "\n",
    "df_tn = pd.read_csv(\"data/known_pairs/tn_pairs_conflated.tsv\", sep=\"\\t\")\n",
    "df_tn.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True positives and true negatives are represented as sets of source-target pairs. `source` is the drug, `target` is the disease.\n",
    "\n",
    "Now, Maya will inspect the embeddings that will be used as featured for her model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "raw_embeddings_directory = 'data/embeddings'\n",
    "sample_file_name = \"part-00000-1a0c48cc-af15-4889-8eaf-c6aa98d21c98-c000.snappy.parquet\"\n",
    "df= pd.read_parquet(os.path.join(raw_embeddings_directory, sample_file_name))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"category\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/embeddings_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, Maya (a) removed all pca_embeddings, (b) removed all entities which are not drugs or diseases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "filtered_embeddings_path = Path('data/embeddings_filtered/embeddings.parquet')\n",
    "\n",
    "if filtered_embeddings_path.exists():\n",
    "    print(\"Filtered embeddings already exist, deleting...\")\n",
    "    filtered_embeddings_path.unlink()\n",
    "\n",
    "categories_to_keep = set([\"biolink:DiseaseOrPhenotypicFeature\", \"biolink:Drug\", \"biolink:Disease\"])\n",
    "\n",
    "first_file = next(f for f in os.listdir(raw_embeddings_directory) if f.endswith('.parquet'))\n",
    "first_path = os.path.join(raw_embeddings_directory, first_file)\n",
    "first_chunk = pd.read_parquet(first_path)\n",
    "filtered_chunk = first_chunk[first_chunk[\"category\"].isin(categories_to_keep)]\n",
    "filtered_chunk = filtered_chunk[[\"topological_embedding\", \"id\"]]\n",
    "\n",
    "table = pa.Table.from_pandas(filtered_chunk)\n",
    "\n",
    "\n",
    "with pq.ParquetWriter(filtered_embeddings_path, schema=table.schema) as writer:\n",
    "    for file_no, file_name in enumerate(os.listdir(raw_embeddings_directory)):\n",
    "        print(f\"Processing file number {file_no}: {file_name}\")\n",
    "        if not file_name.endswith(\".parquet\"):\n",
    "            continue\n",
    "            \n",
    "        file_path = os.path.join(raw_embeddings_directory, file_name)\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        \n",
    "        for i in range(parquet_file.num_row_groups):\n",
    "            chunk = parquet_file.read_row_group(i).to_pandas()\n",
    "            filtered_chunk = chunk[chunk[\"category\"].isin(categories_to_keep)]\n",
    "            filtered_chunk = filtered_chunk[[\"topological_embedding\", \"id\"]]\n",
    "            writer.write_table(pa.Table.from_pandas(filtered_chunk))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maya filters down the ground truths to a simple list of node ids, to use in embedding filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ids = set(df_tn[\"target\"].unique()) | set(df_tn[\"source\"].unique()) | set(df_tp[\"target\"].unique()) | set(df_tp[\"source\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MATEUSZ tasks:\n",
    "\n",
    "\n",
    "1. Make the pairs dataset that will combine the embeddings and the ground truth data. \n",
    "Explain what goes into that dataset.\n",
    "\n",
    "2. Create splits. Explain what splits are.\n",
    "\n",
    "3. Make a model that will take all, and train it. Quantify the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Transform ground truth data into known_pairs. Explain what known_pairs actually are.\n",
    "2. Establish where the drugs / diseases are coming from, and how they are connected to the embeddings + what comes out of Matrix.\n",
    "3. Create splits.\n",
    "\n",
    "## Ground Truths\n",
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "Maya needs to create and train a new model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Mock data for simplicity\n",
    "np.random.seed(42)\n",
    "num_samples = 100\n",
    "num_features = 10\n",
    "\n",
    "X = np.random.rand(num_samples, num_features)  # Features\n",
    "true_weights = np.random.rand(num_features)    # True underlying weights\n",
    "y = (X @ true_weights + np.random.normal(0, 0.1, num_samples)) > 0.5  # Binary targets\n",
    "\n",
    "# Initialize and train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y.astype(int))  # Convert y to integers for compatibility\n",
    "\n",
    "# Predict on some test data\n",
    "X_test = np.random.rand(10, num_features)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(\"Predictions:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
