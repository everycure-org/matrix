{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new sample model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to show how a new, sample model with custom dependencies would be developed and integrated into the pipeline.\n",
    "\n",
    "The notebook produces a table with the evaluation metrics for the models. See the bottom of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %load_ext kedro.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import matrix.datasets.pair_generator as pair_generator\n",
    "from matrix.datasets.graph import KnowledgeGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "\n",
    "Maya needs access to the GCS bucket containing the data (currently, `gs://mtrx-us-central1-hub-dev-storage`). \n",
    "\n",
    "### Required data sources\n",
    "\n",
    "1. Embeddings derived from the Knowledge Graph\n",
    "2. Ground truth data\n",
    "\n",
    "Even though the raw embeddings live in the Kedro Data Catalog under name `embeddings.feat.nodes`, before they are ingested by the model they are processed under the modelling pipeline into `modelling.model_input.splits`. Similar preprocessing happens to the ground truth data, which is parsed into `modelling.int.known_pairs`. See the actual code in the [modelling pipeline](https://github.com/matrix-ml/matrix/blob/main/pipelines/matrix/src/matrix/pipelines/modelling/pipeline.py):\n",
    "\n",
    "\n",
    "```python\n",
    "create_model_input = pipeline(\n",
    "    [\n",
    "        # Construct ground_truth\n",
    "        node(\n",
    "            func=nodes.create_int_pairs,\n",
    "            inputs=[\n",
    "                \"embeddings.feat.nodes\",\n",
    "                \"modelling.raw.ground_truth.positives@spark\",\n",
    "                \"modelling.raw.ground_truth.negatives@spark\",\n",
    "            ],\n",
    "            outputs=\"modelling.int.known_pairs@spark\",\n",
    "            name=\"create_int_known_pairs\",\n",
    "        ),\n",
    "        node(\n",
    "            func=nodes.prefilter_nodes,\n",
    "            inputs=[\n",
    "                \"embeddings.feat.nodes\",\n",
    "                \"modelling.raw.ground_truth.positives@spark\",\n",
    "                \"params:modelling.drug_types\",\n",
    "                \"params:modelling.disease_types\",\n",
    "            ],\n",
    "            outputs=\"modelling.model_input.drugs_diseases_nodes@spark\",\n",
    "            name=\"prefilter_nodes\",\n",
    "        ),\n",
    "        node(\n",
    "            func=nodes.make_splits,\n",
    "            inputs=[\n",
    "                \"modelling.int.known_pairs@pandas\",\n",
    "                \"params:modelling.splitter\",\n",
    "            ],\n",
    "            outputs=\"modelling.model_input.splits\",\n",
    "            name=\"create_splits\",\n",
    "        ),\n",
    "    ],\n",
    "    tags=[model[\"model_name\"] for model in settings.DYNAMIC_PIPELINES_MAPPING.get(\"modelling\")],\n",
    ")\n",
    "```\n",
    "\n",
    "Importantly, in this example we will use non-preprocessed and PCA-reduced embeddings, and raw ground truth data that we will preprocess in our notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "We retrieve the version of RTX embeddings that had been reduced to 100 dimensions using PCA. Normal models use embeddings with all dimensions, but for the sake of this example, we will use the PCA version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "\n",
    "!mkdir -p data/pca_embeddings\n",
    "\n",
    "!gsutil -m cp \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00000-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00001-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00002-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00003-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00004-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00005-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00006-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00007-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00008-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00009-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00010-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00011-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00012-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00013-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00014-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00015-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00016-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00017-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00018-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00019-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00020-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00021-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00022-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00023-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00024-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00025-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00026-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00027-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00028-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00029-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rtx-only/datasets/embeddings/feat/tmp_nodes_with_pca_embeddings/part-00030-689eea27-3cdf-4ce8-b3c2-3baf1b89f750-c000.snappy.parquet\" \\\n",
    "  data/pca_embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/ground_truth_data\n",
    "\n",
    "!gsutil -m cp \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/01_raw/ground_truth_data/tn_pairs.txt\" \\\n",
    "  \"gs://mtrx-us-central1-hub-dev-storage/kedro/data/01_raw/ground_truth_data/tp_pairs.txt\" \\\n",
    "  data/known_pairs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "It is notoriously difficult to find the appropriate files in GCS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth data with test-train splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_pairs = pd.read_parquet('/Users/alexei/Documents/data/e2e_14_aug/releases_20240807_05_model_input_splits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KGs with embeddings (KGML-xDTD, old run and new run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input/rtx_kg2_nodes_chunyu', 'rb') as f:\n",
    "    rtx_kg2 = pd.read_parquet(f)\n",
    "rtx_kg2 = rtx_kg2.rename(columns={'embedding': 'topological_embedding'})\n",
    "graph_chunyu = KnowledgeGraph(rtx_kg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/alexei/Documents/data/e2e_14_aug/releases_20240807_04_feature_rtx_kg2_nodes', 'rb') as f:\n",
    "    rtx_kg2 = pd.read_parquet(f)\n",
    "graph_first_run = KnowledgeGraph(rtx_kg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models using Chunyu's embeddings\n",
    "xg_synth_chunyu = joblib.load('/Users/alexei/Documents/repos/matrix/pipelines/matrix/notebooks/scratch/local/generate_matrix/input/xg_balanced_retrain.joblib')\n",
    "rf_chunyu = joblib.load('/Users/alexei/Documents/repos/matrix/pipelines/matrix/notebooks/scratch/local/misc/train-kgml-xdtd/output/kgml_xdtd_split.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model using embeddings from first run\n",
    "with open(\"/Users/alexei/Documents/data/e2e_14_aug/releases_20240807_06_models_xgc_model.pickle\", \"rb\") as f:\n",
    "    xg_ensemble_first_run = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other objects needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model params\n",
    "with open('/Users/alexei/Documents/repos/matrix/pipelines/matrix/conf/base/modelling/parameters/defaults.yml', 'r') as f:\n",
    "    model_params_defaults = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load objects needed for predictions\n",
    "score_col_name = 'treat score'\n",
    "features = model_params_defaults['_model_options']['model_tuning_args']['features']\n",
    "transformers = model_params_defaults['_model_options']['transformers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load drug flags\n",
    "with open('/Users/alexei/Documents/repos/matrix/pipelines/matrix/conf/base/modelling/parameters/xg_baseline.yml', 'r') as f:\n",
    "    model_params_baseline = yaml.safe_load(f)\n",
    "drug_flags = model_params_baseline['modelling.xg_baseline']['_overrides']['generator']['drug_flags']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disease-centric matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating disease-centric matrix\n",
    "matrix = pair_generator.MatrixTestDiseases(drug_flags)\n",
    "matrix = matrix.generate(graph_first_run, known_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "data = matrix.copy()\n",
    "mat_preds_xg_synth_chunyu = make_test_predictions(graph_chunyu, data, transformers, xg_synth_chunyu, features, score_col_name)\n",
    "data = matrix.copy()\n",
    "mat_preds_rf_chunyu = make_test_predictions(graph_chunyu, data, transformers, rf_chunyu, features, score_col_name)\n",
    "data = matrix.copy()\n",
    "mat_preds_xg_ensemble_first_run = make_test_predictions(graph_first_run, data, transformers, xg_ensemble_first_run, features, score_col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_data  = pair_generator.GroundTruthTestPairs()\n",
    "gt_data = gt_data.generate(graph_first_run, known_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "data = gt_data.copy()\n",
    "gt_preds_xg_synth_chunyu = make_test_predictions(graph_chunyu, data, transformers, xg_synth_chunyu, features, score_col_name)\n",
    "data = gt_data.copy()\n",
    "gt_preds_rf_chunyu = make_test_predictions(graph_chunyu, data, transformers, rf_chunyu, features, score_col_name)\n",
    "data = gt_data.copy()\n",
    "gt_preds_xg_ensemble_first_run = make_test_predictions(graph_first_run, data, transformers, xg_ensemble_first_run, features, score_col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading evaluation options from the catalog\n",
    "with open('/Users/alexei/Documents/repos/matrix/pipelines/matrix/conf/base/evaluation/parameters.yml', 'r') as f:\n",
    "    eval_params = yaml.safe_load(f)\n",
    "spec_ranking_options = eval_params['evaluation.disease_specific_ranking']['evaluation_options']['evaluation']\n",
    "mat_ranking_options = eval_params['evaluation.disease_centric_matrix']['evaluation_options']['evaluation']\n",
    "classification_options = eval_params['evaluation.simple_ground_truth_classification']['evaluation_options']['evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing metrics with the pipeline\n",
    "spec_ranking_xg_synth_chunyu = evaluate_test_predictions(mat_preds_xg_synth_chunyu, spec_ranking_options)\n",
    "mat_ranking_xg_synth_chunyu = evaluate_test_predictions(mat_preds_xg_synth_chunyu, mat_ranking_options)\n",
    "classification_xg_synth_chunyu = evaluate_test_predictions(gt_preds_xg_synth_chunyu, classification_options)\n",
    "spec_ranking_rf_chunyu = evaluate_test_predictions(mat_preds_rf_chunyu, spec_ranking_options)\n",
    "mat_ranking_rf_chunyu = evaluate_test_predictions(mat_preds_rf_chunyu, mat_ranking_options)\n",
    "classification_rf_chunyu = evaluate_test_predictions(gt_preds_rf_chunyu, classification_options)\n",
    "spec_ranking_xg_ensemble_first_run = evaluate_test_predictions(mat_preds_xg_ensemble_first_run, spec_ranking_options)\n",
    "mat_ranking_xg_ensemble_first_run = evaluate_test_predictions(mat_preds_xg_ensemble_first_run, mat_ranking_options)\n",
    "classification_xg_ensemble_first_run = evaluate_test_predictions(gt_preds_xg_ensemble_first_run, classification_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_lst = ['xg_synth_chunyu', 'rf_chunyu', 'xg_ensemble_first_run']\n",
    "metric_name_lst = ['auroc', 'ap', 'mrr', 'hit2', 'hit10', 'hit100', 'acc', 'f1']\n",
    "auroc_lst = [eval('mat_ranking_'+model_name)['roc_auc_score'] for model_name in model_name_lst]\n",
    "ap_lst = [eval('mat_ranking_'+model_name)['average_precision_score'] for model_name in model_name_lst]\n",
    "mrr_lst = [eval('spec_ranking_'+model_name)['mrr'] for model_name in model_name_lst]\n",
    "hit2_lst = [eval('spec_ranking_'+model_name)['hit-2'] for model_name in model_name_lst]\n",
    "hit10_lst = [eval('spec_ranking_'+model_name)['hit-10'] for model_name in model_name_lst]\n",
    "hit100_lst = [eval('spec_ranking_'+model_name)['hit-100'] for model_name in model_name_lst]\n",
    "acc_lst = [eval('classification_'+model_name)['accuracy_score'] for model_name in model_name_lst]\n",
    "f1_lst = [eval('classification_'+model_name)['f1_score'] for model_name in model_name_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tedious manual input of latest results (new run 26 Aug)\n",
    "\n",
    "results_xg_ensemble_new_run_3 = {'auroc': 0.9219692860724151,\n",
    "'ap': 0.043389910213820035,\n",
    "'mrr': 0.19728144176219592,\n",
    "'hit2': 0.17863805970149255,\n",
    "'hit10': 0.314365671641791,\n",
    "'hit100': 0.6203358208955224,\n",
    "'acc': 0.8574043565806333,\n",
    "'f1': 0.7843897038472184\n",
    "}\n",
    "results_xg_synth_new_run_3 = {\n",
    "'auroc': 0.8383403010392563,\n",
    "'ap': 0.007503326148252967,\n",
    "'mrr': 0.10459264807623478,\n",
    "'hit2': 0.08861940298507463,\n",
    "'hit10': 0.17583955223880596,\n",
    "'hit100': 0.4398320895522388,\n",
    "'acc': 0.9101226432363171,\n",
    "'f1': 0.8768497617256082\n",
    "}\n",
    "results_xg_ensemble_new_run_6 = {\n",
    "'mrr': 0.14736130097929595,\n",
    "'hit2': 0.13013059701492538,\n",
    "'hit10': 0.2462686567164179,\n",
    "'hit100': 0.554570895522388,\n",
    "'auroc': 0.9055752875277525,\n",
    "'ap': 0.028384013211504897,\n",
    "'acc': 0.8411129416071755,\n",
    "'f1': 0.7535491198182851\n",
    "}\n",
    "results_xg_synth_new_run_6 = {'ap': 0.005491298670828154,\n",
    "'hit100': 0.376865671641791,\n",
    "'auroc': 0.8248684530632286,\n",
    "'mrr': 0.06576272993950255,\n",
    "'hit2': 0.05177238805970149,\n",
    "'hit10': 0.11800373134328358,\n",
    "'acc': 0.9038989566172433,\n",
    "'f1': 0.8678580417820287\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added by piotr\n",
    "model_name_list = ['xg_ensemble_new_run_1', 'xg_synth_new_run_1','xg_ensemble_new_run_3', 'xg_synth_new_run_3', 'xg_ensemble_new_run_4', 'xg_synth_new_run_4','xg_ensemble_new_run_5', 'xg_synth_new_run_5','xg_ensemble_new_run_6', 'xg_synth_new_run_6']\n",
    "model_name_list = [eval('results_' + model_name) for model_name in model_name_list]\n",
    "df_new_runs = pd.concat(model_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added by piotr\n",
    "df_all_runs = pd.concat([df_chunyu,df_new_runs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[model_name_lst = 'xg_ensemble_new_run_1', 'xg_synth_new_run_1','xg_ensemble_new_run_4', 'xg_synth_new_run_4','xg_ensemble_new_run_5', 'xg_synth_new_run_5','xg_ensemble_new_run_3', 'xg_synth_new_run_3', 'xg_ensemble_new_run_6', 'xg_synth_new_run_6']\n",
    "for model_name in model_name_lst[:]:\n",
    "    for metric_name in metric_name_lst:\n",
    "        eval(metric_name + '_lst').append(eval('results_' + model_name)[metric_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({metric_name: eval(metric_name + '_lst') for metric_name in metric_name_lst}, index=model_name_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to highlight the best model for each metric\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "styled_df = df_all_runs.style.apply(highlight_max, axis=0)\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continued by Piotr - 29-08-2024\n",
    "Now that we have most results (except for setup 2), I will compare them all incl. PCA plots (stored in MLFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recreating\n",
    "df_chunyu = pd.DataFrame({\n",
    "    'auroc': [0.867708, 0.741243, 0.844903],\n",
    "    'ap': [0.019273, 0.006884, 0.008687],\n",
    "    'mrr': [0.153431, 0.129286, 0.068847],\n",
    "    'hit2': [0.132929, 0.120336, 0.047575],\n",
    "    'hit10': [0.271455, 0.197295, 0.132929],\n",
    "    'hit100': [0.578358, 0.423041, 0.398787],\n",
    "    'acc': [0.926231, 0.882482, 0.782720],\n",
    "    'f1': [0.900074, 0.833420, 0.629410]\n",
    "}, index=['xg_synth_chunyu', 'rf_chunyu', 'xg_ensemble_first_run'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tedious manual input of latest results (new run 26 Aug)\n",
    "# updated by piotr (29-08-2024)\n",
    "\n",
    "results_xg_ensemble_new_run_3 = pd.DataFrame({'auroc': 0.9219692860724151,\n",
    "'ap': 0.043389910213820035,\n",
    "'mrr': 0.19728144176219592,\n",
    "'hit2': 0.17863805970149255,\n",
    "'hit10': 0.314365671641791,\n",
    "'hit100': 0.6203358208955224,\n",
    "'acc': 0.8574043565806333,\n",
    "'f1': 0.7843897038472184\n",
    "}, index=['xg_ensemble_new_run_3'])\n",
    "results_xg_synth_new_run_3 = pd.DataFrame({\n",
    "'auroc': 0.8383403010392563,\n",
    "'ap': 0.007503326148252967,\n",
    "'mrr': 0.10459264807623478,\n",
    "'hit2': 0.08861940298507463,\n",
    "'hit10': 0.17583955223880596,\n",
    "'hit100': 0.4398320895522388,\n",
    "'acc': 0.9101226432363171,\n",
    "'f1': 0.8768497617256082\n",
    "}, index=['xg_synth_new_run_3'])\n",
    "\n",
    "results_xg_ensemble_new_run_6 = pd.DataFrame({\n",
    "'mrr': 0.14736130097929595,\n",
    "'hit2': 0.13013059701492538,\n",
    "'hit10': 0.2462686567164179,\n",
    "'hit100': 0.554570895522388,\n",
    "'auroc': 0.9055752875277525,\n",
    "'ap': 0.028384013211504897,\n",
    "'acc': 0.8411129416071755,\n",
    "'f1': 0.7535491198182851\n",
    "}, index=['xg_ensemble_new_run_6'])\n",
    "results_xg_synth_new_run_6 = pd.DataFrame({'ap': 0.005491298670828154,\n",
    "'hit100': 0.376865671641791,\n",
    "'auroc': 0.8248684530632286,\n",
    "'mrr': 0.06576272993950255,\n",
    "'hit2': 0.05177238805970149,\n",
    "'hit10': 0.11800373134328358,\n",
    "'acc': 0.9038989566172433,\n",
    "'f1': 0.8678580417820287\n",
    "}, index=['xg_synth_new_run_6'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated by piotr (29-08-2024)\n",
    "# Tedious manual input of latest results (new run 26 Aug)\n",
    "\n",
    "results_xg_ensemble_new_run_1 = pd.DataFrame({'auroc': 0.9205314905728452,\n",
    "'ap': 0.019094170910145973,\n",
    "'mrr': 0.1327579085776529,\n",
    "'hit2': 0.11847014925373134,\n",
    "'hit10': 0.21315298507462688,\n",
    "'hit100': 0.4855410447761194,\n",
    "'acc': 0.7913234486545854,\n",
    "'f1': 0.5530636737628546,\n",
    "}, index=['xg_ensemble_new_run_1'])\n",
    "\n",
    "results_xg_synth_new_run_1 = pd.DataFrame({\n",
    "'auroc': 0.938394157682914,\n",
    "'ap': 0.00311571370242041,\n",
    "'mrr': 0.04100465783353993,\n",
    "'hit2': 0.032182835820895525,\n",
    "'hit10': 0.07136194029850747,\n",
    "'hit100': 0.28451492537313433,\n",
    "'acc': 0.8795533589602782,\n",
    "'f1': 0.5898006125192427\n",
    "}, index=['xg_synth_new_run_1'])\n",
    "\n",
    "results_xg_ensemble_new_run_4 = pd.DataFrame({\n",
    "'mrr': 0.19004647175750133,\n",
    "'hit2': 0.16511194029850745,\n",
    "'hit10': 0.3162313432835821,\n",
    "'hit100': 0.6324626865671642,\n",
    "'auroc': 0.9691564117496279,\n",
    "'ap': 0.04163573984107834,\n",
    "'acc': 0.8378180486911954,\n",
    "'f1': 0.5825615645694927\n",
    "}, index=['xg_ensemble_new_run_4'])\n",
    "results_xg_synth_new_run_4 = pd.DataFrame({\n",
    "'ap': 0.008760532171652465,\n",
    "'hit100': 0.46548507462686567,\n",
    "'auroc': 0.9755692012969199,\n",
    "'mrr': 0.11174423545734395,\n",
    "'hit2': 0.09188432835820895,\n",
    "'hit10': 0.19682835820895522,\n",
    "'acc': 0.9104887424492037,\n",
    "'f1': 0.6150820759908692\n",
    "}, index=['xg_synth_new_run_4'])\n",
    "\n",
    "results_xg_ensemble_new_run_5 = pd.DataFrame({\n",
    "'mrr': 0.14120923929593135,\n",
    "'hit2': 0.1226679104477612,\n",
    "'hit10': 0.25046641791044777,\n",
    "'hit100': 0.5783582089552238,\n",
    "'auroc': 0.9496864937143991,\n",
    "'ap': 0.027617631373212527,\n",
    "'acc': 0.8310452132527915,\n",
    "'f1': 0.5783746628159361\n",
    "}, index=['xg_ensemble_new_run_5'])\n",
    "\n",
    "results_xg_synth_new_run_5 = pd.DataFrame({\n",
    "'ap': 0.006890048606767505,\n",
    "'hit100': 0.40671641791044777,\n",
    "'auroc': 0.957490908150954,\n",
    "'mrr': 0.07863691385785539,\n",
    "'hit2': 0.06343283582089553,\n",
    "'hit10': 0.14412313432835822,\n",
    "'acc': 0.9022515101592532,\n",
    "'f1': 0.6104670872531631\n",
    "}, index=['xg_synth_new_run_5'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_list = ['xg_ensemble_new_run_1', 'xg_synth_new_run_1','xg_ensemble_new_run_3', 'xg_synth_new_run_3', 'xg_ensemble_new_run_4', 'xg_synth_new_run_4','xg_ensemble_new_run_5', 'xg_synth_new_run_5','xg_ensemble_new_run_6', 'xg_synth_new_run_6']\n",
    "model_name_list = [eval('results_' + model_name) for model_name in model_name_list]\n",
    "df_new_runs = pd.concat(model_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_runs = pd.concat([df_chunyu,df_new_runs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to highlight the best model for each metric\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "styled_df = df_all_runs.style.apply(highlight_max, axis=0)\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA plots examination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open('scratch/pca_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open('scratch/pca_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open('scratch/pca_3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open('scratch/pca_4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open('scratch/pca_5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open('scratch/pca_6.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCAs look much better, no parabolic shapes, with clusters being the most visible for setups 5 and 6. PCAs from setup 3 and 4 are a bit more mixed which could be due to lower learning rate/higher number of iterations. PCAs from setups 1 and 2 are the most similar to Chunyus. We need to note though that PCAs are not indicative of downstream task performance and at the end of the day, its only model performance that matters.\n",
    "\n",
    "Nevertheless, the problem of wiggly spaghetti-like PCAs is now solved and could be assinged to the sigmoid activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best results were achieved by ensemble models from run 3 and run 4; the results by these two setups are comparable for majority of metrics but setup 3 was superior in terms of F1 score where setup 4 scored much lower. Setups 3 and 4 had different search depth (100 vs 4) showing that it is not a significant factor in the performance (at least for our case). Both setups 3 and 4 had **learning rate 0.01** (lr of 0.1 was likely too high, leading to worse performance of setup 1, 5, and 6).  Both setup 3 and setup 4 also had **max iter 100**, meaning that now that we have ReLU activation, longer training is not necessarily bad. While remaining setups did have higher learning rate and lower number of max iteration (0.1 and 10 respectively), lets remember that with lower learning rate we need to have higher number of iterations/epochs as our 'steps' during weights optimization are smaller.\n",
    "\n",
    "In terms of Sample size - while we dont have results from setup 2 to compare, setup 1 (25,10) and setup 5 (96,96) show the effect of increasing the sample size per layer. Setup 5 achieved better results across all metrics than setup 1 did, **potentially showing that sampleSize [96,96] is more optimal for our use case** (thats the same one that Chunyu used). It would be interesting to see if setups 3 and setup 4 performance would improve upon having such sample size.\n",
    "\n",
    "In summary, we are at a good place with the embeddings as Alexei has mentioned - GraphSAGE is now optimized well (however there is likely some room for improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental setups (for reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Setups\n",
    "\n",
    "    ##### setup 1 \n",
    "    activation: ReLU\n",
    "    maxIterations: 10\n",
    "    epochs: 10\n",
    "    learningRate: 0.1\n",
    "    sampleSizes: [25, 10]\n",
    "    searchDepth: 100\n",
    "\n",
    "    ##### setup 2 \n",
    "    activation: ReLU\n",
    "    maxIterations: 10\n",
    "    epochs: 10\n",
    "    learningRate: 0.1\n",
    "    sampleSizes: [25, 10]\n",
    "    searchDepth: 4\n",
    "\n",
    "    ##### setup 3 \n",
    "    activation: ReLU\n",
    "    maxIterations: 100\n",
    "    epochs: 10\n",
    "    learning rate - 0.01\n",
    "    sampleSizes - [25, 10]\n",
    "    searchDepth: 100\n",
    "\n",
    "    ##### setup 4\n",
    "    activation: ReLU\n",
    "    maxIterations: 100\n",
    "    epochs: 10\n",
    "    learning rate - 0.01\n",
    "    sampleSizes - [25, 10]\n",
    "    searchDepth: 4\n",
    "\n",
    "    ##### setup 5 \n",
    "    activation: ReLU\n",
    "    maxIterations: 10\n",
    "    epochs: 10\n",
    "    learningRate: 0.1\n",
    "    sampleSizes: [96, 96]\n",
    "    searchDepth: 100\n",
    "\n",
    "    ##### setup 6 \n",
    "    activation: ReLU\n",
    "    maxIterations: 10\n",
    "    epochs: 10\n",
    "    learningRate: 0.1\n",
    "    sampleSizes: [96, 96]\n",
    "    searchDepth: 4\n",
    "\n",
    "\n",
    "#### Potential next experiments\n",
    "\n",
    "To ensure what the optimal number of max iterations are \n",
    "\n",
    "    ##### setup 3\n",
    "    activation: ReLU\n",
    "    maxIterations: 10\n",
    "    epochs: 10\n",
    "    learning rate - 0.01\n",
    "    sampleSizes - [25, 10]\n",
    "    searchDepth: 100\n",
    "\n",
    "    ##### setup 1 \n",
    "    activation: ReLU\n",
    "    maxIterations: 100\n",
    "    epochs: 10\n",
    "    learningRate: 0.1\n",
    "    sampleSizes: [25, 10]\n",
    "    searchDepth: 100\n",
    "\n",
    "\n",
    "To see the effect of SampleSize\n",
    "\n",
    "    ##### setup 3\n",
    "    activation: ReLU\n",
    "    maxIterations: 100\n",
    "    epochs: 10\n",
    "    learning rate - 0.01\n",
    "    sampleSizes - [96, 96]\n",
    "    searchDepth: 100\n",
    "\n",
    "\n",
    "    ##### setup 4\n",
    "    activation: ReLU\n",
    "    maxIterations: 100\n",
    "    epochs: 10\n",
    "    learning rate - 0.01\n",
    "    sampleSizes - [96, 96]\n",
    "    searchDepth: 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
