{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA - Embeddings\n",
    "\n",
    "In this notebook I will conduct EDA of node embeddings generated with help of two LLMs and one non-LLM method:\n",
    "* OpenAI - generic text-embedding-3-small model with concurrency 50, currently implemented in our pipeline\n",
    "* PubMedBert - biomedical embedding model, used in KGML-xDTD publication\n",
    "* Spacy - generic pipeline with a pretrained language model - *en_core_web_md, web data training*\n",
    "* SciSpacy - biomedical pipeline with a pretrained language model - *en_core_sci_md, biomedical data training*\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "I created a (stratified) sample dataframe which consists of 93449 nodes (original has ~3.5 million nodes). For each node, I then generated embeddings using the models mentioned above (calculated them with a standalone script). Then I conducted a quantiative and qualitative analysis - compared distribution of embeddings, their similarity cosine/euclidean distance, PCA, tSNE. Finally I examined potential data leakage by comparing similarity of embeddings of drug nodes which are known to have treat/no treat relationship with disease nodes (based on GT dataset) - for better 'big picture' I also compared the similarity between drug nodes and disease nodes with no known relationships. \n",
    "\n",
    "Aditionally I examined mean different embedding generation approaches to compare Chunyus approach with different pooliong strategy - still WIP\n",
    "\n",
    "\n",
    "ToC:\n",
    "* Node Data - where I load node df from kedro, conduct stratified sampling based on category, and save subsamples locally\n",
    "* Load embeddings - where I load embeddings (post-calculation which is done in a standalone script)\n",
    "* Qualitative and quantitative assessment - comparison of embeddings through visualisation (tSNE, PCA), cosine similarity (post- and pre-PCA), distribution ks test\n",
    "* Data Leakage Assessment - loading gt datasets and checking similarity between drug-disease pairs\n",
    "* PubMedBERT vs PubMedBERT embeddings - compare LLM used by Chunyu vs sentence transformer of the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.spatial import distance\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import pyspark as ps\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, concat_ws\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "# Setting the root path and changing the directory\n",
    "root_path = subprocess.check_output(['git', 'rev-parse', '--show-toplevel']).decode().strip()\n",
    "os.chdir(Path(root_path) / 'pipelines' / 'matrix')\n",
    "\n",
    "%load_ext kedro.ipython\n",
    "%reload_kedro  --env base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data \n",
    "input_nodes_raw = catalog.load('integration.model_input.nodes')\n",
    "input_nodes_raw.show()\n",
    "\n",
    "#show categories\n",
    "input_nodes_raw.groupby('category').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for quick inspeciton we want to have a representative sample: we dont care about connectivity FOR NOW so we can create a 90 000 node\n",
    "# this way we can also just obtain embeddings in a notebook (we can explore full embeddings later)\n",
    "\n",
    "def stratified_sample(df, sample_size: int=90000, stratify_col: str='category'):\n",
    "    total_count = df.count()\n",
    "    category_counts = df.groupBy(stratify_col).count().collect()\n",
    "    fractions = {row[stratify_col]: sample_size * (row['count'] / total_count) / row['count'] for row in category_counts}\n",
    "    \n",
    "    print(fractions)\n",
    "    sampled_df = df.sampleBy(stratify_col, fractions, seed=42)\n",
    "    return sampled_df, fractions\n",
    "\n",
    "sample, fracs = stratified_sample(input_nodes_raw)\n",
    "\n",
    "sample.show()\n",
    "sample.count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subsample has 93 000 nodes (contrary to original )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features from the sample df and save\n",
    "sample = sample_spark.withColumn(\"feat\", concat_ws(\"+\", col(\"name\"), col(\"category\")))\n",
    "features = sample.select('feat').rdd.flatMap(lambda x:x).collect()\n",
    "joblib.dump(features, 'sm_sample_features.joblib')\n",
    "\n",
    "#save sample df in parquet\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Protob Conversion to Parquet\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sample.write.parquet(\"sm_sample_df.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings\n",
    "\n",
    "I calculated these embeddings using a python script (compute_embeddings) - code is below. PubMedBERT was much slower than OpenAI for the same subset and using the same batches (not entirely comparables as OpenAI has an API call but this still shows spacy being much faster)\n",
    "\n",
    "PubMedBERT for subsample 5164 s\n",
    "OpenAI for subsample 3120 s\n",
    "spaCy for subsample 1200s (with ner included )\n",
    "sciSpacy for subsamle - 920s (ner excl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load embedding: turn into df\n",
    "\n",
    "#pubmedbert\n",
    "pubmed_emb = joblib.load('scratch/pubmedbert_sm_embed.joblib')\n",
    "\n",
    "#openai\n",
    "openai_emb = np.array(joblib.load('scratch/openai_sm_embed.joblib'))\n",
    "\n",
    "#spacy\n",
    "spacy_emb = joblib.load('scratch/spacy_md_sm_embed.joblib')\n",
    "\n",
    "#scispacy\n",
    "scispacy_emb = joblib.load('scratch/scispacy_sm_embed.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative and Quantitative assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check distribution of embeddings\n",
    "plt.hist(np.array(pubmed_emb).flatten(), bins=50, alpha=0.5, label='pubmed')\n",
    "plt.hist(np.array(openai_emb).flatten(), bins=50, alpha=0.5, label='openai')\n",
    "plt.hist(np.array(spacy_emb).flatten(), bins=50, alpha=0.5, label='spacy')\n",
    "plt.hist(np.array(scispacy_emb).flatten(), bins=50, alpha=0.5, label='scispacy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.suptitle('distribution of values in embeddings')\n",
    "plt.savefig('distribution_plot.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantitative assessment - distance between PubMedBERT and OpenAI only as they have the same length\n",
    "\n",
    "dist_dict = {'pubmed':np.array(pubmed_emb),'openai':np.array(openai_emb)}\n",
    "\n",
    "for name in dist_dict.keys():\n",
    "    print(name)\n",
    "    new_dict = dist_dict.copy()\n",
    "    new_dict.pop(name)\n",
    "    for name2 in new_dict.keys():\n",
    "        euc_dist=[]\n",
    "        cos_dist=[]\n",
    "        for i in range(len(dist_dict[name])):\n",
    "            euc_dist.append(distance.euclidean(dist_dict[name][i], new_dict[name2][i]))\n",
    "            cos_dist.append(cosine_similarity(dist_dict[name][i].reshape(1,-1), new_dict[name2][i].reshape(1,-1)))\n",
    "\n",
    "print('euc')\n",
    "print('mean', np.mean(euc_dist), 'std', np.std(euc_dist))\n",
    "print('cos')\n",
    "print('mean', np.mean(cos_dist), 'std', np.std(cos_dist))\n",
    "\n",
    "#sanity to check they come from diff distributions\n",
    "print(ks_2samp(np.array(pubmed_emb).flatten(), np.array(openai_emb).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA to ensure they are all 100 components long\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "pubmed_pca = pca.fit_transform(pubmed_emb)\n",
    "openai_pca = pca.fit_transform(openai_emb)\n",
    "spacy_pca = pca.fit_transform(spacy_emb)\n",
    "scispacy_pca = pca.fit_transform(scispacy_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare similarity post pca\n",
    "\n",
    "dist_dict = {'pubmed':pubmed_pca,'openai':openai_pca, 'scispacy':scispacy_pca, 'spacy':spacy_pca}\n",
    "\n",
    "for name in dist_dict.keys():\n",
    "    new_dict = dist_dict.copy()\n",
    "    new_dict.pop(name)\n",
    "    for name2 in new_dict.keys():\n",
    "        print(name, '-', name2)\n",
    "        euc_dist=[]\n",
    "        cos_dist=[]\n",
    "        for i in range(len(dist_dict[name])):\n",
    "            euc_dist.append(distance.euclidean(dist_dict[name][i], new_dict[name2][i]))\n",
    "            cos_dist.append(cosine_similarity(dist_dict[name][i].reshape(1,-1), new_dict[name2][i].reshape(1,-1)))\n",
    "        print('euc distance results')\n",
    "        print('mean', np.mean(euc_dist), 'std', np.std(euc_dist))\n",
    "        print('cosine similarity results')\n",
    "        print('mean', np.mean(cos_dist), 'std', np.std(cos_dist), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the distribution, distance & similarity checks so far:\n",
    "* no embeddings are identical or close to identical\n",
    "* the histogram shows that the distribution partially overlaps for OpenAI, PubMedBERT and SciSpacy but definitely not the same distribution\n",
    "* spacy embeddings are 'an outcast' as they are very different from remaining embeddings\n",
    "* openai and pubmedbert are the most similar to each other out of four, however scispacy is not hugely different either"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tSNE - 2 components\n",
    "\n",
    "tSNE should be able to indicate the ability of the models to cluster nodes and how well they align with their categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=30.0)\n",
    "pubmed_tsne = tsne.fit_transform(np.array(pubmed_emb))\n",
    "openai_tsne = tsne.fit_transform(np.array(openai_emb))\n",
    "spacy_tsne = tsne.fit_transform(np.array(spacy_emb))\n",
    "scispacy_tsne = tsne.fit_transform(np.array(scispacy_emb))\n",
    "\n",
    "#turn into df, concat with category for tsne\n",
    "pubmed_emb_df = pd.DataFrame(pubmed_tsne)\n",
    "openai_emb_df = pd.DataFrame(openai_tsne)\n",
    "spacy_emb_df = pd.DataFrame(spacy_tsne)\n",
    "scispacy_emb_df = pd.DataFrame(scispacy_tsne)\n",
    "\n",
    "#read sample metadata\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Protob Conversion to Parquet\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "metadata=spark.read.parquet('sm_sample_df.parquet').toPandas()\n",
    "\n",
    "#concat\n",
    "pubmed_emb_df = pd.concat([metadata,pubmed_emb_df], axis=1)\n",
    "openai_emb_df = pd.concat([metadata,openai_emb_df], axis=1)\n",
    "spacy_emb_df = pd.concat([metadata,spacy_emb_df], axis=1)\n",
    "scispacy_emb_df = pd.concat([metadata,scispacy_emb_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.groupby('category').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#modify colors per category\n",
    "plt.subplots(1,4, figsize=(25,10))\n",
    "plt.subplot(1,4,1)\n",
    "sns.scatterplot(data = pubmed_emb_df, x=0, y=1, hue='category', alpha=0.2, legend=False)\n",
    "plt.title('pubmed')\n",
    "plt.xlabel('tsne 1')\n",
    "plt.ylabel('tsne 2')\n",
    "plt.subplot(1,4,2)\n",
    "sns.scatterplot(data = spacy_emb_df, x=0, y=1, hue='category', alpha=0.2, legend=False)\n",
    "plt.title('spacy')\n",
    "plt.xlabel('tsne 1')\n",
    "plt.ylabel('tsne 2')\n",
    "plt.subplot(1,4,3)\n",
    "sns.scatterplot(data = openai_emb_df, x=0, y=1, hue='category', alpha=0.2, legend=False)\n",
    "plt.title('openai')\n",
    "plt.xlabel('tsne 1')\n",
    "plt.ylabel('tsne 2')\n",
    "plt.subplot(1,4,4)\n",
    "sns.scatterplot(data = scispacy_emb_df, x=0, y=1, hue='category', alpha=0.2, legend=False)\n",
    "plt.title('scispacy')\n",
    "plt.xlabel('tsne 1')\n",
    "plt.ylabel('tsne 2')\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PubMedBERT seems to have the nicestt clusters and least scattered points but openai embeddings are relatively similar (bit more scattered). Spacy fails at capturing categories together, scispacy does it better but not as good as LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA 2 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pubmed_pca = pca.fit_transform(pubmed_emb)\n",
    "openai_pca = pca.fit_transform(openai_emb)\n",
    "#spacy_md_pca = pca.fit_transform(spacy_md_emb)\n",
    "spacy_pca = pca.fit_transform(spacy_emb)\n",
    "scispacy_pca = pca.fit_transform(scispacy_emb)\n",
    "\n",
    "#turn into df, concat with category for pca\n",
    "pubmed_emb_df = pd.DataFrame(pubmed_pca)\n",
    "openai_emb_df = pd.DataFrame(openai_pca)\n",
    "spacy_emb_df = pd.DataFrame(spacy_pca)\n",
    "scispacy_emb_df = pd.DataFrame(scispacy_pca)\n",
    "\n",
    "metadata=spark.read.parquet('sm_sample_df.parquet').toPandas()\n",
    "\n",
    "#concat\n",
    "pubmed_emb_df = pd.concat([metadata,pubmed_emb_df], axis=1)\n",
    "openai_emb_df = pd.concat([metadata,openai_emb_df], axis=1)\n",
    "spacy_emb_df = pd.concat([metadata,spacy_emb_df], axis=1)\n",
    "scispacy_emb_df = pd.concat([metadata,scispacy_emb_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "#modify colors per category\n",
    "plt.subplots(1,4, figsize=(25,10))\n",
    "plt.subplot(1,4,1)\n",
    "sns.scatterplot(data = pubmed_emb_df, x=0, y=1, hue='category', alpha=0.2, legend=False)\n",
    "plt.title('pubmed')\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.subplot(1,4,2)\n",
    "sns.scatterplot(data = spacy_emb_df, x=0, y=1, hue='category', alpha=0.2, legend=False)\n",
    "plt.title('spacy')\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.subplot(1,4,3)\n",
    "sns.scatterplot(data = openai_emb_df, x=0, y=1, hue='category', alpha=0.2, legend=False)\n",
    "plt.title('openai')\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.subplot(1,4,4)\n",
    "sns.scatterplot(data = scispacy_emb_df, x=0, y=1, hue='category', alpha=0.2, legend=False)\n",
    "plt.title('scispacy')\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#modify colors per category\n",
    "plt.subplots(1,4, figsize=(25,10), sharex=True, sharey=True)\n",
    "plt.subplot(1,4,1)\n",
    "sns.scatterplot(data = pubmed_emb_df, x=0, y=1, hue='category', alpha=0.2, legend=False)\n",
    "plt.title('pubmed')\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.subplot(1,4,2)\n",
    "sns.scatterplot(data = spacy_emb_df, x=0, y=1, hue='category', alpha=0.2, legend=False)\n",
    "plt.title('spacy')\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.subplot(1,4,3)\n",
    "sns.scatterplot(data = openai_emb_df, x=0, y=1, hue='category', alpha=0.2, legend=False)\n",
    "plt.title('openai')\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.subplot(1,4,4)\n",
    "sns.scatterplot(data = scispacy_emb_df, x=0, y=1, hue='category', alpha=0.2, legend=False)\n",
    "plt.title('scispacy')\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PubMedBERT seems to have much greater variance within the embeddings compared to remaining models, especially OpenAI. This is consistent with the distribution plots shown earlieir. Could indicate that PubMedBERT embeddings are more 'diverse' and informative than the remaining ones but might also mean that data leakage is much more likely the case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine similarity between GT for data leakage detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "#read gt\n",
    "#gt = catalog.load('integration.int.known_pairs@pandas')\n",
    "#gt.to_csv('scratch/gt.csv')\n",
    "gt = pd.read_csv('scratch/gt.csv').drop('Unnamed: 0',axis=1)\n",
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Protob Conversion to Parquet\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sample_df = spark.read.parquet(\"sm_sample_df.parquet\").toPandas()\n",
    "sample_df\n",
    "\n",
    "#create a subsample which contains both diseases and drugs from gt\n",
    "sampled_df_gt = sample_df.loc[( sample_df.id.isin(gt.source) | sample_df.id.isin(gt.target) )]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubmedBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_gt_df=sampled_df_gt[sampled_df_gt.category.isin(['biolink:Drug','biolink:SmallMolecule'])]\n",
    "\n",
    "treat_pubmed_cos_sim_list=[]\n",
    "no_treat_pubmed_cos_sim_list=[]\n",
    "unknown_treat_pubmed_cos_sim_list=[]\n",
    "pubmed_cos_sim_list=[]\n",
    "pubmed_dot_prod=[]\n",
    "pubmed_euc=[]\n",
    "for node_id in drugs_gt_df.index:\n",
    "    drug_name = drugs_gt_df.loc[node_id, 'id']\n",
    "    drug_node = pubmed_emb[node_id]\n",
    "    targets = []\n",
    "    targets.extend(gt.loc[(gt.source==drug_name)].target.values)\n",
    "    for target in targets:\n",
    "        if all(sampled_df_gt.id!=target):\n",
    "            continue\n",
    "        target_id =int(sampled_df_gt.loc[target==sampled_df_gt.id].index.values)\n",
    "        disease_node = pubmed_emb[target_id]\n",
    "        if gt.loc[(gt.source==drug_name)&(gt.target==target)].y.values[0]==1:\n",
    "            relation='treats'\n",
    "        elif gt.loc[(gt.source==drug_name)&(gt.target==target)].y.values[0]==0:\n",
    "            relation='not treats'\n",
    "        else:\n",
    "            relation='unknown if treats'\n",
    "        print(drug_name, relation, target, end='\\t')\n",
    "        print('Cosine similarity :', cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1)))\n",
    "        pubmed_cos_sim_list.append(cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1))[0][0])\n",
    "        dot_product=np.dot(drug_node,disease_node)\n",
    "        print('Dot product: ',dot_product)\n",
    "        pubmed_dot_prod.append(dot_product)\n",
    "        euc_dist=distance.euclidean(drug_node,disease_node)\n",
    "        print('Euclidean distance: ',euc_dist)\n",
    "        pubmed_euc.append(euc_dist)\n",
    "        if gt.loc[(gt.source==drug_name)&(gt.target==target)].y.values[0]==1:\n",
    "            treat_pubmed_cos_sim_list.append(cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1))[0][0])\n",
    "        elif gt.loc[(gt.source==drug_name)&(gt.target==target)].y.values[0]==0:\n",
    "            no_treat_pubmed_cos_sim_list.append(cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1))[0][0])\n",
    "        else:\n",
    "            unknown_treat_pubmed_cos_sim_list.append(cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "sns.kdeplot(pubmed_cos_sim_list)\n",
    "plt.subplot(1,3,2)\n",
    "sns.kdeplot(pubmed_dot_prod)\n",
    "plt.subplot(1,3,3)\n",
    "sns.kdeplot(pubmed_euc)\n",
    "plt.tight_layout()\n",
    "print('Mean ', np.mean(pubmed_cos_sim_list))\n",
    "print('std ',np.std(pubmed_cos_sim_list))\n",
    "print('Min ',min(pubmed_cos_sim_list))\n",
    "print('Max ',max(pubmed_cos_sim_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "sns.kdeplot(pubmed_cos_sim_list)\n",
    "plt.title('all gt pairs')\n",
    "plt.subplot(1,3,2)\n",
    "sns.kdeplot(treat_pubmed_cos_sim_list, label='y=1', legend=True)\n",
    "#plt.subplot(1,3,3)\n",
    "sns.kdeplot(no_treat_pubmed_cos_sim_list, label='y=0', legend=True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PubmedBERT - random pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for non drug-disase pairs\n",
    "pubmed_cos_sim_list_nodd=[]\n",
    "for i, node_id in enumerate(drugs_gt_df.index):\n",
    "    drug_name = drugs_gt_df.loc[node_id, 'id']\n",
    "    if i!=88:\n",
    "        num = i+1\n",
    "    else:\n",
    "        num = 0\n",
    "    drug_name_next = drugs_gt_df.loc[drugs_gt_df.index[num], 'id']\n",
    "    drug_node = pubmed_emb[node_id]\n",
    "    targets = []\n",
    "    targets.extend(gt.loc[(gt.source==drug_name_next)].target.values)\n",
    "    for target in targets:\n",
    "        if all(sampled_df_gt.id!=target):\n",
    "            continue\n",
    "        if gt.loc[(gt.source==drug_name) & (gt.target==target)].shape!=(0,3):\n",
    "            continue\n",
    "        target_id =int(sampled_df_gt.loc[target==sampled_df_gt.id].index.values)\n",
    "        disease_node = pubmed_emb[target_id]\n",
    "        print(drug_name, '-', target, end='\\t')\n",
    "        print('Cosine similarity :', cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1)))\n",
    "        pubmed_cos_sim_list_nodd.append(cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1))[0][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(pubmed_cos_sim_list_nodd)\n",
    "print('Mean ', np.mean(pubmed_cos_sim_list_nodd))\n",
    "print('std ',np.std(pubmed_cos_sim_list_nodd))\n",
    "print('Min ',min(pubmed_cos_sim_list_nodd))\n",
    "print('Max ',max(pubmed_cos_sim_list_nodd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_gt_df=sampled_df_gt[sampled_df_gt.category.isin(['biolink:Drug','biolink:SmallMolecule'])]\n",
    "\n",
    "treat_openai_cos_sim_list=[]\n",
    "no_treat_openai_cos_sim_list=[]\n",
    "unknown_treat_openai_cos_sim_list=[]\n",
    "openai_cos_sim_list=[]\n",
    "openai_dot_prod=[]\n",
    "openai_euc=[]\n",
    "for node_id in drugs_gt_df.index:\n",
    "    drug_name = drugs_gt_df.loc[node_id, 'id']\n",
    "    drug_node = openai_emb[node_id]\n",
    "    targets = []\n",
    "    targets.extend(gt.loc[(gt.source==drug_name)].target.values)\n",
    "    for target in targets:\n",
    "        if all(sampled_df_gt.id!=target):\n",
    "            continue\n",
    "        target_id =int(sampled_df_gt.loc[target==sampled_df_gt.id].index.values)\n",
    "        disease_node = openai_emb[target_id]\n",
    "        if gt.loc[(gt.source==drug_name)&(gt.target==target)].y.values[0]==1:\n",
    "            relation='treats'\n",
    "        elif gt.loc[(gt.source==drug_name)&(gt.target==target)].y.values[0]==0:\n",
    "            relation='not treats'\n",
    "        else:\n",
    "            relation='unknown if treats'\n",
    "        print(drug_name, relation, target, end='\\t')\n",
    "        print('Cosine similarity :', cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1)))\n",
    "        openai_cos_sim_list.append(cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1))[0][0])\n",
    "        dot_product=np.dot(drug_node,disease_node)\n",
    "        print('Dot product: ',dot_product)\n",
    "        openai_dot_prod.append(dot_product)\n",
    "        euc_dist=distance.euclidean(drug_node,disease_node)\n",
    "        print('Euclidean distance: ',euc_dist)\n",
    "        openai_euc.append(euc_dist)\n",
    "        if gt.loc[(gt.source==drug_name)&(gt.target==target)].y.values[0]==1:\n",
    "            treat_openai_cos_sim_list.append(cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1))[0][0])\n",
    "        elif gt.loc[(gt.source==drug_name)&(gt.target==target)].y.values[0]==0:\n",
    "            no_treat_openai_cos_sim_list.append(cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1))[0][0])\n",
    "        else:\n",
    "            unknown_treat_openai_cos_sim_list.append(cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1))[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(openai_cos_sim_list)\n",
    "print('Mean ', np.mean(openai_cos_sim_list))\n",
    "print('std ',np.std(openai_cos_sim_list))\n",
    "print('Min ',min(openai_cos_sim_list))\n",
    "print('Max ',max(openai_cos_sim_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.kdeplot(openai_cos_sim_list)\n",
    "plt.title('all gt pairs')\n",
    "plt.subplot(1,3,2)\n",
    "sns.kdeplot(treat_openai_cos_sim_list, label='y=1', legend=True)\n",
    "sns.kdeplot(no_treat_openai_cos_sim_list, label='y=0', legend=True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI - random pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for non drug-disase pairs\n",
    "openai_cos_sim_list_nodd=[]\n",
    "for i, node_id in enumerate(drugs_gt_df.index):\n",
    "    drug_name = drugs_gt_df.loc[node_id, 'id']\n",
    "    if i!=88:\n",
    "        num = i+1\n",
    "    else:\n",
    "        num = 0\n",
    "    drug_name_next = drugs_gt_df.loc[drugs_gt_df.index[num], 'id']\n",
    "    drug_node = openai_emb[node_id]\n",
    "    targets = []\n",
    "    targets.extend(gt.loc[(gt.source==drug_name_next)].target.values)\n",
    "    for target in targets:\n",
    "        if all(sampled_df_gt.id!=target):\n",
    "            continue\n",
    "        print(gt.loc[(gt.source==drug_name) & (gt.target==target)].shape)\n",
    "        if gt.loc[(gt.source==drug_name) & (gt.target==target)].shape!=(0,3):\n",
    "            continue\n",
    "        target_id =int(sampled_df_gt.loc[target==sampled_df_gt.id].index.values)\n",
    "        disease_node = openai_emb[target_id]\n",
    "        print(drug_name_next, '-', target, end='\\t')\n",
    "        print('Cosine similarity :', cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1)))\n",
    "        openai_cos_sim_list_nodd.append(cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1))[0][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(openai_cos_sim_list_nodd)\n",
    "print('Mean ', np.mean(openai_cos_sim_list_nodd))\n",
    "print('std ',np.std(openai_cos_sim_list_nodd))\n",
    "print('Min ',min(openai_cos_sim_list_nodd))\n",
    "print('Max ',max(openai_cos_sim_list_nodd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_gt_df=sampled_df_gt[sampled_df_gt.category.isin(['biolink:Drug','biolink:SmallMolecule'])]\n",
    "\n",
    "treat_spacy_cos_sim_list=[]\n",
    "no_treat_spacy_cos_sim_list=[]\n",
    "unknown_treat_spacy_cos_sim_list=[]\n",
    "spacy_cos_sim_list=[]\n",
    "spacy_dot_prod=[]\n",
    "spacy_euc=[]\n",
    "for node_id in drugs_gt_df.index:\n",
    "    drug_name = drugs_gt_df.loc[node_id, 'id']\n",
    "    drug_node = spacy_emb[node_id]\n",
    "    targets = []\n",
    "    targets.extend(gt.loc[(gt.source==drug_name)].target.values)\n",
    "    for target in targets:\n",
    "        if all(sampled_df_gt.id!=target):\n",
    "            continue\n",
    "        target_id =int(sampled_df_gt.loc[target==sampled_df_gt.id].index.values)\n",
    "        disease_node = spacy_emb[target_id]\n",
    "        if gt.loc[(gt.source==drug_name)&(gt.target==target)].y.values[0]==1:\n",
    "            relation='treats'\n",
    "        elif gt.loc[(gt.source==drug_name)&(gt.target==target)].y.values[0]==0:\n",
    "            relation='not treats'\n",
    "        else:\n",
    "            relation='unknown if treats'\n",
    "        print(drug_name, relation, target, end='\\t')\n",
    "        print('Cosine similarity :', cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1)))\n",
    "        spacy_cos_sim_list.append(cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1))[0][0])\n",
    "        dot_product=np.dot(drug_node,disease_node)\n",
    "        print('Dot product: ',dot_product)\n",
    "        spacy_dot_prod.append(dot_product)\n",
    "        euc_dist=distance.euclidean(drug_node,disease_node)\n",
    "        print('Euclidean distance: ',euc_dist)\n",
    "        spacy_euc.append(euc_dist)\n",
    "        if gt.loc[(gt.source==drug_name)&(gt.target==target)].y.values[0]==1:\n",
    "            treat_spacy_cos_sim_list.append(cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1))[0][0])\n",
    "        elif gt.loc[(gt.source==drug_name)&(gt.target==target)].y.values[0]==0:\n",
    "            no_treat_spacy_cos_sim_list.append(cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1))[0][0])\n",
    "        else:\n",
    "            unknown_treat_spacy_cos_sim_list.append(cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1))[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(spacy_cos_sim_list)\n",
    "\n",
    "print('Mean ', np.mean(spacy_cos_sim_list))\n",
    "print('std ',np.std(spacy_cos_sim_list))\n",
    "print('Min ',min(spacy_cos_sim_list))\n",
    "print('Max ',max(spacy_cos_sim_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.kdeplot(spacy_cos_sim_list)\n",
    "plt.title('all gt pairs')\n",
    "plt.subplot(1,3,2)\n",
    "sns.kdeplot(treat_spacy_cos_sim_list, label='y=1', legend=True)\n",
    "sns.kdeplot(no_treat_spacy_cos_sim_list, label='y=0', legend=True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spacy - random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for non drug-disase pairs\n",
    "spacy_cos_sim_list_nodd=[]\n",
    "for i, node_id in enumerate(drugs_gt_df.index):\n",
    "    drug_name = drugs_gt_df.loc[node_id, 'id']\n",
    "    if i!=88:\n",
    "        num = i+1\n",
    "    else:\n",
    "        num = 0\n",
    "    drug_name_next = drugs_gt_df.loc[drugs_gt_df.index[num], 'id']\n",
    "    drug_node = spacy_emb[node_id]\n",
    "    targets = []\n",
    "    targets.extend(gt.loc[(gt.source==drug_name_next)].target.values)\n",
    "    for target in targets:\n",
    "        if all(sampled_df_gt.id!=target):\n",
    "            continue\n",
    "        print(gt.loc[(gt.source==drug_name) & (gt.target==target)].shape)\n",
    "        if gt.loc[(gt.source==drug_name) & (gt.target==target)].shape!=(0,3):\n",
    "            continue\n",
    "        target_id =int(sampled_df_gt.loc[target==sampled_df_gt.id].index.values)\n",
    "        other_node = spacy_emb[target_id]\n",
    "        print(drug_name_next, '-', target, end='\\t')\n",
    "        print('Cosine similarity :', cosine_similarity(drug_node.reshape(1, -1), other_node.reshape(1, -1)))\n",
    "        spacy_cos_sim_list_nodd.append(cosine_similarity(drug_node.reshape(1, -1), other_node.reshape(1, -1))[0][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(spacy_cos_sim_list_nodd)\n",
    "\n",
    "print('Mean ', np.mean(spacy_cos_sim_list_nodd))\n",
    "print('std ',np.std(spacy_cos_sim_list_nodd))\n",
    "print('Min ',min(spacy_cos_sim_list_nodd))\n",
    "print('Max ',max(spacy_cos_sim_list_nodd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SciSPacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_gt_df=sampled_df_gt[sampled_df_gt.category.isin(['biolink:Drug','biolink:SmallMolecule'])]\n",
    "\n",
    "treat_scispacy_cos_sim_list=[]\n",
    "no_treat_scispacy_cos_sim_list=[]\n",
    "unknown_treat_scispacy_cos_sim_list=[]\n",
    "scispacy_cos_sim_list=[]\n",
    "scispacy_dot_prod=[]\n",
    "scispacy_euc=[]\n",
    "for node_id in drugs_gt_df.index:\n",
    "    drug_name = drugs_gt_df.loc[node_id, 'id']\n",
    "    drug_node = scispacy_emb[node_id]\n",
    "    targets = []\n",
    "    targets.extend(gt.loc[(gt.source==drug_name)].target.values)\n",
    "    for target in targets:\n",
    "        if all(sampled_df_gt.id!=target):\n",
    "            continue\n",
    "        target_id =int(sampled_df_gt.loc[target==sampled_df_gt.id].index.values)\n",
    "        disease_node = scispacy_emb[target_id]\n",
    "        if gt.loc[(gt.source==drug_name)&(gt.target==target)].y.values[0]==1:\n",
    "            relation='treats'\n",
    "        elif gt.loc[(gt.source==drug_name)&(gt.target==target)].y.values[0]==0:\n",
    "            relation='not treats'\n",
    "        else:\n",
    "            relation='unknown if treats'\n",
    "        print(drug_name, relation, target, end='\\t')\n",
    "        print('Cosine similarity :', cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1)))\n",
    "        scispacy_cos_sim_list.append(cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1))[0][0])\n",
    "        dot_product=np.dot(drug_node,disease_node)\n",
    "        print('Dot product: ',dot_product)\n",
    "        scispacy_dot_prod.append(dot_product)\n",
    "        euc_dist=distance.euclidean(drug_node,disease_node)\n",
    "        print('Euclidean distance: ',euc_dist)\n",
    "        scispacy_euc.append(euc_dist)\n",
    "        if gt.loc[(gt.source==drug_name)&(gt.target==target)].y.values[0]==1:\n",
    "            treat_scispacy_cos_sim_list.append(cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1))[0][0])\n",
    "        elif gt.loc[(gt.source==drug_name)&(gt.target==target)].y.values[0]==0:\n",
    "            no_treat_scispacy_cos_sim_list.append(cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1))[0][0])\n",
    "        else:\n",
    "            unknown_treat_scispacy_cos_sim_list.append(cosine_similarity(drug_node.reshape(1, -1), disease_node.reshape(1, -1))[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(scispacy_cos_sim_list)\n",
    "print('Mean ', np.mean(scispacy_cos_sim_list))\n",
    "print('std ',np.std(scispacy_cos_sim_list))\n",
    "print('Min ',min(scispacy_cos_sim_list))\n",
    "print('Max ',max(scispacy_cos_sim_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "sns.kdeplot(scispacy_cos_sim_list)\n",
    "plt.title('all gt pairs')\n",
    "plt.subplot(1,3,2)\n",
    "sns.kdeplot(treat_scispacy_cos_sim_list, label='y=1', legend=True)\n",
    "sns.kdeplot(no_treat_scispacy_cos_sim_list, label='y=0', legend=True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scispacy - random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for non drug-disase pairs\n",
    "scispacy_cos_sim_list_nodd=[]\n",
    "for i, node_id in enumerate(drugs_gt_df.index):\n",
    "    drug_name = drugs_gt_df.loc[node_id, 'id']\n",
    "    if i!=88:\n",
    "        num = i+1\n",
    "    else:\n",
    "        num = 0\n",
    "    drug_name_next = drugs_gt_df.loc[drugs_gt_df.index[num], 'id']\n",
    "    drug_node = scispacy_emb[node_id]\n",
    "    targets = []\n",
    "    targets.extend(gt.loc[(gt.source==drug_name_next)].target.values)\n",
    "    for target in targets:\n",
    "        if all(sampled_df_gt.id!=target):\n",
    "            continue\n",
    "        print(gt.loc[(gt.source==drug_name) & (gt.target==target)].shape)\n",
    "        if gt.loc[(gt.source==drug_name) & (gt.target==target)].shape!=(0,3):\n",
    "            continue\n",
    "        target_id =int(sampled_df_gt.loc[target==sampled_df_gt.id].index.values)\n",
    "        other_node = scispacy_emb[target_id]\n",
    "        print(drug_name_next, '-', target, end='\\t')\n",
    "        print('Cosine similarity :', cosine_similarity(drug_node.reshape(1, -1), other_node.reshape(1, -1)))\n",
    "        scispacy_cos_sim_list_nodd.append(cosine_similarity(drug_node.reshape(1, -1), other_node.reshape(1, -1))[0][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(scispacy_cos_sim_list_nodd)\n",
    "print('Mean ', np.mean(scispacy_cos_sim_list_nodd))\n",
    "print('std ',np.std(scispacy_cos_sim_list_nodd))\n",
    "print('Min ',min(scispacy_cos_sim_list_nodd))\n",
    "print('Max ',max(scispacy_cos_sim_list_nodd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1,4, figsize=(15,5), sharex=True, sharey=True)\n",
    "plt.subplot(1,4,1)\n",
    "plt.title('Pubmed')\n",
    "sns.kdeplot(pubmed_cos_sim_list, label='Pubmed')\n",
    "plt.subplot(1,4,2)\n",
    "plt.title('OpenAI')\n",
    "sns.kdeplot(openai_cos_sim_list, label='OpenAI')\n",
    "plt.subplot(1,4,3)\n",
    "plt.title('Spacy')\n",
    "sns.kdeplot(spacy_cos_sim_list, label='Spacy')\n",
    "plt.subplot(1,4,4)\n",
    "plt.title('SciSpacy')\n",
    "sns.kdeplot(scispacy_cos_sim_list, label='SciSpacy')\n",
    "plt.suptitle('Cosine similarity distribution for drug-disease pairs in GT')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1,4, figsize=(15,5), sharex=True, sharey=True)\n",
    "plt.subplot(1,4,1)\n",
    "plt.title('Pubmed')\n",
    "sns.kdeplot(treat_pubmed_cos_sim_list, label='y=1', legend=True)\n",
    "sns.kdeplot(no_treat_pubmed_cos_sim_list, label='y=0', legend=True)\n",
    "plt.legend()\n",
    "plt.subplot(1,4,2)\n",
    "plt.title('OpenAI')\n",
    "sns.kdeplot(treat_openai_cos_sim_list, label='y=1', legend=True)\n",
    "sns.kdeplot(no_treat_openai_cos_sim_list, label='y=0', legend=True)\n",
    "plt.legend()\n",
    "plt.subplot(1,4,3)\n",
    "plt.title('Spacy')\n",
    "sns.kdeplot(treat_spacy_cos_sim_list, label='y=1', legend=True)\n",
    "sns.kdeplot(no_treat_spacy_cos_sim_list, label='y=0', legend=True)\n",
    "plt.legend()\n",
    "plt.subplot(1,4,4)\n",
    "plt.title('SciSpacy')\n",
    "sns.kdeplot(treat_scispacy_cos_sim_list, label='y=1', legend=True)\n",
    "sns.kdeplot(no_treat_scispacy_cos_sim_list, label='y=0', legend=True)\n",
    "plt.suptitle('Cosine similarity distribution fordrug-disease pairs ')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1,4, figsize=(15,5), sharex=True, sharey=True)\n",
    "plt.subplot(1,4,1)\n",
    "plt.title('Pubmed')\n",
    "sns.kdeplot(pubmed_cos_sim_list, label='Pubmed')\n",
    "sns.kdeplot(pubmed_cos_sim_list_nodd, label='random')\n",
    "plt.legend()\n",
    "plt.subplot(1,4,2)\n",
    "plt.title('OpenAI')\n",
    "sns.kdeplot(openai_cos_sim_list, label='random')\n",
    "sns.kdeplot(openai_cos_sim_list_nodd, label='random')\n",
    "plt.legend()\n",
    "plt.subplot(1,4,3)\n",
    "plt.title('Spacy')\n",
    "sns.kdeplot(spacy_cos_sim_list, label='random')\n",
    "sns.kdeplot(spacy_cos_sim_list_nodd, label='random')\n",
    "plt.legend()\n",
    "plt.subplot(1,4,4)\n",
    "plt.title('SciSpacy')\n",
    "sns.kdeplot(scispacy_cos_sim_list, label='random')\n",
    "sns.kdeplot(scispacy_cos_sim_list_nodd, label='random')\n",
    "plt.legend()\n",
    "plt.suptitle('Cosine similarity distribution for random vs gt drug-disease pairs ')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PubMedBERT vs PubMedBERT embeddings - WIP\n",
    "\n",
    "Chunyu generated pubmedbert embeddings with base pubmedbert, but there is a specific embedding model from pubmedbert on HF - could be much faster. The model is actually using mean pooling (which is common when extracting embeddings) but what Chunyus is using is simply pooler output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanpooling(output, mask):\n",
    "    embeddings = output[0] # First element of model_output contains all token embeddings\n",
    "    mask = mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    return torch.sum(embeddings * mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n",
    "\n",
    "t1=time.time()\n",
    "sentences = features\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuml/pubmedbert-base-embeddings\")\n",
    "model = AutoModel.from_pretrained(\"neuml/pubmedbert-base-embeddings\")\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "with torch.no_grad():\n",
    "    output = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "embeddings = meanpooling(output, inputs['attention_mask'])\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(embeddings)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = joblib.load('sm_sample_features.joblib')[:1000]\n",
    "sentences = features\n",
    "t1=time.time()\n",
    "model = SentenceTransformer(\"neuml/pubmedbert-base-embeddings\")\n",
    "embeddings0 = model.encode(sentences)\n",
    "print(embeddings)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t1=time.time()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuml/pubmedbert-base-embeddings\")\n",
    "model = AutoModel.from_pretrained(\"neuml/pubmedbert-base-embeddings\")\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "with torch.no_grad():\n",
    "    embeddings1 = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "print(embeddings1)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t1=time.time()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "with torch.no_grad():\n",
    "    embeddings2 = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "print(embeddings2)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t1=time.time()\n",
    "def meanpooling(output, mask):\n",
    "    embeddings = output[0]\n",
    "    mask = mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    return torch.sum(embeddings * mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "embeddings = meanpooling(output, inputs['attention_mask'])\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(embeddings)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#same?\n",
    "print(ks_2samp(np.array(embeddings1).flatten(), np.array(embeddings2).flatten()))\n",
    "print(ks_2samp(np.array(embeddings1).flatten(), np.array(embeddings).flatten()))\n",
    "print(ks_2samp(np.array(embeddings).flatten(), np.array(embeddings2).flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare embed1 and embed2\n",
    "euc_dist = []\n",
    "cos_dist = []\n",
    "for i in range(len(embeddings1)):\n",
    "    euc_dist.append(distance.euclidean(np.array(embeddings1[i]), np.array(embeddings2[i])))\n",
    "    cos_dist.append(distance.cosine(np.array(embeddings1[i]), np.array(embeddings2[i])))\n",
    "print('Euc mean ',np.mean(euc_dist), ' std ', np.std(euc_dist))\n",
    "print('Cos mean ',np.mean(cos_dist), ' std ', np.std(cos_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare embed1 and embed\n",
    "euc_dist = []\n",
    "cos_dist = []\n",
    "for i in range(len(embeddings1)):\n",
    "    euc_dist.append(distance.euclidean(np.array(embeddings1[i]), np.array(embeddings[i])))\n",
    "    cos_dist.append(distance.cosine(np.array(embeddings1[i]), np.array(embeddings[i])))\n",
    "print('Euc mean ',np.mean(euc_dist), ' std ', np.std(euc_dist))\n",
    "print('Cos mean ',np.mean(cos_dist), ' std ', np.std(cos_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare embed2 and embed\n",
    "euc_dist = []\n",
    "cos_dist = []\n",
    "for i in range(len(embeddings1)):\n",
    "    euc_dist.append(distance.euclidean(np.array(embeddings2[i]), np.array(embeddings[i])))\n",
    "    cos_dist.append(distance.cosine(np.array(embeddings2[i]), np.array(embeddings[i])))\n",
    "print('Euc mean ',np.mean(euc_dist), ' std ', np.std(euc_dist))\n",
    "print('Cos mean ',np.mean(cos_dist), ' std ', np.std(cos_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence transformer is much faster than the tokenizer -> model -> hidden layers: the embeddings differ from when they are obtained through NeuML or original base model. Question to be answered tho: why did Chunyu use pooler_output rather than mean pooling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "* run graphsage on the subsets of embeddings and commpare how topological embeddings differ\n",
    "* train ML classifier to predict drug-disease interactions to see how the performance differs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
