{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Author & point of contact: Piotr Kaniewski (piotr@everycure.org)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e2e classifier evaluation check\n",
    "We noticed our graphsage produces weirdly looking topological embeddings which likely impact downstream performance; we should try to optimize graphsage so that it produces more informative embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary - high level \n",
    "\n",
    "We had the following runs: \n",
    "\n",
    "**2024-08-07 - pre-troubleshooting e2e run**\n",
    "* LR = 0.1; BS = 20 000; Search Depth = 100; MaxIter = 10; SizeSample(25,10); Embedding Dimensions and hidden layer dims = 512; 10 epochs\n",
    "* loss slowly decreasing, approx 0.26-> 0.19\n",
    "* This is where we noticed that downstream task performance is odd (mainly due to bugs in evaluation matrix) and when we noticed the infamous 'spaghetti monster' UMAP.\n",
    "\n",
    "**2024-08-19 - first-troubleshooting e2e run**\n",
    "* LR = 0.01; BS = 20 000; Search Depth = 100; MaxIter = 10; SizeSample(25,10); Embedding Dimensions and hidden layer dims = 512; 10 epochs\n",
    "* The weird looking PCA persisted;This is where we noticed that downstream task performance is odd (mainly due to bugs in evaluation matrix) and when we noticed the infamous 'spaghetti monster' UMAP.\n",
    "\n",
    "**2024-08-20 - 'random' e2e run**\n",
    "* LR = 0.01; BS = 20 000; Search Depth = 100; MaxIter = 1; SizeSample(25,10); Embedding Dimensions and hidden layer dims = 512; 1 epoch\n",
    "* Surprisingly, this gave initially nice looking PCA and a really good task performance (highlighting that we didnt remove all drug-disease edges - now it's fixed thanks to Laurens). This data leakage however was a proof that **GraphSage topological enrichment worked** which we cannot say for sure for the rest \n",
    "* yesterday this specific setting was re-run and while PCA looked a bit odd, the downstream task performance was consistenly really good (bit lower but with different run it is expected)\n",
    "\n",
    "**2024-08-20 - Chunyu-like e2e run**\n",
    "* LR = 0.001; BS = 256; Search Depth = 100; MaxIter = 1000; SizeSample(96,96); Embedding Dimensions and hidden layer dims = 512; 10 epoch\n",
    "* Chunyu like parameters but took ages, so was terminated. It was likely to overfit anyway knowing what we know now\n",
    "\n",
    "**2024-08-21 - small Chunyu-like e2e run**\n",
    "* LR = 0.001; BS = 256; Search Depth = 100; MaxIter = 100; SizeSample(96,96); Embedding Dimensions and hidden layer dims = 512; 10 epoch\n",
    "* Gave even worse results than first-troubleshooting e2e run, with weird looking PCA and really bad downstream task performance, showing potential overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My explanation: \n",
    "* we use very high search depth (100). What this essentially does is that when Graphsage samples a node, it is approximated by nodes within its 100-hops which is probably the whole graph. This is done for 20 000 nodes * 4 times (batch size x concurrency 4) which gives us a lot of repeated and consistent information. With batch size being large we get quite stable training as well.\n",
    "* sigmoid activation function sucks because of its quickly vanishing gradient (hence everyone uses ReLU nowadays), but because we only have one epoch and one iteration, this vanishing gradient is not yet apparent. Nevertheless, that would potentially explain the low variance and plateuing loss being more and more apparent as we train with more iterations/epochs (as we increase no.epochs, we increase forward passess through sigmoid functions which squish the values between 0 and 1, giving low variance)\n",
    "  * In a notebook e2e_pca.ipynb, this can be clearly visible - when we train graphsage only on 1 epoch and 1 iteration, ther eis very subtle difference between sigmoid and others. Only after a longer \n",
    "* wiggly line is a sign of overtraining, graphsage is supposed to approximate a node based on its connectivity i.e. neighbors. Think about neighbor A. In the first epoch we have relatively original features being aggregated and averaged to represent A, even when we use 100-hops. But in the second epoch then, we have approximate aggregation of aggregations from epoch 1, which decreases in value due to sigmoid function. These aggregated values should be more and more similar to each other (I think) and also decrease in variance due to sigmoig, which eventually leads to the squiggly line. \n",
    "  * This is consistent with the check I have done in e2e_pca.ipynb, where as we increase no. epochs/iterations, the wiggly line starts appearing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import subprocess\n",
    "import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier                                    \n",
    "\n",
    "# Setting the root path and changing the directory\n",
    "root_path = subprocess.check_output(['git', 'rev-parse', '--show-toplevel']).decode().strip()\n",
    "os.chdir(Path(root_path) / 'pipelines' / 'matrix')\n",
    "\n",
    "#%load_ext kedro.ipython\n",
    "#%reload_kedro  --env base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load metadata and gt\n",
    "nodes_df = pd.read_parquet(\"data/03_primary/rtx_kg2/nodes\")\n",
    "\n",
    "#load gt \n",
    "gt = pd.read_csv('scratch/gt.csv').drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20240807 - pre trobuleshooting e2e run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_graph = pd.read_csv('notebooks/topological_embeddings_20240807.csv', index_col=0)\n",
    "\n",
    "openai_graph['topological_embedding'] = [np.fromstring(key.strip('[]'), sep=' ') for key in openai_graph.topological_embedding]\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2 )\n",
    "top_embed = pca.fit_transform(pd.DataFrame(openai_graph['topological_embedding'].tolist()))\n",
    "\n",
    "plt.scatter(top_embed.transpose()[0], top_embed.transpose()[1])\n",
    "plt.show()\n",
    "\n",
    "sns.kdeplot(np.array(openai_graph['topological_embedding'].tolist()).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sub-dfs\n",
    "DRUG_TYPE = ['biolink:Drug', 'biolink:SmallMolecule']\n",
    "DISEASE_TYPE = ['biolink:Disease', 'biolink:PhenotypicFeature', 'biolink:BehavioralFeature', 'biolink:DiseaseOrPhenotypicFeature']\n",
    "\n",
    "#sample\n",
    "nodes_df_drugs = nodes_df[nodes_df['category'].isin(DRUG_TYPE)]\n",
    "nodes_df_disease = nodes_df[nodes_df['category'].isin(DISEASE_TYPE)]\n",
    "\n",
    "#train test split \n",
    "train, test = train_test_split(gt, stratify=gt['y'], test_size=0.1, random_state=42)\n",
    "train_tp_df = train[train['y']==1]\n",
    "train_tp_df_drugs = train_tp_df['source'].reset_index(drop=True)\n",
    "train_tp_df_diseases = train_tp_df['target'].reset_index(drop=True)\n",
    "len_tp_tr = len(train_tp_df)\n",
    "n_rep = 3\n",
    "\n",
    "# create random drug-disease pairs\n",
    "rand_drugs = nodes_df_drugs['id'].sample(n_rep*len_tp_tr, replace=True, ignore_index = True, random_state = 42) # 42\n",
    "rand_disease = nodes_df_disease['id'].sample(n_rep*len_tp_tr, replace=True, ignore_index = True, random_state = 42) # 42\n",
    "train_tp_diseases_copies = pd.concat([train_tp_df_diseases for _ in range(n_rep)], ignore_index = True)\n",
    "train_tp_drugs_copies = pd.concat([train_tp_df_drugs for _ in range(n_rep)], ignore_index = True)\n",
    "tmp_1 = pd.DataFrame({'source': rand_drugs, 'target': train_tp_diseases_copies, 'y': 2})\n",
    "tmp_2 = pd.DataFrame({'source': train_tp_drugs_copies, 'target': rand_disease, 'y': 2})\n",
    "un_data_1 =  pd.concat([tmp_1,tmp_2], ignore_index =True)\n",
    "train_df_1 = pd.concat([train, un_data_1]).sample(frac=1, random_state = 42).reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "\n",
    "# Collecting relevant info from TP test set \n",
    "test_tp_df = test[test['y'] == 1]\n",
    "len_tp_tst = len(test_tp_df)\n",
    "test_tp_drugs = test_tp_df['source'].reset_index(drop=True)\n",
    "test_tp_diseases = test_tp_df['target'].reset_index(drop=True)\n",
    "\n",
    "# Adding synthetic data to TP+TN dataset\n",
    "rand_drugs_tmp = nodes_df_drugs['id'].sample(len_tp_tst, replace=True, ignore_index = True, random_state = 42)\n",
    "rand_dis_tmp = nodes_df_disease['id'].sample(len_tp_tst, replace=True, ignore_index = True, random_state = 42)\n",
    "tmp_1 = pd.DataFrame({'source': rand_drugs_tmp, 'target': test_tp_diseases, 'y': 2})\n",
    "tmp_2 = pd.DataFrame({'source': test_tp_drugs, 'target': rand_dis_tmp, 'y': 2})\n",
    "un_data_kgml = pd.concat([tmp_1,tmp_2], ignore_index =True)\n",
    "test = pd.concat([test, un_data_kgml]).sample(frac=1, random_state = 42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dictionary for fast lookup\n",
    "embedding_dict = openai_graph.set_index('id')['topological_embedding'].to_dict()\n",
    "\n",
    "# Initialize feature matrices\n",
    "feature_length = 1024\n",
    "X_openai = np.empty((len(train_df_1), feature_length), dtype='float32')\n",
    "X_openai_test = np.empty((len(test), feature_length), dtype='float32')\n",
    "\n",
    "# Function to get concatenated vectors\n",
    "def get_concatenated_vector(row):\n",
    "    drug_vector = embedding_dict[row['source']]\n",
    "    disease_vector = embedding_dict[row['target']]\n",
    "    return np.concatenate([drug_vector, disease_vector])\n",
    "\n",
    "# Apply the function to each row of train_df_1\n",
    "X_openai = np.vstack(train_df_1.apply(get_concatenated_vector, axis=1))\n",
    "\n",
    "# Convert target variable to numpy array\n",
    "y_openai = train_df_1['y'].to_numpy()\n",
    "\n",
    "# Reset index for test DataFrame\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "X_openai_test = np.vstack(test.apply(get_concatenated_vector, axis=1))\n",
    "\n",
    "y_openai_test = test['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "xgb = XGBClassifier(random_state = 42)\n",
    "xgb.fit(X_openai, y_openai)\n",
    "\n",
    "y_openai_pred = xgb.predict_proba(X_openai_test)\n",
    "y_openai_proba = xgb.predict_proba(X_openai_test)\n",
    "joblib.dump(xgb, 'notebooks/e2e_troubleshoot/openai_e2e_0_xgb.pkl')\n",
    "\n",
    "print('xgboost scores (not treat; treat; unknown)')\n",
    "print(y_openai_proba)\n",
    "\n",
    "openai_df_full_graph_1 = pd.DataFrame(y_openai_proba)\n",
    "openai_df_full_graph_1.columns = ['not-treat-score', 'treat-score', 'unknown-treat-score']\n",
    "openai_df_full_graph_1.to_csv('notebooks/e2e_troubleshoot/openai_e2e_0_xgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_df_full_graph_1.columns=['not treat','treat', 'unknown']\n",
    "openai_df_full_graph_1['GT']=test.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_df_full_1=openai_df_full_graph_1\n",
    "# Define a function to plot KDEs\n",
    "def plot_kde(ax, data, column, gt, title):\n",
    "    sns.kdeplot(data=data.loc[data.GT==gt], x=column, ax=ax, label='Non-topological Embeddings')\n",
    "    sns.kdeplot(data=openai_df_full_graph_1.loc[openai_df_full_graph_1.GT==gt], x=column, ax=ax, label='Topological Embeddings')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "# Create a 2x3 grid for subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot True Positives\n",
    "plot_kde(axes[0, 0], openai_df_full_1, 'not treat', 1, 'OpenAI - Distribution of not treat scores for True Positives')\n",
    "plot_kde(axes[0, 1], openai_df_full_1, 'treat', 1, 'OpenAI - Distribution of treat scores for True Positives')\n",
    "plot_kde(axes[0, 2], openai_df_full_1, 'unknown', 1, 'OpenAI - Distribution of unknown scores for True Positives')\n",
    "\n",
    "# Plot True Negatives\n",
    "plot_kde(axes[1, 0], openai_df_full_1, 'not treat', 0, 'OpenAI - Distribution of not treat scores for True Negatives')\n",
    "plot_kde(axes[1, 1], openai_df_full_1, 'treat', 0, 'OpenAI - Distribution of treat scores for True Negatives')\n",
    "plot_kde(axes[1, 2], openai_df_full_1, 'unknown', 0, 'OpenAI - Distribution of unknown scores for True Negatives')\n",
    "\n",
    "# Plot True Negatives\n",
    "plot_kde(axes[2, 0], openai_df_full_1, 'not treat', 0, 'OpenAI - Distribution of not treat scores for True Unknown')\n",
    "plot_kde(axes[2, 1], openai_df_full_1, 'treat', 0, 'OpenAI - Distribution of treat scores for True Unknown')\n",
    "plot_kde(axes[2, 2], openai_df_full_1, 'unknown', 0, 'OpenAI - Distribution of unknown scores for True Unknown')\n",
    "\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Evaluation\n",
    "Checking quantiative scores on test set 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for \"alexei_fellowship.ipynb\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## Dataset utilities\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def give_vectorised_dataset(df, emb_dict):\n",
    "    '''\n",
    "    Args:\n",
    "    - df (pd.DataFrame): drug-disease dataset\n",
    "    - emb_dict (dictionary with np.array values): drug and disease embeddings\n",
    "    Out: vectorised dataset `(X, y)` (tuple of np.array)\n",
    "    '''\n",
    "    # Generating design matrix\n",
    "    drug_ids = pd.Series(df['source'].unique())\n",
    "    dis_ids = pd.Series(df['target'].unique())\n",
    "    vector_length_drug = len(emb_dict[drug_ids[0]]) # Here we assume consistent array lengths\n",
    "    vector_length_dis = len(emb_dict[dis_ids[0]]) \n",
    "    X = np.empty(shape=(len(df), vector_length_drug+vector_length_dis), dtype = 'float32')\n",
    "    for index, row in df.reset_index().iterrows():\n",
    "        drug_id = row['source']\n",
    "        disease_id = row['target']\n",
    "        drug_vector = emb_dict[drug_id]\n",
    "        disease_vector = emb_dict[disease_id]\n",
    "        X[index] = np.concatenate([drug_vector, disease_vector])\n",
    "    # Target vector\n",
    "    y = df['y'].to_numpy()\n",
    "    return X, y\n",
    "    \n",
    "\n",
    "def give_X(df, emb_dict):\n",
    "    '''\n",
    "    Args:\n",
    "    - df (pd.DataFrame): drug-disease input dataset (no labels)\n",
    "    - emb_dict (dictionary with np.array values): drug and disease embeddings\n",
    "    Out: vectorised input dataset `X` (np.array)\n",
    "    '''\n",
    "    # Generating design matrix\n",
    "    drug_ids = pd.Series(df['source'].unique())\n",
    "    dis_ids = pd.Series(df['target'].unique())\n",
    "    vector_length_drug = len(emb_dict[drug_ids[0]]) # Here we assume consistent array lengths\n",
    "    vector_length_dis = len(emb_dict[dis_ids[0]]) \n",
    "    X = np.empty(shape=(len(df), vector_length_drug+vector_length_dis), dtype = 'float32')\n",
    "    for index, row in df.reset_index().iterrows():\n",
    "        drug_id = row['source']\n",
    "        disease_id = row['target']\n",
    "        drug_vector = emb_dict[drug_id]\n",
    "        disease_vector = emb_dict[disease_id]\n",
    "        X[index] = np.concatenate([drug_vector, disease_vector])\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def train_test_split_pd(df, test_size, random_state):\n",
    "    '''\n",
    "    Returns train-test split for a Pandas dataframe.\n",
    "    '''\n",
    "    df_train, df_test, y_train, y_test = train_test_split(df[['source','target']].to_numpy(), \n",
    "         df['y'].to_numpy(), test_size=test_size, stratify=df['y'].to_numpy(), random_state=random_state)\n",
    "    train_df = pd.concat([pd.DataFrame(df_train), pd.DataFrame(y_train)], axis=1)\n",
    "    train_df.columns = df.columns\n",
    "    test_df = pd.concat([pd.DataFrame(df_test), pd.DataFrame(y_test)], axis=1)\n",
    "    test_df.columns = df.columns\n",
    "    return train_df, test_df\n",
    "    \n",
    "    \n",
    "## MRR ank Hit@k computation\n",
    "\n",
    "import bisect\n",
    "pd.options.mode.chained_assignment = None  # silence warning: see https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas\n",
    "\n",
    "\n",
    "def give_rank(prob, rand_pairs):\n",
    "    '''\n",
    "    Computes rank of a probability score `prob` among pd.DataFrame `rand_pairs`, \n",
    "    which has a column 'treat' probability scores sorted in a descending fashion.\n",
    "    '''\n",
    "    N = len(rand_pairs)\n",
    "    rand_prob_lst = list(rand_pairs['treat'])\n",
    "    rand_prob_lst.reverse() # Ascending order for bisect package \n",
    "    rank_ascending = bisect.bisect(rand_prob_lst, prob)\n",
    "    rank = N - rank_ascending + 1\n",
    "    return rank\n",
    "\n",
    "\n",
    "def give_mrr(N, model, test_tptn_df, drug_nodes, disease_nodes, emb_dict,\n",
    "                portion = 1, load_bar = True):\n",
    "    '''\n",
    "    Computes MRR using the following sampling method: \n",
    "    For each TP drug-disease pair, produce `N` random samples \n",
    "    by N/2 replacements of the drug and N/2 replacements of the disease.\n",
    "    '''\n",
    "    # Number of replacements\n",
    "    N_samps = int(N/2)\n",
    "    \n",
    "    # Creating TP dataset and sampling portion\n",
    "    test_tp_df = test_tptn_df[test_tptn_df['y']==1]\n",
    "    test_tp_df_portion = test_tp_df.sample(frac=portion, replace = False)\n",
    "    test_tp_df_portion = test_tp_df_portion.reset_index(drop=True)\n",
    "\n",
    "    # Compute treat probability scores for test TP drug-disease pairs\n",
    "    test_tp_X = give_X(test_tp_df_portion, emb_dict)\n",
    "    tp_probs = model.predict_proba(test_tp_X)[:,1]\n",
    "\n",
    "    # Loading bar\n",
    "    if load_bar == False:\n",
    "        tqdm_tmp = lambda x: x\n",
    "    else:\n",
    "        tqdm_tmp = tqdm\n",
    "        \n",
    "    rec_rank_total = 0\n",
    "    for idx, row in tqdm_tmp(test_tp_df_portion.iterrows()):\n",
    "        drug = row['source']\n",
    "        disease = row['target']\n",
    "        # Generate random samples\n",
    "        rand_drugs = drug_nodes['id'].sample(N_samps, replace=False, ignore_index=True)\n",
    "        rand_dis = disease_nodes['id'].sample(N_samps, replace=False, ignore_index=True)\n",
    "        tmp_1 = pd.DataFrame({'source': rand_drugs, 'target': disease, 'y': 2})\n",
    "        tmp_2 = pd.DataFrame({'source': drug, 'target': rand_dis, 'y': 2})\n",
    "        rand_pairs = pd.concat([tmp_1,tmp_2], ignore_index=True)\n",
    "        # Compute treat probability scores for random pairs\n",
    "        rand_X = give_X(rand_pairs, emb_dict)\n",
    "        rand_pairs['treat'] = model.predict_proba(rand_X)[:,1]\n",
    "        rand_pairs = rand_pairs.sort_values(by = 'treat', ascending=False)\n",
    "        # Compute rank and add to sum\n",
    "        row_prob = tp_probs[idx]\n",
    "        rank = give_rank(row_prob, rand_pairs)\n",
    "        rec_rank_total += 1/rank\n",
    "\n",
    "    mrr = rec_rank_total/len(test_tp_df_portion)\n",
    "    return mrr\n",
    "\n",
    "\n",
    "def give_hitk(k, N, model, test_tptn_df, drug_nodes, disease_nodes, emb_dict,\n",
    "                portion = 1, load_bar = True):\n",
    "    '''\n",
    "    Computes Hit@k using the following sampling method: \n",
    "    For each TP drug-disease pair, produce `N` random samples \n",
    "    by N/2 replacements of the drug and N/2 replacements of the disease.\n",
    "    '''\n",
    "    # Number of replacements\n",
    "    N_samps = int(N/2)\n",
    "    \n",
    "    # Creating TP dataset and sampling portion\n",
    "    test_tp_df = test_tptn_df[test_tptn_df['y']==1]\n",
    "    test_tp_df_portion = test_tp_df.sample(frac=portion, replace = False)\n",
    "    test_tp_df_portion = test_tp_df_portion.reset_index(drop=True)\n",
    "\n",
    "    # Compute treat probability scores for test TP drug-disease pairs\n",
    "    test_tp_X = give_X(test_tp_df_portion, emb_dict)\n",
    "    tp_probs = model.predict_proba(test_tp_X)[:,1]\n",
    "\n",
    "    # Loading bar\n",
    "    if load_bar == False:\n",
    "        tqdm_tmp = lambda x: x\n",
    "    else:\n",
    "        tqdm_tmp = tqdm\n",
    "        \n",
    "    geqk_count = 0\n",
    "    for idx, row in tqdm_tmp(test_tp_df_portion.iterrows()):\n",
    "        drug = row['source']\n",
    "        disease = row['target']\n",
    "        # Generate random samples\n",
    "        rand_drugs = drug_nodes['id'].sample(N_samps, replace=False, ignore_index = True)\n",
    "        rand_dis = disease_nodes['id'].sample(N_samps, replace=False, ignore_index = True)\n",
    "        tmp_1 = pd.DataFrame({'source': rand_drugs, 'target': disease, 'y': 2})\n",
    "        tmp_2 = pd.DataFrame({'source': drug, 'target': rand_dis, 'y': 2})\n",
    "        rand_pairs = pd.concat([tmp_1,tmp_2], ignore_index =True)\n",
    "        # Compute treat probability scores for random pairs\n",
    "        rand_X = give_X(rand_pairs, emb_dict)\n",
    "        rand_pairs['treat'] = model.predict_proba(rand_X)[:,1]\n",
    "        rand_pairs = rand_pairs.sort_values(by = 'treat', ascending = False)\n",
    "        # Determine if rank <= k\n",
    "        row_prob = tp_probs[idx]\n",
    "        rank = give_rank(row_prob, rand_pairs)\n",
    "        if rank <= k:\n",
    "            geqk_count += 1\n",
    "\n",
    "    hitk = geqk_count/len(test_tp_df_portion)\n",
    "    return hitk\n",
    "    \n",
    "\n",
    "## Bayesian hyperparameter optimisation    \n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from skopt import gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.plots import plot_convergence\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "class tqdm_skopt(object):\n",
    "    'Loading bar for hyperparameter optimisation'\n",
    "    def __init__(self, **kwargs):\n",
    "        self._bar = tqdm(**kwargs) \n",
    "        \n",
    "    def __call__(self, res):\n",
    "        self._bar.update()\n",
    "\n",
    "\n",
    "def perform_hyperparameter_opt(df, search_space, args_xg, eval_score, emb_dict,\n",
    "                                n_calls=100, val_size=1/9, verbose=True, random_state=1):\n",
    "    \"\"\"\n",
    "    Trains an XGBoost model on the training set `df` (pd.DataFrame) with hyperparameter optimisation. \n",
    "    The objective function,  `eval_score` must take the model and validation set as arguments. \n",
    "    Output: \n",
    "    `best_model`: XGBoost model instance with best validation score (trained on the full training set `(X_ftr,y_ftr)`)\n",
    "    `result`: scikit-optimize `OptimizeResult` instance\n",
    "    \"\"\"\n",
    "    # Define the function used to evaluate a given configuration\n",
    "    @use_named_args(search_space)\n",
    "    def evaluate_model(**params):\n",
    "        # Configure the model with specific hyperparameters\n",
    "        model = XGBClassifier(**args_xg)\n",
    "        model.set_params(**params)\n",
    "        # Train and evaluate with different split every round\n",
    "        train_set, val_set = train_test_split_pd(df, test_size = val_size, random_state=None)\n",
    "        X_tr, y_tr = give_vectorised_dataset(train_set, emb_dict)\n",
    "        model.fit(X_tr, y_tr, verbose = False)\n",
    "        score = eval_score(model, val_set)\n",
    "        # Convert from a maximizing score to a minimising score\n",
    "        return 1.0 - score\n",
    "    \n",
    "    # Perform optimisation\n",
    "    start_time = time.time()\n",
    "    if verbose == False:\n",
    "        callback=[tqdm_skopt(total=n_calls, desc=\"Progress\")]\n",
    "    else:\n",
    "        callback = None\n",
    "    result = gp_minimize(evaluate_model, search_space, \n",
    "                         verbose = verbose, \n",
    "                         n_calls = n_calls,\n",
    "                         callback=callback,\n",
    "                         random_state=random_state)\n",
    "    \n",
    "\n",
    "    # Train best model on full training set `(X_ftr, y_ftr)`\n",
    "    opt_params = {param.name:param_val for param, param_val in zip(search_space,result.x)}\n",
    "    best_model = XGBClassifier(**args_xg)\n",
    "    best_model.set_params(**opt_params)\n",
    "    X_ftr, y_ftr = give_vectorised_dataset(df, emb_dict)\n",
    "    best_model.fit(X_ftr, y_ftr)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Summarising finding\n",
    "    plot_convergence(result)\n",
    "    plt.show()\n",
    "    print('Best validation score: %.3f' % (1.0 - result.fun))\n",
    "    print('Time taken (seconds) %.3f' % (end_time - start_time))\n",
    "    \n",
    "    return best_model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "#from utils import *\n",
    "\n",
    "# map\n",
    "openai_df_full_graph_1['PT'] = openai_df_full_graph_1.round().drop('GT', axis=1).idxmax(axis=1).map({'not treat': 0, 'treat': 1, 'unknown': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pubmed full\n",
    "f1_score(openai_df_full_graph_1['PT'],openai_df_full_graph_1['GT'], average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_mrr(N, model, test_tptn_df, drug_nodes, disease_nodes, emb_dict,\n",
    "                portion = 1, load_bar = True):\n",
    "    '''\n",
    "    Computes MRR using the following sampling method: \n",
    "    For each TP drug-disease pair, produce `N` random samples \n",
    "    by N/2 replacements of the drug and N/2 replacements of the disease.\n",
    "    '''\n",
    "    # Number of replacements\n",
    "    N_samps = int(N/2)\n",
    "    \n",
    "    # Creating TP dataset and sampling portion\n",
    "    test_tp_df = test_tptn_df[test_tptn_df['y']==1]\n",
    "    test_tp_df_portion = test_tp_df.sample(frac=portion, replace = False)\n",
    "    test_tp_df_portion = test_tp_df_portion.reset_index(drop=True)\n",
    "\n",
    "    # Compute treat probability scores for test TP drug-disease pairs\n",
    "    test_tp_X = give_X(test_tp_df_portion, emb_dict)\n",
    "    tp_probs = model.predict_proba(test_tp_X)[:,1]\n",
    "\n",
    "    # Loading bar\n",
    "    if load_bar == False:\n",
    "        tqdm_tmp = lambda x: x\n",
    "    else:\n",
    "        tqdm_tmp = tqdm\n",
    "        \n",
    "    rec_rank_total = 0\n",
    "    for idx, row in tqdm_tmp(test_tp_df_portion.iterrows()):\n",
    "        drug = row['source']\n",
    "        disease = row['target']\n",
    "        # Generate random samples\n",
    "        rand_drugs = drug_nodes['id'].sample(N_samps, replace=False, ignore_index=True)\n",
    "        rand_dis = disease_nodes['id'].sample(N_samps, replace=False, ignore_index=True)\n",
    "        tmp_1 = pd.DataFrame({'source': rand_drugs, 'target': disease, 'y': 2})\n",
    "        tmp_2 = pd.DataFrame({'source': drug, 'target': rand_dis, 'y': 2})\n",
    "        rand_pairs = pd.concat([tmp_1,tmp_2], ignore_index=True)\n",
    "        # Compute treat probability scores for random pairs\n",
    "        rand_X = give_X(rand_pairs, emb_dict)\n",
    "        rand_pairs['treat'] = model.predict_proba(rand_X)[:,1]\n",
    "        rand_pairs = rand_pairs.sort_values(by = 'treat', ascending=False)\n",
    "        # Compute rank and add to sum\n",
    "        row_prob = tp_probs[idx]\n",
    "        rank = give_rank(row_prob, rand_pairs)\n",
    "        rec_rank_total += 1/rank\n",
    "\n",
    "    mrr = rec_rank_total/len(test_tp_df_portion)\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "other_args = {'drug_nodes':nodes_df_drugs, 'disease_nodes':nodes_df_disease, 'emb_dict':embedding_dict, \n",
    "              'portion':1, 'load_bar':False} \n",
    "\n",
    "# Computing scores for all models\n",
    "metrics_lst = []\n",
    "mrr_tmp = give_mrr(1000, xgb, test[test.y!=2], **other_args)\n",
    "hit_metrics = []\n",
    "for k in [1,3,5]:\n",
    "    hit_metrics.append(give_hitk(k, 1000, xgb, test[test.y!=2],  **other_args))\n",
    "print(hit_metrics)\n",
    "print(mrr_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20240819 - first trobuleshooting e2e run (lower lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_graph = pd.read_csv('notebooks/topological_embeddings_20240819.csv', index_col=0)\n",
    "\n",
    "openai_graph['topological_embedding'] = [np.fromstring(key.strip('[]'), sep=' ') for key in openai_graph.topological_embedding]\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2 )\n",
    "top_embed = pca.fit_transform(pd.DataFrame(openai_graph['topological_embedding'].tolist()))\n",
    "\n",
    "plt.scatter(top_embed.transpose()[0], top_embed.transpose()[1])\n",
    "plt.show()\n",
    "\n",
    "sns.kdeplot(np.array(openai_graph['topological_embedding'].tolist()).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dictionary for fast lookup\n",
    "embedding_dict = openai_graph.set_index('id')['topological_embedding'].to_dict()\n",
    "\n",
    "# Initialize feature matrices\n",
    "feature_length = 1024\n",
    "X_openai = np.empty((len(train_df_1), feature_length), dtype='float32')\n",
    "X_openai_test = np.empty((len(test), feature_length), dtype='float32')\n",
    "\n",
    "# Function to get concatenated vectors\n",
    "def get_concatenated_vector(row):\n",
    "    drug_vector = embedding_dict[row['source']]\n",
    "    disease_vector = embedding_dict[row['target']]\n",
    "    return np.concatenate([drug_vector, disease_vector])\n",
    "\n",
    "# Apply the function to each row of train_df_1\n",
    "X_openai = np.vstack(train_df_1.apply(get_concatenated_vector, axis=1))\n",
    "\n",
    "# Convert target variable to numpy array\n",
    "y_openai = train_df_1['y'].to_numpy()\n",
    "\n",
    "# Reset index for test DataFrame\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "X_openai_test = np.vstack(test.apply(get_concatenated_vector, axis=1))\n",
    "\n",
    "y_openai_test = test['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "xgb = XGBClassifier(random_state = 42)\n",
    "xgb.fit(X_openai, y_openai)\n",
    "\n",
    "y_openai_pred = xgb.predict_proba(X_openai_test)\n",
    "y_openai_proba = xgb.predict_proba(X_openai_test)\n",
    "joblib.dump(xgb, 'notebooks/e2e_troubleshoot/openai_e2e_1_xgb.pkl')\n",
    "\n",
    "print('xgboost scores (not treat; treat; unknown)')\n",
    "print(y_openai_proba)\n",
    "\n",
    "openai_df_full_graph_1 = pd.DataFrame(y_openai_proba)\n",
    "openai_df_full_graph_1.columns = ['not-treat-score', 'treat-score', 'unknown-treat-score']\n",
    "openai_df_full_graph_1.to_csv('notebooks/e2e_troubleshoot/openai_e2e_1_xgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_df_full_graph_1.columns=['not treat','treat', 'unknown']\n",
    "openai_df_full_graph_1['GT']=test.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_df_full_1=openai_df_full_graph_1\n",
    "# Define a function to plot KDEs\n",
    "def plot_kde(ax, data, column, gt, title):\n",
    "    sns.kdeplot(data=data.loc[data.GT==gt], x=column, ax=ax, label='Non-topological Embeddings')\n",
    "    sns.kdeplot(data=openai_df_full_graph_1.loc[openai_df_full_graph_1.GT==gt], x=column, ax=ax, label='Topological Embeddings')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "# Create a 2x3 grid for subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot True Positives\n",
    "plot_kde(axes[0, 0], openai_df_full_1, 'not treat', 1, 'OpenAI - Distribution of not treat scores for True Positives')\n",
    "plot_kde(axes[0, 1], openai_df_full_1, 'treat', 1, 'OpenAI - Distribution of treat scores for True Positives')\n",
    "plot_kde(axes[0, 2], openai_df_full_1, 'unknown', 1, 'OpenAI - Distribution of unknown scores for True Positives')\n",
    "\n",
    "# Plot True Negatives\n",
    "plot_kde(axes[1, 0], openai_df_full_1, 'not treat', 0, 'OpenAI - Distribution of not treat scores for True Negatives')\n",
    "plot_kde(axes[1, 1], openai_df_full_1, 'treat', 0, 'OpenAI - Distribution of treat scores for True Negatives')\n",
    "plot_kde(axes[1, 2], openai_df_full_1, 'unknown', 0, 'OpenAI - Distribution of unknown scores for True Negatives')\n",
    "\n",
    "# Plot True Negatives\n",
    "plot_kde(axes[2, 0], openai_df_full_1, 'not treat', 0, 'OpenAI - Distribution of not treat scores for True Unknown')\n",
    "plot_kde(axes[2, 1], openai_df_full_1, 'treat', 0, 'OpenAI - Distribution of treat scores for True Unknown')\n",
    "plot_kde(axes[2, 2], openai_df_full_1, 'unknown', 0, 'OpenAI - Distribution of unknown scores for True Unknown')\n",
    "\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "#from utils import *\n",
    "\n",
    "# map\n",
    "openai_df_full_graph_1['PT'] = openai_df_full_graph_1.round().drop('GT', axis=1).idxmax(axis=1).map({'not treat': 0, 'treat': 1, 'unknown': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pubmed full\n",
    "f1_score(openai_df_full_graph_1['PT'],openai_df_full_graph_1['GT'], average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "other_args = {'drug_nodes':nodes_df_drugs, 'disease_nodes':nodes_df_disease, 'emb_dict':embedding_dict, \n",
    "              'portion':1, 'load_bar':False} \n",
    "\n",
    "# Computing scores for all models\n",
    "metrics_lst = []\n",
    "mrr_tmp = give_mrr(1000, xgb, test[test.y!=2], **other_args)\n",
    "hit_metrics = []\n",
    "for k in [1,3,5]:\n",
    "    hit_metrics.append(give_hitk(k, 1000, xgb, test[test.y!=2],  **other_args))\n",
    "print(hit_metrics)\n",
    "print(mrr_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20240820 - e2e run - most recent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_graph = pd.read_csv('notebooks/topological_embeddings_20240820.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_graph = pd.read_csv('notebooks/topological_embeddings_20240820.csv', index_col=0)\n",
    "\n",
    "openai_graph['topological_embedding'] = [np.fromstring(key.strip('[]'), sep=' ') for key in openai_graph.topological_embedding]\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2 )\n",
    "top_embed = pca.fit_transform(pd.DataFrame(openai_graph['topological_embedding'].tolist()))\n",
    "\n",
    "plt.scatter(top_embed.transpose()[0], top_embed.transpose()[1])\n",
    "plt.show()\n",
    "\n",
    "sns.kdeplot(np.array(openai_graph['topological_embedding'].tolist()).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dictionary for fast lookup\n",
    "embedding_dict = openai_graph.set_index('id')['topological_embedding'].to_dict()\n",
    "\n",
    "# Initialize feature matrices\n",
    "feature_length = 1024\n",
    "X_openai = np.empty((len(train_df_1), feature_length), dtype='float32')\n",
    "X_openai_test = np.empty((len(test), feature_length), dtype='float32')\n",
    "\n",
    "# Function to get concatenated vectors\n",
    "def get_concatenated_vector(row):\n",
    "    drug_vector = embedding_dict[row['source']]\n",
    "    disease_vector = embedding_dict[row['target']]\n",
    "    return np.concatenate([drug_vector, disease_vector])\n",
    "\n",
    "# Apply the function to each row of train_df_1\n",
    "X_openai = np.vstack(train_df_1.apply(get_concatenated_vector, axis=1))\n",
    "\n",
    "# Convert target variable to numpy array\n",
    "y_openai = train_df_1['y'].to_numpy()\n",
    "\n",
    "# Reset index for test DataFrame\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "X_openai_test = np.vstack(test.apply(get_concatenated_vector, axis=1))\n",
    "\n",
    "y_openai_test = test['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "xgb = XGBClassifier(random_state = 42)\n",
    "xgb.fit(X_openai, y_openai)\n",
    "\n",
    "y_openai_pred = xgb.predict_proba(X_openai_test)\n",
    "y_openai_proba = xgb.predict_proba(X_openai_test)\n",
    "joblib.dump(xgb, 'notebooks/e2e_troubleshoot/openai_e2e_2_xgb.pkl')\n",
    "\n",
    "print('xgboost scores (not treat; treat; unknown)')\n",
    "print(y_openai_proba)\n",
    "\n",
    "openai_df_full_graph_1 = pd.DataFrame(y_openai_proba)\n",
    "openai_df_full_graph_1.columns = ['not-treat-score', 'treat-score', 'unknown-treat-score']\n",
    "openai_df_full_graph_1.to_csv('notebooks/e2e_troubleshoot/openai_e2e_2_xgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_df_full_graph_1.columns=['not treat','treat', 'unknown']\n",
    "openai_df_full_graph_1['GT']=test.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_df_full_1=openai_df_full_graph_1\n",
    "# Define a function to plot KDEs\n",
    "def plot_kde(ax, data, column, gt, title):\n",
    "    sns.kdeplot(data=data.loc[data.GT==gt], x=column, ax=ax, label='Non-topological Embeddings')\n",
    "    sns.kdeplot(data=openai_df_full_graph_1.loc[openai_df_full_graph_1.GT==gt], x=column, ax=ax, label='Topological Embeddings')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "# Create a 2x3 grid for subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot True Positives\n",
    "plot_kde(axes[0, 0], openai_df_full_1, 'not treat', 1, 'OpenAI - Distribution of not treat scores for True Positives')\n",
    "plot_kde(axes[0, 1], openai_df_full_1, 'treat', 1, 'OpenAI - Distribution of treat scores for True Positives')\n",
    "plot_kde(axes[0, 2], openai_df_full_1, 'unknown', 1, 'OpenAI - Distribution of unknown scores for True Positives')\n",
    "\n",
    "# Plot True Negatives\n",
    "plot_kde(axes[1, 0], openai_df_full_1, 'not treat', 0, 'OpenAI - Distribution of not treat scores for True Negatives')\n",
    "plot_kde(axes[1, 1], openai_df_full_1, 'treat', 0, 'OpenAI - Distribution of treat scores for True Negatives')\n",
    "plot_kde(axes[1, 2], openai_df_full_1, 'unknown', 0, 'OpenAI - Distribution of unknown scores for True Negatives')\n",
    "\n",
    "# Plot True Negatives\n",
    "plot_kde(axes[2, 0], openai_df_full_1, 'not treat', 0, 'OpenAI - Distribution of not treat scores for True Unknown')\n",
    "plot_kde(axes[2, 1], openai_df_full_1, 'treat', 0, 'OpenAI - Distribution of treat scores for True Unknown')\n",
    "plot_kde(axes[2, 2], openai_df_full_1, 'unknown', 0, 'OpenAI - Distribution of unknown scores for True Unknown')\n",
    "\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "#from utils import *\n",
    "\n",
    "# map\n",
    "openai_df_full_graph_1['PT'] = openai_df_full_graph_1.round().drop('GT', axis=1).idxmax(axis=1).map({'not treat': 0, 'treat': 1, 'unknown': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pubmed full\n",
    "f1_score(openai_df_full_graph_1['PT'],openai_df_full_graph_1['GT'], average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "other_args = {'drug_nodes':nodes_df_drugs, 'disease_nodes':nodes_df_disease, 'emb_dict':embedding_dict, \n",
    "              'portion':1, 'load_bar':False} \n",
    "\n",
    "# Computing scores for all models\n",
    "metrics_lst = []\n",
    "mrr_tmp = give_mrr(1000, xgb, test[test.y!=2], **other_args)\n",
    "hit_metrics = []\n",
    "for k in [1,3,5]:\n",
    "    hit_metrics.append(give_hitk(k, 1000, xgb, test[test.y!=2],  **other_args))\n",
    "print(hit_metrics)\n",
    "print(mrr_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20240821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "openai_graph = pd.read_csv('notebooks/topological_embeddings_20240821.csv', index_col=0)\n",
    "\n",
    "openai_graph['topological_embedding'] = [np.fromstring(key.strip('[]'), sep=' ') for key in openai_graph.topological_embedding]\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2 )\n",
    "top_embed = pca.fit_transform(pd.DataFrame(openai_graph['topological_embedding'].tolist()))\n",
    "\n",
    "plt.scatter(top_embed.transpose()[0], top_embed.transpose()[1])\n",
    "plt.show()\n",
    "\n",
    "#sns.kdeplot(np.array(openai_graph['topological_embedding'].tolist()).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dictionary for fast lookup\n",
    "embedding_dict = openai_graph.set_index('id')['topological_embedding'].to_dict()\n",
    "\n",
    "# Initialize feature matrices\n",
    "feature_length = 1024\n",
    "X_openai = np.empty((len(train_df_1), feature_length), dtype='float32')\n",
    "X_openai_test = np.empty((len(test), feature_length), dtype='float32')\n",
    "\n",
    "# Function to get concatenated vectors\n",
    "def get_concatenated_vector(row):\n",
    "    drug_vector = embedding_dict[row['source']]\n",
    "    disease_vector = embedding_dict[row['target']]\n",
    "    return np.concatenate([drug_vector, disease_vector])\n",
    "\n",
    "# Apply the function to each row of train_df_1\n",
    "X_openai = np.vstack(train_df_1.apply(get_concatenated_vector, axis=1))\n",
    "\n",
    "# Convert target variable to numpy array\n",
    "y_openai = train_df_1['y'].to_numpy()\n",
    "\n",
    "# Reset index for test DataFrame\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "X_openai_test = np.vstack(test.apply(get_concatenated_vector, axis=1))\n",
    "\n",
    "y_openai_test = test['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "xgb = XGBClassifier(random_state = 42)\n",
    "xgb.fit(X_openai, y_openai)\n",
    "\n",
    "y_openai_pred = xgb.predict_proba(X_openai_test)\n",
    "y_openai_proba = xgb.predict_proba(X_openai_test)\n",
    "joblib.dump(xgb, 'notebooks/e2e_troubleshoot/openai_e2e_3_xgb.pkl')\n",
    "\n",
    "print('xgboost scores (not treat; treat; unknown)')\n",
    "print(y_openai_proba)\n",
    "\n",
    "openai_df_full_graph_1 = pd.DataFrame(y_openai_proba)\n",
    "openai_df_full_graph_1.columns = ['not-treat-score', 'treat-score', 'unknown-treat-score']\n",
    "openai_df_full_graph_1.to_csv('notebooks/e2e_troubleshoot/openai_e2e_3_xgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_df_full_graph_1.columns=['not treat','treat', 'unknown']\n",
    "openai_df_full_graph_1['GT']=test.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_df_full_1=openai_df_full_graph_1\n",
    "# Define a function to plot KDEs\n",
    "def plot_kde(ax, data, column, gt, title):\n",
    "    sns.kdeplot(data=data.loc[data.GT==gt], x=column, ax=ax, label='Non-topological Embeddings')\n",
    "    sns.kdeplot(data=openai_df_full_graph_1.loc[openai_df_full_graph_1.GT==gt], x=column, ax=ax, label='Topological Embeddings')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "# Create a 2x3 grid for subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot True Positives\n",
    "plot_kde(axes[0, 0], openai_df_full_1, 'not treat', 1, 'OpenAI - Distribution of not treat scores for True Positives')\n",
    "plot_kde(axes[0, 1], openai_df_full_1, 'treat', 1, 'OpenAI - Distribution of treat scores for True Positives')\n",
    "plot_kde(axes[0, 2], openai_df_full_1, 'unknown', 1, 'OpenAI - Distribution of unknown scores for True Positives')\n",
    "\n",
    "# Plot True Negatives\n",
    "plot_kde(axes[1, 0], openai_df_full_1, 'not treat', 0, 'OpenAI - Distribution of not treat scores for True Negatives')\n",
    "plot_kde(axes[1, 1], openai_df_full_1, 'treat', 0, 'OpenAI - Distribution of treat scores for True Negatives')\n",
    "plot_kde(axes[1, 2], openai_df_full_1, 'unknown', 0, 'OpenAI - Distribution of unknown scores for True Negatives')\n",
    "\n",
    "# Plot True Negatives\n",
    "plot_kde(axes[2, 0], openai_df_full_1, 'not treat', 0, 'OpenAI - Distribution of not treat scores for True Unknown')\n",
    "plot_kde(axes[2, 1], openai_df_full_1, 'treat', 0, 'OpenAI - Distribution of treat scores for True Unknown')\n",
    "plot_kde(axes[2, 2], openai_df_full_1, 'unknown', 0, 'OpenAI - Distribution of unknown scores for True Unknown')\n",
    "\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "#from utils import *\n",
    "\n",
    "# map\n",
    "openai_df_full_graph_1['PT'] = openai_df_full_graph_1.round().drop('GT', axis=1).idxmax(axis=1).map({'not treat': 0, 'treat': 1, 'unknown': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pubmed full\n",
    "f1_score(openai_df_full_graph_1['PT'],openai_df_full_graph_1['GT'], average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "other_args = {'drug_nodes':nodes_df_drugs, 'disease_nodes':nodes_df_disease, 'emb_dict':embedding_dict, \n",
    "              'portion':1, 'load_bar':False} \n",
    "\n",
    "# Computing scores for all models\n",
    "metrics_lst = []\n",
    "mrr_tmp = give_mrr(1000, xgb, test[test.y!=2], **other_args)\n",
    "hit_metrics = []\n",
    "for k in [1,3,5]:\n",
    "    hit_metrics.append(give_hitk(k, 1000, xgb, test[test.y!=2],  **other_args))\n",
    "print(hit_metrics)\n",
    "print(mrr_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# node sidecar (reproducing 240820)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes_sidecar=pd.read_csv('notebooks/e2e_troubleshoot/topological_embeddings_nodes_sidecar.csv', sep=',',encoding='utf-8')#, index_col=0)\n",
    "#mport csv\n",
    "\n",
    "df = pd.read_csv('notebooks/e2e_troubleshoot/topological_embeddings_nodes_sidecar.csv', index_col=0)#, header = None, delimiter=\",\", quoting=csv.QUOTE_NONE, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topological_embedding'] = [np.fromstring(key.strip('[]'), sep=' ') for key in df.topological_embedding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2 )\n",
    "top_embed = pca.fit_transform(pd.DataFrame(df['topological_embedding'].tolist()))\n",
    "\n",
    "plt.scatter(top_embed.transpose()[0], top_embed.transpose()[1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(np.array(df['topological_embedding'].tolist()).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dictionary for fast lookup\n",
    "embedding_dict = df.set_index('id')['topological_embedding'].to_dict()\n",
    "\n",
    "# Initialize feature matrices\n",
    "feature_length = 1024\n",
    "X_openai = np.empty((len(train_df_1), feature_length), dtype='float32')\n",
    "X_openai_test = np.empty((len(test), feature_length), dtype='float32')\n",
    "\n",
    "# Function to get concatenated vectors\n",
    "def get_concatenated_vector(row):\n",
    "    print(row)\n",
    "    print(row['source'])\n",
    "    print(row['target'])\n",
    "    print(row['source'] in embedding_dict.keys())\n",
    "    drug_vector = embedding_dict[row['source']]\n",
    "    disease_vector = embedding_dict[row['target']]\n",
    "    return np.concatenate([drug_vector, disease_vector])\n",
    "\n",
    "# Apply the function to each row of train_df_1\n",
    "X_openai = np.vstack(train_df_1.apply(get_concatenated_vector, axis=1))\n",
    "\n",
    "# Convert target variable to numpy array\n",
    "y_openai = train_df_1['y'].to_numpy()\n",
    "\n",
    "# Reset index for test DataFrame\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "X_openai_test = np.vstack(test.apply(get_concatenated_vector, axis=1))\n",
    "\n",
    "y_openai_test = test['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "xgb = XGBClassifier(random_state = 42)\n",
    "xgb.fit(X_openai, y_openai)\n",
    "\n",
    "y_openai_pred = xgb.predict_proba(X_openai_test)\n",
    "y_openai_proba = xgb.predict_proba(X_openai_test)\n",
    "joblib.dump(xgb, 'notebooks/e2e_troubleshoot/openai_e2e_sidecar_xgb.pkl')\n",
    "\n",
    "print('xgboost scores (not treat; treat; unknown)')\n",
    "print(y_openai_proba)\n",
    "\n",
    "openai_df_full_graph_1 = pd.DataFrame(y_openai_proba)\n",
    "openai_df_full_graph_1.columns = ['not-treat-score', 'treat-score', 'unknown-treat-score']\n",
    "openai_df_full_graph_1.to_csv('notebooks/e2e_troubleshoot/openai_e2e_sidecar_xgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_df_full_graph_1.columns=['not treat','treat', 'unknown']\n",
    "openai_df_full_graph_1['GT']=test.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_df_full_1=openai_df_full_graph_1\n",
    "# Define a function to plot KDEs\n",
    "def plot_kde(ax, data, column, gt, title):\n",
    "    sns.kdeplot(data=data.loc[data.GT==gt], x=column, ax=ax, label='Non-topological Embeddings')\n",
    "    sns.kdeplot(data=openai_df_full_graph_1.loc[openai_df_full_graph_1.GT==gt], x=column, ax=ax, label='Topological Embeddings')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "# Create a 2x3 grid for subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot True Positives\n",
    "plot_kde(axes[0, 0], openai_df_full_1, 'not treat', 1, 'OpenAI - Distribution of not treat scores for True Positives')\n",
    "plot_kde(axes[0, 1], openai_df_full_1, 'treat', 1, 'OpenAI - Distribution of treat scores for True Positives')\n",
    "plot_kde(axes[0, 2], openai_df_full_1, 'unknown', 1, 'OpenAI - Distribution of unknown scores for True Positives')\n",
    "\n",
    "# Plot True Negatives\n",
    "plot_kde(axes[1, 0], openai_df_full_1, 'not treat', 0, 'OpenAI - Distribution of not treat scores for True Negatives')\n",
    "plot_kde(axes[1, 1], openai_df_full_1, 'treat', 0, 'OpenAI - Distribution of treat scores for True Negatives')\n",
    "plot_kde(axes[1, 2], openai_df_full_1, 'unknown', 0, 'OpenAI - Distribution of unknown scores for True Negatives')\n",
    "\n",
    "# Plot True Negatives\n",
    "plot_kde(axes[2, 0], openai_df_full_1, 'not treat', 0, 'OpenAI - Distribution of not treat scores for True Unknown')\n",
    "plot_kde(axes[2, 1], openai_df_full_1, 'treat', 0, 'OpenAI - Distribution of treat scores for True Unknown')\n",
    "plot_kde(axes[2, 2], openai_df_full_1, 'unknown', 0, 'OpenAI - Distribution of unknown scores for True Unknown')\n",
    "\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "#from utils import *\n",
    "\n",
    "# map\n",
    "openai_df_full_graph_1['PT'] = openai_df_full_graph_1.round().drop('GT', axis=1).idxmax(axis=1).map({'not treat': 0, 'treat': 1, 'unknown': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pubmed full\n",
    "f1_score(openai_df_full_graph_1['PT'],openai_df_full_graph_1['GT'], average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "other_args = {'drug_nodes':nodes_df_drugs, 'disease_nodes':nodes_df_disease, 'emb_dict':embedding_dict, \n",
    "              'portion':1, 'load_bar':False} \n",
    "\n",
    "# Computing scores for all models\n",
    "metrics_lst = []\n",
    "mrr_tmp = give_mrr(1000, xgb, test[test.y!=2], **other_args)\n",
    "hit_metrics = []\n",
    "for k in [1,3,5]:\n",
    "    hit_metrics.append(give_hitk(k, 1000, xgb, test[test.y!=2],  **other_args))\n",
    "print(hit_metrics)\n",
    "print(mrr_tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
