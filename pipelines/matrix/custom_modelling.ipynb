{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling walkthrough\n",
    "\n",
    "The purpose of this notebook is to show how a new, sample model with custom dependencies would be developed and integrated into the pipeline.\n",
    "\n",
    "This notebook follows a hypothetical scenario where Machine Learning Engineer Maya is developing a new model, with the aim of generating her own predictions of drug-disease treatment efficacy scores. Maya is new to the EveryCure / Matrix ecosystem, and is learning as she goes.\n",
    "\n",
    "In the end, she wants to train and submit a new model to the pipeline, and have it evaluated along with the other models.\n",
    "\n",
    "## Modelling assumptions\n",
    "\n",
    "Maya's goal is to train a new model that will predict the efficacy of drug-disease interactions.\n",
    "\n",
    "**Embeddings from Knowledge Graph:** Maya knows that EveryCure has generated embeddings for biomedical knowledge graph nodes, which meaningfully encode semantics of the nodes. Many of those nodes are drugs and diseases between which she wants to predict treatment efficacy.\n",
    "\n",
    "**Training Data:** Maya expects the training data to be a set of known positives and negatives, i.e. drug-disease pairs for which the treatment is known to be effective or ineffective.\n",
    "\n",
    "**Evaluation:** Maya assumes that the model will be evaluated using AUC-ROC. She also assumes that she will need to perform train-validation splits on her data, and that Matrix's pipeline downstream will be able to further test the predictions of her model.\n",
    "\n",
    "**Retrieving Data:** Importantly, Maya will retrieve the embeddings and training data from pipelines other than the modelling pipeline. She will avoid preprocessing the data itself as much as possible, relying on other resources provided by EveryCure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequesites\n",
    "\n",
    "Maya needs access to the GCS bucket containing the data (currently, `gs://mtrx-us-central1-hub-dev-storage`). Maya could retrieve those data sources manually from the bucket, using tools such as `gsutil`, but a much better way is to use Kedro's API.\n",
    "\n",
    "## Dev Environment\n",
    "\n",
    "Maya will build her prototype in Jupyter Kedro Lab. It is a standard jupyter lab, but with kedro context loaded into the notebook. To run kedro notebooks, using cloud environment (to have access to datasets in the cloud), Maya will run the command:\n",
    "\n",
    "```\n",
    "kedro jupyter notebook --env cloud\n",
    "```\n",
    "\n",
    "Importantly, she also set her `RELEASE_VERSION` in her `.env` file to `v0.2.4-rc.1`. She chose this release arbitrarily.\n",
    "\n",
    "Now, Maya will have access to Kedro datasets via Kedro API. For a full tutorial for Kedro API, see [the official documentation](https://docs.kedro.org/en/stable/notebooks_and_ipython/kedro_and_notebooks.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mKedroContext\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mproject_path\u001b[0m=\u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'/Users/mpw/Code/matrix/pipelines/matrix'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[33mconfig_loader\u001b[0m=\u001b[1;35mOmegaConfigLoader\u001b[0m\u001b[1m(\u001b[0m\u001b[33mconf_source\u001b[0m=\u001b[35m/Users/mpw/Code/matrix/pipelines/matrix/\u001b[0m\u001b[95mconf\u001b[0m, \u001b[33menv\u001b[0m=\u001b[35mcloud\u001b[0m, \u001b[33mconfig_patterns\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'catalog'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'catalog*'\u001b[0m, \u001b[32m'catalog*/**'\u001b[0m, \u001b[32m'**/catalog*'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'parameters'\u001b[0m: \u001b[1m<\u001b[0m\u001b[1;95mBoxList:\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'parameters*'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'parameters*/**'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'**/parameters*'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'**/parameters*/**'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m>, \u001b[0m\u001b[32m'credentials'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'credentials*'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'credentials*/**'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'**/credentials*'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'globals'\u001b[0m\u001b[39m: <BoxList: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'globals*'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'globals*/**'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'**/globals*'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m>, \u001b[0m\u001b[32m'spark'\u001b[0m\u001b[39m: <BoxList: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'spark*'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'spark*/**'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m>, \u001b[0m\u001b[32m'mlflow'\u001b[0m\u001b[39m: <BoxList: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'mlflow*'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'mlflow*/**'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m}\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33menv\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'cloud'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33m_package_name\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33m_hook_manager\u001b[0m\u001b[39m=<pluggy._manager.PluginManager object at \u001b[0m\u001b[1;36m0x16b2e9550\u001b[0m\u001b[1m>\u001b[0m,\n",
       "    \u001b[33m_extra_params\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'embeddings.feat.bucketized_nodes@spark'\u001b[0m,\n",
       "    \u001b[32m'embeddings.feat.bucketized_nodes@partitioned'\u001b[0m,\n",
       "    \u001b[32m'embeddings.feat.graph.node_embeddings@partitioned'\u001b[0m,\n",
       "    \u001b[32m'embeddings.feat.graph.node_embeddings@spark'\u001b[0m,\n",
       "    \u001b[32m'embeddings.feat.graph.pca_node_embeddings'\u001b[0m,\n",
       "    \u001b[32m'embeddings.feat.graph.edges_for_topological'\u001b[0m,\n",
       "    \u001b[32m'embeddings.tmp.input_nodes'\u001b[0m,\n",
       "    \u001b[32m'embeddings.tmp.input_edges'\u001b[0m,\n",
       "    \u001b[32m'embeddings.models.topological'\u001b[0m,\n",
       "    \u001b[32m'embeddings.model_output.topological'\u001b[0m,\n",
       "    \u001b[32m'embeddings.feat.nodes'\u001b[0m,\n",
       "    \u001b[32m'embeddings.reporting.loss'\u001b[0m,\n",
       "    \u001b[32m'embeddings.reporting.topological_pca'\u001b[0m,\n",
       "    \u001b[32m'embeddings.reporting.topological_pca_plot'\u001b[0m\n",
       "\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.list(\"^embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Required data sources\n",
    "\n",
    "### Embeddings \n",
    "\n",
    "Maya will use the embeddings generated by the Knowledge Graph pipeline to encode the drugs and disease into a vector space.\n",
    "\n",
    "In the embeddings pipeline, embeddings are extracted from Neo4j and saved to GCS. The pipeline is defined in the [embeddings pipeline](https://github.com/matrix-ml/matrix/blob/main/pipelines/matrix/src/matrix/pipelines/embeddings/pipeline.py). \n",
    "\n",
    "```python\n",
    "node(\n",
    "    func=nodes.extract_node_embeddings,\n",
    "    inputs={\n",
    "        \"nodes\": \"embeddings.model_output.graphsage\",\n",
    "        \"string_col\": \"params:embeddings.write_topological_col\",\n",
    "    },\n",
    "    outputs=\"embeddings.feat.nodes\",\n",
    "    name=\"extract_nodes_edges_from_db\",\n",
    "    tags=[\n",
    "        \"argowf.fuse\",\n",
    "        \"argowf.fuse-group.topological_embeddings\",\n",
    "        \"argowf.template-neo4j\",\n",
    "    ],\n",
    "),\n",
    "```\n",
    "\n",
    "Kedro Dataset to which the embeddings are saved: \n",
    "\n",
    "```yml\n",
    "embeddings.feat.nodes:\n",
    "  <<: *_spark_parquet\n",
    "  filepath: ${globals:paths.embeddings}/feat/nodes_with_embeddings\n",
    "```\n",
    "\n",
    "\n",
    "Maya knows that `${globals:paths.embeddings}/feat/nodes_with_embeddings` converts to `gs://mtrx-us-central1-hub-dev-storage/kedro/data/releases/v0.2.4-rc.1/datasets/embeddings/feat/nodes_with_embeddings`, and this is where that dataset will be available in GCP. However, a much easier way to retrieve it is via kedro catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mmatrix.datasets.gcp.SparkDatasetWithBQExternalTable\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x30484c8d0\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.datasets.embeddings__feat__nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/06/24 15:37:31] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading data from <span style=\"color: #ff8700; text-decoration-color: #ff8700\">embeddings.feat.nodes</span>                            <a href=\"file:///Users/mpw/Code/matrix/pipelines/matrix/.venv/lib/python3.11/site-packages/kedro/io/data_catalog.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">data_catalog.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/mpw/Code/matrix/pipelines/matrix/.venv/lib/python3.11/site-packages/kedro/io/data_catalog.py#508\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">508</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"font-weight: bold\">(</span>SparkDatasetWithBQExternalTable<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/06/24 15:37:31]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading data from \u001b[38;5;208membeddings.feat.nodes\u001b[0m                            \u001b]8;id=395075;file:///Users/mpw/Code/matrix/pipelines/matrix/.venv/lib/python3.11/site-packages/kedro/io/data_catalog.py\u001b\\\u001b[2mdata_catalog.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=741400;file:///Users/mpw/Code/matrix/pipelines/matrix/.venv/lib/python3.11/site-packages/kedro/io/data_catalog.py#508\u001b\\\u001b[2m508\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[1m(\u001b[0mSparkDatasetWithBQExternalTable\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m                               \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Executing for enviornment: cloud                                          <a href=\"file:///Users/mpw/Code/matrix/pipelines/matrix/src/matrix/hooks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">hooks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/mpw/Code/matrix/pipelines/matrix/src/matrix/hooks.py#145\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">145</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Executing for enviornment: cloud                                          \u001b]8;id=92656;file:///Users/mpw/Code/matrix/pipelines/matrix/src/matrix/hooks.py\u001b\\\u001b[2mhooks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=240332;file:///Users/mpw/Code/matrix/pipelines/matrix/src/matrix/hooks.py#145\u001b\\\u001b[2m145\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> With ARGO_POD_UID set to:                                                 <a href=\"file:///Users/mpw/Code/matrix/pipelines/matrix/src/matrix/hooks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">hooks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/mpw/Code/matrix/pipelines/matrix/src/matrix/hooks.py#146\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">146</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m With ARGO_POD_UID set to:                                                 \u001b]8;id=641418;file:///Users/mpw/Code/matrix/pipelines/matrix/src/matrix/hooks.py\u001b\\\u001b[2mhooks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=139387;file:///Users/mpw/Code/matrix/pipelines/matrix/src/matrix/hooks.py#146\u001b\\\u001b[2m146\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Thus determined not to be in k8s cluster and executing with               <a href=\"file:///Users/mpw/Code/matrix/pipelines/matrix/src/matrix/hooks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">hooks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/mpw/Code/matrix/pipelines/matrix/src/matrix/hooks.py#147\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">147</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         service-account.json file                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Thus determined not to be in k8s cluster and executing with               \u001b]8;id=428176;file:///Users/mpw/Code/matrix/pipelines/matrix/src/matrix/hooks.py\u001b\\\u001b[2mhooks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=376637;file:///Users/mpw/Code/matrix/pipelines/matrix/src/matrix/hooks.py#147\u001b\\\u001b[2m147\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         service-account.json file                                                 \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/06 15:37:32 WARN Utils: Your hostname, Mateuszs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.1.6.105 instead (on interface en0)\n",
      "24/12/06 15:37:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/mpw/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/mpw/.ivy2/jars\n",
      "com.google.cloud.spark#spark-3.5-bigquery added as a dependency\n",
      "org.neo4j#neo4j-connector-apache-spark_2.12 added as a dependency\n",
      "org.xerial#sqlite-jdbc added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-14a9dc89-2b7c-47a8-8fae-90ce16c0258d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.google.cloud.spark#spark-3.5-bigquery;0.39.0 in central\n",
      "\tfound com.google.cloud.spark#spark-bigquery-dsv2-common;0.39.0 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/mpw/Code/matrix/pipelines/matrix/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.google.cloud.spark#spark-bigquery-connector-common;0.39.0 in central\n",
      "\tfound com.google.cloud.spark#bigquery-connector-common;0.39.0 in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-bigquerystorage-v1;3.5.1 in central\n",
      "\tfound io.grpc#grpc-api;1.64.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.23.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.64.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.64.0 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.39.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.25.3 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-bigquerystorage-v1;3.5.1 in central\n",
      "\tfound com.google.api#api-common;2.31.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.4 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;3.0.0 in central\n",
      "\tfound com.google.guava#guava;33.2.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.2 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      "\tfound com.google.cloud#google-cloud-bigquery;2.40.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.38.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.25.3 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.34.0 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.38.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.4.0 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.35.0 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.44.1 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.44.1 in central\n",
      "\tfound com.google.api#gax-httpjson;2.48.1 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.44.1 in central\n",
      "\tfound com.google.http-client#google-http-client;1.44.1 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.14 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.16 in central\n",
      "\tfound io.grpc#grpc-context;1.64.0 in central\n",
      "\tfound org.checkerframework#checker-compat-qual;2.5.6 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.23.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.23.0 in central\n",
      "\tfound com.google.apis#google-api-services-bigquery;v2-rev20240323-2.0.0 in central\n",
      "\tfound com.google.api#gax;2.48.1 in central\n",
      "\tfound org.threeten#threetenbp;1.6.9 in central\n",
      "\tfound org.threeten#threeten-extra;1.8.0 in central\n",
      "\tfound com.google.cloud#google-cloud-bigquerystorage;3.5.1 in central\n",
      "\tfound io.grpc#grpc-util;1.64.0 in central\n",
      "\tfound io.grpc#grpc-core;1.64.0 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.4 in central\n",
      "\tfound com.google.api#gax-grpc;2.48.1 in central\n",
      "\tfound io.grpc#grpc-inprocess;1.64.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.64.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.64.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.64.0 in central\n",
      "\tfound org.json#json;20231013 in central\n",
      "\tfound commons-codec#commons-codec;1.16.0 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.64.0 in central\n",
      "\tfound org.apache.arrow#arrow-vector;16.0.0 in central\n",
      "\tfound org.apache.arrow#arrow-memory-core;16.0.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.17.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.17.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.17.1 in central\n",
      "\tfound com.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.17.1 in central\n",
      "\tfound com.google.flatbuffers#flatbuffers-java;23.5.26 in central\n",
      "\tfound org.apache.arrow#arrow-format;15.0.2 in central\n",
      "\tfound net.bytebuddy#byte-buddy;1.14.9 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.9 in central\n",
      "\tfound org.eclipse.collections#eclipse-collections;11.1.0 in central\n",
      "\tfound org.eclipse.collections#eclipse-collections-api;11.1.0 in central\n",
      "\tfound com.google.inject#guice;5.1.0 in central\n",
      "\tfound javax.inject#javax.inject;1 in central\n",
      "\tfound aopalliance#aopalliance;1.0 in central\n",
      "\tfound io.grpc#grpc-netty;1.64.0 in central\n",
      "\tfound io.netty#netty-codec-http2;4.1.109.Final in central\n",
      "\tfound io.netty#netty-common;4.1.109.Final in central\n",
      "\tfound io.netty#netty-buffer;4.1.109.Final in central\n",
      "\tfound io.netty#netty-transport;4.1.109.Final in central\n",
      "\tfound io.netty#netty-resolver;4.1.109.Final in central\n",
      "\tfound io.netty#netty-codec;4.1.109.Final in central\n",
      "\tfound io.netty#netty-handler;4.1.109.Final in central\n",
      "\tfound io.netty#netty-transport-native-unix-common;4.1.109.Final in central\n",
      "\tfound io.netty#netty-codec-http;4.1.109.Final in central\n",
      "\tfound io.netty#netty-tcnative-boringssl-static;2.0.65.Final in central\n",
      "\tfound io.netty#netty-tcnative-classes;2.0.65.Final in central\n",
      "\tfound org.apache.arrow#arrow-memory-netty;16.0.0 in central\n",
      "\tfound org.apache.arrow#arrow-memory-netty-buffer-patch;16.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.9.1 in central\n",
      "\tfound org.apache.beam#beam-sdks-java-io-hadoop-common;2.43.0 in central\n",
      "\tfound org.apache.arrow#arrow-compression;16.0.0 in central\n",
      "\tfound org.apache.commons#commons-compress;1.26.0 in central\n",
      "\tfound commons-io#commons-io;2.15.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.14.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.9-1 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.64.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound io.perfmark#perfmark-api;0.27.0 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.23 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.64.0 in central\n",
      "\tfound io.netty#netty-handler-proxy;4.1.109.Final in central\n",
      "\tfound io.netty#netty-codec-socks;4.1.109.Final in central\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.12;5.3.0_for_spark_3 in central\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.12_common;5.3.0 in central\n",
      "\tfound org.neo4j.driver#neo4j-java-driver;4.4.13 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound org.apache.xbean#xbean-asm6-shaded;4.10 in central\n",
      "\tfound org.neo4j#neo4j-cypher-dsl;2022.9.1 in central\n",
      "\tfound org.apiguardian#apiguardian-api;1.1.2 in central\n",
      "\tfound org.xerial#sqlite-jdbc;3.47.0.0 in central\n",
      ":: resolution report :: resolve 896ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\taopalliance#aopalliance;1.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.17.1 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.17.1 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.17.1 from central in [default]\n",
      "\tcom.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.17.1 from central in [default]\n",
      "\tcom.github.luben#zstd-jni;1.4.9-1 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.31.0 from central in [default]\n",
      "\tcom.google.api#gax;2.48.1 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.48.1 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;2.48.1 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.4.0 from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-bigquerystorage-v1;3.5.1 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-bigquerystorage-v1;3.5.1 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.39.0 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.34.0 from central in [default]\n",
      "\tcom.google.apis#google-api-services-bigquery;v2-rev20240323-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.23.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.23.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.4 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.4 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-bigquery;2.40.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-bigquerystorage;3.5.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.38.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.38.0 from central in [default]\n",
      "\tcom.google.cloud.spark#bigquery-connector-common;0.39.0 from central in [default]\n",
      "\tcom.google.cloud.spark#spark-3.5-bigquery;0.39.0 from central in [default]\n",
      "\tcom.google.cloud.spark#spark-bigquery-connector-common;0.39.0 from central in [default]\n",
      "\tcom.google.cloud.spark#spark-bigquery-dsv2-common;0.39.0 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.9.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.23.0 from central in [default]\n",
      "\tcom.google.flatbuffers#flatbuffers-java;23.5.26 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.2 from central in [default]\n",
      "\tcom.google.guava#guava;33.2.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.44.1 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.44.1 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.44.1 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.44.1 from central in [default]\n",
      "\tcom.google.inject#guice;5.1.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;3.0.0 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.35.0 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.25.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.25.3 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.16.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.15.1 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.64.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.64.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.64.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.64.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.64.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.64.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.64.0 from central in [default]\n",
      "\tio.grpc#grpc-inprocess;1.64.0 from central in [default]\n",
      "\tio.grpc#grpc-netty;1.64.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.64.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.64.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.64.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.64.0 from central in [default]\n",
      "\tio.grpc#grpc-util;1.64.0 from central in [default]\n",
      "\tio.netty#netty-buffer;4.1.109.Final from central in [default]\n",
      "\tio.netty#netty-codec;4.1.109.Final from central in [default]\n",
      "\tio.netty#netty-codec-http;4.1.109.Final from central in [default]\n",
      "\tio.netty#netty-codec-http2;4.1.109.Final from central in [default]\n",
      "\tio.netty#netty-codec-socks;4.1.109.Final from central in [default]\n",
      "\tio.netty#netty-common;4.1.109.Final from central in [default]\n",
      "\tio.netty#netty-handler;4.1.109.Final from central in [default]\n",
      "\tio.netty#netty-handler-proxy;4.1.109.Final from central in [default]\n",
      "\tio.netty#netty-resolver;4.1.109.Final from central in [default]\n",
      "\tio.netty#netty-tcnative-boringssl-static;2.0.65.Final from central in [default]\n",
      "\tio.netty#netty-tcnative-classes;2.0.65.Final from central in [default]\n",
      "\tio.netty#netty-transport;4.1.109.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-unix-common;4.1.109.Final from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.27.0 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\tjavax.inject#javax.inject;1 from central in [default]\n",
      "\tnet.bytebuddy#byte-buddy;1.14.9 from central in [default]\n",
      "\torg.apache.arrow#arrow-compression;16.0.0 from central in [default]\n",
      "\torg.apache.arrow#arrow-format;15.0.2 from central in [default]\n",
      "\torg.apache.arrow#arrow-memory-core;16.0.0 from central in [default]\n",
      "\torg.apache.arrow#arrow-memory-netty;16.0.0 from central in [default]\n",
      "\torg.apache.arrow#arrow-memory-netty-buffer-patch;16.0.0 from central in [default]\n",
      "\torg.apache.arrow#arrow-vector;16.0.0 from central in [default]\n",
      "\torg.apache.beam#beam-sdks-java-io-hadoop-common;2.43.0 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.26.0 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.14.0 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.14 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.16 from central in [default]\n",
      "\torg.apache.xbean#xbean-asm6-shaded;4.10 from central in [default]\n",
      "\torg.apiguardian#apiguardian-api;1.1.2 from central in [default]\n",
      "\torg.checkerframework#checker-compat-qual;2.5.6 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.23 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.eclipse.collections#eclipse-collections;11.1.0 from central in [default]\n",
      "\torg.eclipse.collections#eclipse-collections-api;11.1.0 from central in [default]\n",
      "\torg.json#json;20231013 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.12;5.3.0_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.12_common;5.3.0 from central in [default]\n",
      "\torg.neo4j#neo4j-cypher-dsl;2022.9.1 from central in [default]\n",
      "\torg.neo4j.driver#neo4j-java-driver;4.4.13 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.9 from central in [default]\n",
      "\torg.threeten#threeten-extra;1.8.0 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.9 from central in [default]\n",
      "\torg.xerial#sqlite-jdbc;3.47.0.0 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.code.gson#gson;2.10.1 by [com.google.code.gson#gson;2.9.1] in [default]\n",
      "\torg.apache.arrow#arrow-format;16.0.0 by [org.apache.arrow#arrow-format;15.0.2] in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.11 by [org.slf4j#slf4j-api;2.0.9] in [default]\n",
      "\torg.checkerframework#checker-qual;3.10.0 by [org.checkerframework#checker-qual;3.42.0] in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 by [io.perfmark#perfmark-api;0.27.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  114  |   0   |   0   |   5   ||  109  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-14a9dc89-2b7c-47a8-8fae-90ce16c0258d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 109 already retrieved (0kB/12ms)\n",
      "24/12/06 15:37:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "embeddings_spark = catalog.load(\"embeddings.feat.nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame\u001b[1m[\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mid\u001b[0m\u001b[39m>: bigint, <labels>: array<string>, topological_embedding: array<float>, pca_embedding: array<double\u001b[0m\u001b[1m>\u001b[0m, id: string, category: string\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth data\n",
    "\n",
    "Maya needs to retrieve the training data from the preprocessing pipeline, containing True / False positives and negatives that she can use to train her model on the previously retrieved embeddings. First input to other modelling pipelines is `modelling.raw.ground_truth.positives@spark`, so Maya will retrieve that dataset first (together with its negative counterpart `modelling.raw.ground_truth.negatives@spark`).\n",
    "\n",
    "```python\n",
    "node(\n",
    "    func=nodes.create_int_pairs,\n",
    "    inputs=[\n",
    "        \"embeddings.feat.nodes\",\n",
    "        \"modelling.raw.ground_truth.positives@spark\",\n",
    "        \"modelling.raw.ground_truth.negatives@spark\",\n",
    "    ],\n",
    "    outputs=\"modelling.int.known_pairs@spark\",\n",
    "    name=\"create_int_known_pairs\",\n",
    "),\n",
    "```\n",
    "\n",
    "We retrieve ground truth data (conflated True Positives and True Negatives) from GCS. Both were produced by the `preprocessing` pipeline, as dataset `modelling.raw.ground_truth.positives@pandas` and `modelling.raw.ground_truth.negatives@pandas`, and will be read in as `@spark` dataframes by modelling steps. Maya will run the command below to copy the data to her local machine. Like in the previous step, the used version is arbitrary.\n",
    "\n",
    "Maya sees that other files live alongside the `*_conflated.tsv` files, and decides to download and investigate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/06/24 15:37:40] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading data from <span style=\"color: #ff8700; text-decoration-color: #ff8700\">modelling.raw.ground_truth.positives@spark</span>       <a href=\"file:///Users/mpw/Code/matrix/pipelines/matrix/.venv/lib/python3.11/site-packages/kedro/io/data_catalog.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">data_catalog.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/mpw/Code/matrix/pipelines/matrix/.venv/lib/python3.11/site-packages/kedro/io/data_catalog.py#508\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">508</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"font-weight: bold\">(</span>SparkDataset<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/06/24 15:37:40]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading data from \u001b[38;5;208mmodelling.raw.ground_truth.positives@spark\u001b[0m       \u001b]8;id=811385;file:///Users/mpw/Code/matrix/pipelines/matrix/.venv/lib/python3.11/site-packages/kedro/io/data_catalog.py\u001b\\\u001b[2mdata_catalog.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=663306;file:///Users/mpw/Code/matrix/pipelines/matrix/.venv/lib/python3.11/site-packages/kedro/io/data_catalog.py#508\u001b\\\u001b[2m508\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[1m(\u001b[0mSparkDataset\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m                                                  \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/06/24 15:37:44] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading data from <span style=\"color: #ff8700; text-decoration-color: #ff8700\">modelling.raw.ground_truth.negatives@spark</span>       <a href=\"file:///Users/mpw/Code/matrix/pipelines/matrix/.venv/lib/python3.11/site-packages/kedro/io/data_catalog.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">data_catalog.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/mpw/Code/matrix/pipelines/matrix/.venv/lib/python3.11/site-packages/kedro/io/data_catalog.py#508\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">508</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"font-weight: bold\">(</span>SparkDataset<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/06/24 15:37:44]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading data from \u001b[38;5;208mmodelling.raw.ground_truth.negatives@spark\u001b[0m       \u001b]8;id=397743;file:///Users/mpw/Code/matrix/pipelines/matrix/.venv/lib/python3.11/site-packages/kedro/io/data_catalog.py\u001b\\\u001b[2mdata_catalog.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=140512;file:///Users/mpw/Code/matrix/pipelines/matrix/.venv/lib/python3.11/site-packages/kedro/io/data_catalog.py#508\u001b\\\u001b[2m508\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[1m(\u001b[0mSparkDataset\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m                                                  \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "ground_truths_positives = catalog.load(\"modelling.raw.ground_truth.positives@spark\")\n",
    "ground_truths_negatives = catalog.load(\"modelling.raw.ground_truth.negatives@spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "By now, Maya has obtained the embeddings and ground truth data. She will now preprocess the data to create the input for her model. She will also need to create splits for cross-validation.\n",
    "\n",
    "Maya will first inspect the ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "ground_truths_positives_df = ground_truths_positives.toPandas()\n",
    "ground_truths_negatives_df = ground_truths_negatives.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEBI:3699</td>\n",
       "      <td>MONDO:0007186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNII:84H8Z9550J</td>\n",
       "      <td>MONDO:0007186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEBI:7915</td>\n",
       "      <td>MONDO:0007186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEBI:6375</td>\n",
       "      <td>MONDO:0007186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEBI:33130</td>\n",
       "      <td>MONDO:0007186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "\n",
       "            source         target\n",
       "\u001b[1;36m0\u001b[0m       CHEBI:\u001b[1;36m3699\u001b[0m  MONDO:\u001b[1;36m0007186\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m  UNII:84H8Z9550J  MONDO:\u001b[1;36m0007186\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m       CHEBI:\u001b[1;36m7915\u001b[0m  MONDO:\u001b[1;36m0007186\u001b[0m\n",
       "\u001b[1;36m3\u001b[0m       CHEBI:\u001b[1;36m6375\u001b[0m  MONDO:\u001b[1;36m0007186\u001b[0m\n",
       "\u001b[1;36m4\u001b[0m      CHEBI:\u001b[1;36m33130\u001b[0m  MONDO:\u001b[1;36m0007186\u001b[0m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True positives\n",
    "ground_truths_positives_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEBI:32149</td>\n",
       "      <td>MONDO:0006807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEBI:32588</td>\n",
       "      <td>MONDO:0006807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEBI:6804</td>\n",
       "      <td>MONDO:0007186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEBI:8094</td>\n",
       "      <td>MONDO:0007186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL.COMPOUND:CHEMBL1187846</td>\n",
       "      <td>MONDO:0007186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "\n",
       "                          source         target\n",
       "\u001b[1;36m0\u001b[0m                    CHEBI:\u001b[1;36m32149\u001b[0m  MONDO:\u001b[1;36m0006807\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m                    CHEBI:\u001b[1;36m32588\u001b[0m  MONDO:\u001b[1;36m0006807\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m                     CHEBI:\u001b[1;36m6804\u001b[0m  MONDO:\u001b[1;36m0007186\u001b[0m\n",
       "\u001b[1;36m3\u001b[0m                     CHEBI:\u001b[1;36m8094\u001b[0m  MONDO:\u001b[1;36m0007186\u001b[0m\n",
       "\u001b[1;36m4\u001b[0m  CHEMBL.COMPOUN\u001b[1;92mD:C\u001b[0mHEMBL1187846  MONDO:\u001b[1;36m0007186\u001b[0m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True negatives\n",
    "ground_truths_negatives_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True positives and true negatives are represented as sets of source-target pairs. `source` is the drug, `target` is the disease.\n",
    "\n",
    "Maya will not be loading entire PySpark dataframe with embedding to memory - before conducting an EDA, she wants to reduce unnecessary columns.\n",
    "\n",
    "She also wants to see what biolink categories the embeddings might have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'biolink:AnatomicalEntity'\u001b[0m,\n",
       "    \u001b[32m'biolink:Drug'\u001b[0m,\n",
       "    \u001b[32m'biolink:ChemicalEntity'\u001b[0m,\n",
       "    \u001b[32m'biolink:SequenceVariant'\u001b[0m,\n",
       "    \u001b[32m'biolink:Gene'\u001b[0m,\n",
       "    \u001b[32m'biolink:Pathway'\u001b[0m,\n",
       "    \u001b[32m'biolink:Polypeptide'\u001b[0m,\n",
       "    \u001b[32m'biolink:Protein'\u001b[0m,\n",
       "    \u001b[32m'biolink:MolecularActivity'\u001b[0m,\n",
       "    \u001b[32m'biolink:GrossAnatomicalStructure'\u001b[0m,\n",
       "    \u001b[32m'biolink:DiseaseOrPhenotypicFeature'\u001b[0m,\n",
       "    \u001b[32m'biolink:PhysiologicalProcess'\u001b[0m,\n",
       "    \u001b[32m'biolink:Disease'\u001b[0m,\n",
       "    \u001b[32m'biolink:NucleicAcidEntity'\u001b[0m,\n",
       "    \u001b[32m'biolink:PhenotypicFeature'\u001b[0m,\n",
       "    \u001b[32m'biolink:BiologicalProcess'\u001b[0m,\n",
       "    \u001b[32m'biolink:GeneFamily'\u001b[0m,\n",
       "    \u001b[32m'biolink:Transcript'\u001b[0m,\n",
       "    \u001b[32m'biolink:CellularComponent'\u001b[0m,\n",
       "    \u001b[32m'biolink:ChemicalMixture'\u001b[0m,\n",
       "    \u001b[32m'biolink:MolecularMixture'\u001b[0m,\n",
       "    \u001b[32m'biolink:MolecularEntity'\u001b[0m,\n",
       "    \u001b[32m'biolink:Cell'\u001b[0m,\n",
       "    \u001b[32m'biolink:OrganismTaxon'\u001b[0m,\n",
       "    \u001b[32m'biolink:CellLine'\u001b[0m,\n",
       "    \u001b[32m'biolink:SmallMolecule'\u001b[0m,\n",
       "    \u001b[32m'biolink:Food'\u001b[0m,\n",
       "    \u001b[32m'biolink:BiologicalEntity'\u001b[0m,\n",
       "    \u001b[32m'biolink:BehavioralFeature'\u001b[0m,\n",
       "    \u001b[32m'biolink:RNAProduct'\u001b[0m,\n",
       "    \u001b[32m'biolink:Behavior'\u001b[0m,\n",
       "    \u001b[32m'biolink:Vitamin'\u001b[0m,\n",
       "    \u001b[32m'biolink:BiologicalProcessOrActivity'\u001b[0m,\n",
       "    \u001b[32m'biolink:NoncodingRNAProduct'\u001b[0m,\n",
       "    \u001b[32m'biolink:ClinicalAttribute'\u001b[0m,\n",
       "    \u001b[32m'biolink:MicroRNA'\u001b[0m,\n",
       "    \u001b[32m'biolink:ComplexMolecularMixture'\u001b[0m,\n",
       "    \u001b[32m'biolink:OrganismAttribute'\u001b[0m\n",
       "\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Get unique values from the \"category\" column\n",
    "unique_categories = embeddings_spark.select(col(\"category\")).distinct()\n",
    "\n",
    "# Collect unique values to a list (will bring data to the driver)\n",
    "unique_values = [row[\"category\"] for row in unique_categories.collect()]\n",
    "\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, Maya (a) removed all pca_embeddings, (b) removed all entities which are not drugs or diseases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define the categories to keep\n",
    "categories_to_keep = {\n",
    "    \"biolink:DiseaseOrPhenotypicFeature\", \n",
    "    \"biolink:Drug\", \n",
    "    \"biolink:Disease\", \n",
    "    \"biolink:BehavioralFeature\", \n",
    "    \"biolink:SmallMolecule\", \n",
    "    \"biolink:PhenotypicFeature\"\n",
    "}\n",
    "\n",
    "# Filter the PySpark DataFrame\n",
    "filtered_df = embeddings_spark.filter(F.col(\"category\").isin(*categories_to_keep)) \\\n",
    "    .select(\"topological_embedding\", \"id\") \\\n",
    "    .na.drop(subset=[\"id\", \"topological_embedding\"])\n",
    "\n",
    "# Convert the filtered PySpark DataFrame to a Pandas DataFrame\n",
    "embeddings_df = filtered_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`filtered_embeddings_path` now contains topological embeddings of drugs and diseases.\n",
    "\n",
    "Maya filters down the ground truths to a simple list of node ids, to create training data for her model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_ids = set(ground_truths_negatives_df[\"target\"].unique()) | set(ground_truths_negatives_df[\"source\"].unique()) | set(ground_truths_positives_df[\"target\"].unique()) | set(ground_truths_positives_df[\"source\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topological_embedding</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.030589668, 0.10044213, 0.19094326, 0.16870...</td>\n",
       "      <td>ATC:L01XY01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.008864332, -0.022826852, 0.056452665, 0.07...</td>\n",
       "      <td>ATC:A10BD14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.0649597, 0.020033212, 0.106236674, 0.15575...</td>\n",
       "      <td>ATC:G03AA11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.014651415, 0.009964101, 0.05925943, 0.06414...</td>\n",
       "      <td>ATC:C09DX02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.015761247, -0.045603417, 0.102571644, 0.13...</td>\n",
       "      <td>ATC:J05AP51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "\n",
       "                               topological_embedding           id\n",
       "\u001b[1;36m0\u001b[0m  \u001b[1m[\u001b[0m\u001b[1;36m-0.030589668\u001b[0m, \u001b[1;36m0.10044213\u001b[0m, \u001b[1;36m0.19094326\u001b[0m, \u001b[1;36m0.16870\u001b[0m\u001b[33m...\u001b[0m  ATC:L01XY01\n",
       "\u001b[1;36m1\u001b[0m  \u001b[1m[\u001b[0m\u001b[1;36m-0.008864332\u001b[0m, \u001b[1;36m-0.022826852\u001b[0m, \u001b[1;36m0.056452665\u001b[0m, \u001b[1;36m0.07\u001b[0m\u001b[33m...\u001b[0m  AT\u001b[1;92mC:A10B\u001b[0mD14\n",
       "\u001b[1;36m2\u001b[0m  \u001b[1m[\u001b[0m\u001b[1;36m-0.0649597\u001b[0m, \u001b[1;36m0.020033212\u001b[0m, \u001b[1;36m0.106236674\u001b[0m, \u001b[1;36m0.15575\u001b[0m\u001b[33m...\u001b[0m  ATC:G03AA11\n",
       "\u001b[1;36m3\u001b[0m  \u001b[1m[\u001b[0m\u001b[1;36m0.014651415\u001b[0m, \u001b[1;36m0.009964101\u001b[0m, \u001b[1;36m0.05925943\u001b[0m, \u001b[1;36m0.06414\u001b[0m\u001b[33m...\u001b[0m  AT\u001b[1;92mC:C09D\u001b[0mX02\n",
       "\u001b[1;36m4\u001b[0m  \u001b[1m[\u001b[0m\u001b[1;36m-0.015761247\u001b[0m, \u001b[1;36m-0.045603417\u001b[0m, \u001b[1;36m0.102571644\u001b[0m, \u001b[1;36m0.13\u001b[0m\u001b[33m...\u001b[0m  ATC:J05AP51"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m0.9672624018707199\u001b[0m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate how many ground truths ids have an embedding\n",
    "\n",
    "len(ground_truth_ids.intersection(embeddings_df[\"id\"].unique())) / len(ground_truth_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tn_filtered = ground_truths_negatives_df[ground_truths_negatives_df[\"target\"].isin(embeddings_df[\"id\"]) & ground_truths_negatives_df[\"source\"].isin(embeddings_df[\"id\"])]\n",
    "df_tp_filtered = ground_truths_positives_df[ground_truths_positives_df[\"target\"].isin(embeddings_df[\"id\"]) & ground_truths_positives_df[\"source\"].isin(embeddings_df[\"id\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m0.9346806918015995\u001b[0m"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many of the ground truth pairs are left\n",
    "(len(df_tn_filtered) + len(df_tp_filtered)) / (len(ground_truths_negatives_df) + len(ground_truths_positives_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maya filtered down the dataset to 3GB from 24GB, and reduced it to only relevant drugs and diseases. 97% of drugs and diseases from the ground truth data are included in the filtered embeddings, which is satisfactory.\n",
    "\n",
    "Now, she can proceed to creating her model. \n",
    "\n",
    "- `embeddings_df` is the filtered embeddings plus node ids\n",
    "- `df_tn_filtered` and `df_tp_filtered` are the ground truth data, filtered down to only include rows with a drug and disease that have an embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for modelling\n",
    "\n",
    "Maya combines the filtered embeddings with the ground truth data to create a dataset for model training. She concatenates true positives and negatives, adding a label column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate true positives and negatives, adding label column\n",
    "df_model = pd.concat([\n",
    "    df_tp_filtered.assign(label=1),\n",
    "    df_tn_filtered.assign(label=0)\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "# Join with embeddings to get source and target embeddings\n",
    "df_model = (\n",
    "    df_model\n",
    "    .merge(\n",
    "        embeddings_df[['id', 'topological_embedding']],\n",
    "        left_on='source',\n",
    "        right_on='id',\n",
    "        how='left'\n",
    "    )\n",
    "    .drop('id', axis=1)\n",
    "    .rename(columns={'topological_embedding': 'source_embedding'})\n",
    "    .merge(\n",
    "        embeddings_df[['id', 'topological_embedding']], \n",
    "        left_on='target',\n",
    "        right_on='id',\n",
    "        how='left'\n",
    "    )\n",
    "    .drop('id', axis=1)\n",
    "    .rename(columns={'topological_embedding': 'target_embedding'})\n",
    ")\n",
    "\n",
    "print(f\"Final dataset shape: {df_model.shape}\")\n",
    "df_model.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataframe with all drug-disease pairs\n",
    "\n",
    "Maya creates a cartesian product of all unique drugs and diseases to generate every possible drug-disease combination that needs a prediction. This creates a comprehensive matrix of all possible pairs, regardless of whether they were in the training data or not.\n",
    "\n",
    "The resulting matrix (shown in the heatmap) allows for easy visualization of predicted relationships across the entire drug-disease space.\n",
    "\n",
    "The code shows that this creates a large number of pairs (number of unique drugs × number of unique diseases), which is why Maya later implements batch processing to handle the predictions efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique diseases from both dataframes\n",
    "all_diseases = pd.concat([\n",
    "    df_tn_filtered[\"target\"],\n",
    "    df_tp_filtered[\"target\"]\n",
    "]).dropna().unique()\n",
    "\n",
    "# Get unique drugs from both dataframes \n",
    "all_drugs = pd.concat([\n",
    "    df_tn_filtered[\"source\"],\n",
    "    df_tp_filtered[\"source\"]\n",
    "]).dropna().unique()\n",
    "\n",
    "print(f\"Number of unique diseases: {len(all_diseases)}\")\n",
    "print(f\"Number of unique drugs: {len(all_drugs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all possible combinations of drugs and diseases\n",
    "all_pairs = pd.DataFrame(\n",
    "    [(drug, disease) for drug in all_drugs for disease in all_diseases],\n",
    "    columns=['source', 'target']\n",
    ")\n",
    "\n",
    "print(f\"Total number of drug-disease pairs: {len(all_pairs):,}\")\n",
    "print(f\"Shape of all pairs dataframe: {all_pairs.shape}\")\n",
    "all_pairs.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare features dataset\n",
    "\n",
    "\n",
    "Maya prepares features by converting embeddings into numpy arrays and concatenating them to form input features for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "\n",
    "def prepare_features(df):\n",
    "    # Convert list embeddings to numpy arrays\n",
    "    source_embeddings = np.vstack(df['source_embedding'].values)\n",
    "    target_embeddings = np.vstack(df['target_embedding'].values)\n",
    "    \n",
    "    # Concatenate the embeddings horizontally\n",
    "    return np.hstack([source_embeddings, target_embeddings])\n",
    "\n",
    "# Prepare features\n",
    "X = prepare_features(df_model)\n",
    "y = df_model['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train / Test split\n",
    "\n",
    "Maya splits the data into training and test sets, ensuring an 80/20 split, and stratifies the data based on labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Maya uses logistic regression for model training, performing cross-validation to evaluate the model's performance on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cross-validation on training data only\n",
    "n_splits = 5\n",
    "cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    ")\n",
    "\n",
    "# Perform cross-validation on training data\n",
    "cv_scores = cross_val_score(\n",
    "    model, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    cv=cv,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"Cross-validation results (on training data):\")\n",
    "print(f\"CV scores: {cv_scores}\")\n",
    "print(f\"Mean AUC-ROC: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on all training data\n",
    "final_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    ")\n",
    "trained_model = final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Maya evaluates the trained model on a held-out test set, calculating the AUC-ROC to assess its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on held-out test set\n",
    "test_predictions = trained_model.predict_proba(X_test)[:, 1]\n",
    "test_auc = roc_auc_score(y_test, test_predictions)\n",
    "print(f\"\\nFinal model performance on test set:\")\n",
    "print(f\"Test AUC-ROC: {test_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce model predictions\n",
    "\n",
    "Maya generates efficacy score predictions for all possible drug-disease pairs using a batching function to handle large data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Maya wants to generate efficacy score predictions for all drug-disease pairs. To avoid fitting all embedidngs in memory, she creates a batching function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_pairs_with_embeddings(all_pairs, embeddings_df, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Generate batches of drug-disease pairs with their embeddings.\n",
    "    \n",
    "    Args:\n",
    "        all_pairs (pd.DataFrame): DataFrame with 'source' and 'target' columns\n",
    "        embeddings_df (pd.DataFrame): DataFrame with 'id' and 'topological_embedding' columns\n",
    "        batch_size (int): Number of pairs to process in each batch\n",
    "        \n",
    "    Yields:\n",
    "        np.array: Array of concatenated source and target embeddings for the batch\n",
    "        pd.DataFrame: Corresponding batch of pairs\n",
    "    \"\"\"\n",
    "    # Create embeddings lookup dictionary for faster access\n",
    "    embeddings_dict = dict(zip(embeddings_df['id'], embeddings_df['topological_embedding']))\n",
    "    \n",
    "    # Process pairs in batches\n",
    "    for start_idx in range(0, len(all_pairs), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(all_pairs))\n",
    "        batch_pairs = all_pairs.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Get embeddings for the batch\n",
    "        source_embeddings = np.vstack([\n",
    "            embeddings_dict[source] for source in batch_pairs['source']\n",
    "        ])\n",
    "        target_embeddings = np.vstack([\n",
    "            embeddings_dict[target] for target in batch_pairs['target']\n",
    "        ])\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        batch_features = np.hstack([source_embeddings, target_embeddings])\n",
    "        \n",
    "        yield batch_features, batch_pairs\n",
    "\n",
    "# Example usage:\n",
    "def predict_all_pairs(model, all_pairs, embeddings_df, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Generate predictions for all drug-disease pairs.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model with predict_proba method\n",
    "        all_pairs (pd.DataFrame): DataFrame with all drug-disease pairs\n",
    "        embeddings_df (pd.DataFrame): DataFrame with embeddings\n",
    "        batch_size (int): Batch size for processing\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Original pairs with prediction scores\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "    all_processed_pairs = []\n",
    "    \n",
    "    for batch_features, batch_pairs in batch_pairs_with_embeddings(all_pairs, embeddings_df, batch_size):\n",
    "        # Get predictions for the batch\n",
    "        batch_predictions = model.predict_proba(batch_features)[:, 1]\n",
    "        \n",
    "        # Store results\n",
    "        batch_results = batch_pairs.copy()\n",
    "        batch_results['prediction_score'] = batch_predictions\n",
    "        all_processed_pairs.append(batch_results)\n",
    "            \n",
    "    # Combine all results\n",
    "    final_results = pd.concat(all_processed_pairs, ignore_index=True)\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "results_df = predict_all_pairs(\n",
    "    model=trained_model,\n",
    "    all_pairs=all_pairs,\n",
    "    embeddings_df=embeddings_df,\n",
    "    batch_size=50000\n",
    ")\n",
    "\n",
    "# View results\n",
    "print(f\"Generated predictions for {len(results_df):,} pairs\")\n",
    "print(\"\\nSample predictions:\")\n",
    "print(results_df.sort_values('prediction_score', ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Prediction Score distributions\n",
    "\n",
    "Maya visualizes the distribution of prediction scores using histograms and density plots to understand the model's output.\n",
    "\n",
    "### Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create figure and axes\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Histogram\n",
    "sns.histplot(\n",
    "    data=test_predictions,\n",
    "    bins=50,\n",
    "    ax=ax1\n",
    ")\n",
    "ax1.set_title('Distribution of Prediction Scores (Histogram)')\n",
    "ax1.set_xlabel('Prediction Score')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "# Separate distributions for positive and negative classes\n",
    "sns.kdeplot(\n",
    "    data=test_predictions[y_test == 1],\n",
    "    ax=ax2,\n",
    "    label='Positive Class',\n",
    "    color='green'\n",
    ")\n",
    "sns.kdeplot(\n",
    "    data=test_predictions[y_test == 0],\n",
    "    ax=ax2,\n",
    "    label='Negative Class',\n",
    "    color='red'\n",
    ")\n",
    "ax2.set_title('Distribution of Prediction Scores by Class (Density)')\n",
    "ax2.set_xlabel('Prediction Score')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some basic statistics\n",
    "print(\"\\nPrediction Score Statistics:\")\n",
    "print(f\"Mean: {test_predictions.mean():.3f}\")\n",
    "print(f\"Median: {np.median(test_predictions):.3f}\")\n",
    "print(f\"Std Dev: {test_predictions.std():.3f}\")\n",
    "print(f\"Min: {test_predictions.min():.3f}\")\n",
    "print(f\"Max: {test_predictions.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Matrix predictions\n",
    "\n",
    "Maya creates a heatmap to visualize prediction scores for a random sample of drug-disease pairs, providing insights into the model's predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Overall distribution of all predictions\n",
    "sns.kdeplot(\n",
    "    data=results_df['prediction_score'],\n",
    "    ax=ax,\n",
    "    label='All Predictions',\n",
    "    color='blue'\n",
    ")\n",
    "\n",
    "ax.set_title('Distribution of Prediction Scores for All Drug-Disease Pairs')\n",
    "ax.set_xlabel('Prediction Score')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print basic statistics for all predictions\n",
    "print(\"\\nPrediction Score Statistics for All Pairs:\")\n",
    "print(f\"Mean: {results_df['prediction_score'].mean():.3f}\")\n",
    "print(f\"Median: {results_df['prediction_score'].median():.3f}\")\n",
    "print(f\"Std Dev: {results_df['prediction_score'].std():.3f}\")\n",
    "print(f\"Min: {results_df['prediction_score'].min():.3f}\")\n",
    "print(f\"Max: {results_df['prediction_score'].max():.3f}\")\n",
    "print(f\"Total number of predictions: {len(results_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Randomly sample 20 drugs and 20 diseases\n",
    "np.random.seed(42)\n",
    "sample_drugs = np.random.choice(all_drugs, size=20, replace=False)\n",
    "sample_diseases = np.random.choice(all_diseases, size=20, replace=False)\n",
    "\n",
    "# Filter results for sampled drugs and diseases\n",
    "sample_results = results_df[\n",
    "    results_df['source'].isin(sample_drugs) & \n",
    "    results_df['target'].isin(sample_diseases)\n",
    "]\n",
    "\n",
    "# Create prediction matrix\n",
    "pred_matrix = sample_results.pivot(\n",
    "    index='source', \n",
    "    columns='target', \n",
    "    values='prediction_score'\n",
    ")\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(\n",
    "    pred_matrix,\n",
    "    cmap='YlOrRd',\n",
    "    center=0.5,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cbar_kws={'label': 'Prediction Score'}\n",
    ")\n",
    "plt.title('Drug-Disease Prediction Scores Heatmap\\n(20 random drugs and diseases)')\n",
    "plt.xlabel('Diseases')\n",
    "plt.ylabel('Drugs')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics about the sampled predictions\n",
    "print(\"\\nSample Prediction Statistics:\")\n",
    "print(f\"Mean: {sample_results['prediction_score'].mean():.3f}\")\n",
    "print(f\"Median: {sample_results['prediction_score'].median():.3f}\")\n",
    "print(f\"Std Dev: {sample_results['prediction_score'].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation conclusion\n",
    "\n",
    "Maya generated a full matrix of drug-disease treatment efficacy scores.\n",
    "\n",
    "We already can see that the model is far from perfect, and it highlights some of the issues our more advanced models has ran into - many drugs and diseases are \"frequent flyers\" with consistently high scores all across the board. She can also see that many too many drugs-disease pairs have treat scored close to 1.\n",
    "\n",
    "However, her model is only a basic logistic regression, and for the sake of this exercise we will not be focusing on improving the results she's obrained. \n",
    "\n",
    "Now Maya will add her new model as Kedro node.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating model with Kedro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that Maya has created and prototyped her model, she wants to integrate it with EveryCure's Matrix repository, train and run it as part of the `matrix` pipeline.\n",
    "\n",
    "### Integration with Kedro Pipeline\n",
    "\n",
    "1. **Pipeline Overview**:\n",
    "   - Data is prepared and preprocessed in `raw`, `kg_raw`, `ingestion`, `integration`, and `embeddings` pipelines.\n",
    "   - These datasets are consumed by downstream `modelling`, `evaluation`, `matrix_generation`, and `inference` pipelines.\n",
    "   - Kedro handles many steps automatically, such as sharding and using ApacheSpark datasets.\n",
    "\n",
    "2. **Model Configuration**:\n",
    "   - Models are injected into the Kedro pipeline via dependency injection mechanism.\n",
    "   - Maya adds her model to `DYNAMIC_PIPELINES_MAPPING` in `pipelines/matrix/src/matrix/settings.py`:\n",
    "\n",
    "   ```python\n",
    "   DYNAMIC_PIPELINES_MAPPING = {\n",
    "       \"modelling\": [\n",
    "           {\"model_name\": \"xg_baseline\", \"num_shards\": 1, \"run_inference\": False},\n",
    "           {\"model_name\": \"xg_ensemble\", \"num_shards\": 3, \"run_inference\": True},\n",
    "           {\"model_name\": \"rf\", \"num_shards\": 1, \"run_inference\": False},\n",
    "           {\"model_name\": \"xg_synth\", \"num_shards\": 1, \"run_inference\": False},\n",
    "           {\"model_name\": \"mayas_logistic_regression\", \"num_shards\": 1, \"run_inference\": False},\n",
    "       ],\n",
    "       \"evaluation\": [\n",
    "   ```\n",
    "\n",
    "3. **Model Parameters**:\n",
    "   - Configure parameters in `pipelines/matrix/conf/base/modelling/parameters/mayas_logistic_regression.yml`:\n",
    "\n",
    "   ```yml\n",
    "   modelling.mayas_logistic_regression:\n",
    "       _overrides:\n",
    "         model_tuning_args:\n",
    "           tuner:\n",
    "             object: matrix.pipelines.modelling.tuning.NopTuner\n",
    "             estimator:\n",
    "               object: sklearn.linear_model.LogisticRegression\n",
    "               random_state: ${globals:random_state}\n",
    "               device: cpu # TODO: Add cuda\n",
    "           features:\n",
    "             - source_+\n",
    "             - target_+\n",
    "           target_col_name: y\n",
    "       model_options: ${merge:${.._model_options},${._overrides}}\n",
    "   ```\n",
    "\n",
    "   - **Key Points**:\n",
    "     - The `estimator` is set to `sklearn.linear_model.LogisticRegression`. If she wanted to use an actual custom model object (one she would define and customise), she could reference it's `sklearn`-compliant class here.\n",
    "     - Uses `NopTuner` for hyperparameter tuning.\n",
    "     - Those parameters get automatically passed and injected into the model.\n",
    "     - Following a similar logic, `modelling.<MODEL_NAME>.model_tuning_args` defines parameters for the estimator object. So  all parameters that `sklearn.linear_model.LogisticRegression` eg. `penalty`, `C`, `class_weight`.  etc. could be passed via the config file. For the full list of parameters, have a look at [official documentation](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html#logisticregression).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Running model\n",
    "\n",
    "Now that the model was added to modelling suite, it will be trained and used to generate matrix when the pipeline is executed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (matrix)",
   "language": "python",
   "name": "kedro_matrix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
