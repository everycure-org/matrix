import logging
from typing import List, Union

import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoModel, AutoTokenizer

BASELINE_MODEL = "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"
CHALLENGER_MODEL = "NeuML/pubmedbert-base-embeddings"


# This is motivated by the following StackOverflow discussion that
# highlights that the BERT models ought not to be used for sentence
# embeddings directly
# https://stackoverflow.com/questions/63461262/bert-sentence-embeddings-from-transformers
class ModelStore:
    """Small convenience model store that keeps two models in memory.

    This class loads the two models on instantiation and keeps them in memory.
    Thus the API can serve requests for either model without reloading.
    It's a bit of a hack but it's convenient for now to test our hypothesis
    that the challenger model is more correct.
    """

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.logger.info("Loading models")
        self.models = {
            BASELINE_MODEL: self._load_baseline_model(),
            CHALLENGER_MODEL: self._load_challenger_model(),
        }

    def _load_baseline_model(self):
        self.logger.info("Loading baseline model")
        tokenizer = AutoTokenizer.from_pretrained(BASELINE_MODEL)
        model = AutoModel.from_pretrained(BASELINE_MODEL)
        return (tokenizer, model)

    def _load_challenger_model(self):
        self.logger.info("Loading challenger model")
        return SentenceTransformer(CHALLENGER_MODEL)

    def get_embeddings(self, texts: Union[str, List[str]], model_name: str) -> np.ndarray:
        """
        Retrieves embeddings for the given texts using the specified model.

        Args:
            texts (Union[str, List[str]]): The text(s) to generate embeddings for.
            model_name (str): The name of the model to use for generating embeddings.

        Returns:
            np.ndarray: The embeddings generated by the model.

        Raises:
            ValueError: If the specified model name is not recognized.
        """
        if model_name not in self.models:
            raise ValueError(f"Unknown model: {model_name}")

        if model_name == BASELINE_MODEL:
            return self._get_baseline_embeddings(texts)
        elif model_name == CHALLENGER_MODEL:
            return self._get_challenger_embeddings(texts)
        else:
            raise ValueError(f"Unknown model: {model_name}")

    def _get_baseline_embeddings(self, texts: Union[str, List[str]]) -> np.ndarray:
        """
        Generates embeddings for the given texts using the baseline model.

        Args:
            texts (Union[str, List[str]]): The text(s) to generate embeddings for.

        Returns:
            np.ndarray: The embeddings generated by the baseline model.
        """
        tokenizer, model = self.models[BASELINE_MODEL]
        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt", max_length=512)
        with torch.no_grad():
            embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output
        return embeddings.detach().to("cpu").numpy()

    def _get_challenger_embeddings(self, texts: Union[str, List[str]]) -> np.ndarray:
        """
        Generates embeddings for the given texts using the challenger model.

        Args:
            texts (Union[str, List[str]]): The text(s) to generate embeddings for.

        Returns:
            np.ndarray: The embeddings generated by the challenger model.
        """
        model = self.models[CHALLENGER_MODEL]
        return model.encode(texts, show_progress_bar=False)
